<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Andrés Millán Muñoz" />
  <meta name="keywords" content="raytracing, ray tracing, Monte
Carlo, Monte Carlo integration, radiometry, path tracing, Vulkan" />
  <title>Métodos de Monte Carlo para síntesis de imágenes.  Análisis teórico e implementaciones basadas en path tracing acelerado por hardware</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
  <link rel="stylesheet" href="https://unpkg.com/bamboo.css/dist/light.min.css">
  <link rel="stylesheet" href="./headers/style.css">
  <link rel="icon" type="image/x-icon" href="./img/favicon.svg">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.26.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.26.0/plugins/autoloader/prism-autoloader.min.js"></script>
  
  <!-- pandoc-eqnos: equation style -->
  <style>
    .eqnos { display: inline-block; position: relative; width: 100%; }
    .eqnos br { display: none; }
    .eqnos-number { position: absolute; right: 0em; top: 50%; line-height: 0; }
  </style>
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Métodos de Monte Carlo para síntesis de imágenes.
 Análisis teórico e implementaciones basadas en path tracing acelerado
por hardware</h1>
<p class="author">Andrés Millán Muñoz</p>
</header>
<nav id="TOC" role="doc-toc">
<h2 id="toc-title">Tabla de contenidos</h2>
<ul>
<li><a href="#sinopsis">Sinopsis</a></li>
<li><a href="#a-brief-overview">A brief overview</a></li>
<li><a href="#dedicatoria">Dedicatoria</a></li>
<li><a href="#introducción">Introducción</a>
<ul>
<li><a href="#nota-histórica">Nota histórica</a></li>
<li><a href="#objetivos-del-trabajo">Objetivos del trabajo</a></li>
<li><a href="#sobre-esta-memoria">Sobre esta memoria</a></li>
<li><a href="#principales-fuentes-consultadas">Principales fuentes
consultadas</a></li>
</ul></li>
<li><a href="#las-bases"><span class="toc-section-number">1</span> Las
bases</a>
<ul>
<li><a href="#eligiendo-direcciones"><span
class="toc-section-number">1.1</span> Eligiendo direcciones</a></li>
<li><a href="#intersecciones-rayo---objeto"><span
class="toc-section-number">1.2</span> Intersecciones rayo - objeto</a>
<ul>
<li><a href="#superficies-implícitas"><span
class="toc-section-number">1.2.1</span> Superficies implícitas</a></li>
<li><a href="#superficies-paramétricas"><span
class="toc-section-number">1.2.2</span> Superficies
paramétricas</a></li>
<li><a href="#intersecciones-con-esferas"><span
class="toc-section-number">1.2.3</span> Intersecciones con
esferas</a></li>
<li><a href="#intersecciones-con-triángulos"><span
class="toc-section-number">1.2.4</span> Intersecciones con
triángulos</a>
<ul>
<li><a href="#coordenadas-baricéntricas"><span
class="toc-section-number">1.2.4.1</span> Coordenadas
baricéntricas</a></li>
<li><a href="#calculando-la-intersección"><span
class="toc-section-number">1.2.4.2</span> Calculando la
intersección</a></li>
</ul></li>
</ul></li>
</ul></li>
<li><a href="#transporte-de-luz"><span
class="toc-section-number">2</span> Transporte de luz</a>
<ul>
<li><a href="#introducción-a-la-radiometría"><span
class="toc-section-number">2.1</span> Introducción a la radiometría</a>
<ul>
<li><a href="#potencia"><span class="toc-section-number">2.1.1</span>
Potencia</a></li>
<li><a href="#irradiancia"><span class="toc-section-number">2.1.2</span>
Irradiancia</a></li>
<li><a href="#ángulos-sólidos"><span
class="toc-section-number">2.1.3</span> Ángulos sólidos</a></li>
<li><a href="#intensidad-radiante"><span
class="toc-section-number">2.1.4</span> Intensidad radiante</a></li>
<li><a href="#radiancia"><span class="toc-section-number">2.1.5</span>
Radiancia</a></li>
<li><a href="#integrales-radiométricas"><span
class="toc-section-number">2.1.6</span> Integrales radiométricas</a>
<ul>
<li><a href="#una-nueva-expresión-de-la-irradiancia-y-el-flujo"><span
class="toc-section-number">2.1.6.1</span> Una nueva expresión de la
irradiancia y el flujo</a></li>
<li><a href="#integrando-sobre-área"><span
class="toc-section-number">2.1.6.2</span> Integrando sobre área</a></li>
</ul></li>
<li><a href="#fotometría-y-radiometría"><span
class="toc-section-number">2.1.7</span> Fotometría y
radiometría</a></li>
</ul></li>
<li><a href="#dispersión-de-luz"><span
class="toc-section-number">2.2</span> Dispersión de luz</a>
<ul>
<li><a
href="#la-función-de-distribución-de-reflectancia-bidireccional-brdf"><span
class="toc-section-number">2.2.1</span> La función de distribución de
reflectancia bidireccional (BRDF)</a></li>
<li><a
href="#la-función-de-distribución-de-transmitancia-bidireccional-btdf"><span
class="toc-section-number">2.2.2</span> La función de distribución de
transmitancia bidireccional (BTDF)</a></li>
<li><a
href="#la-función-de-distribución-de-dispersión-bidireccional-bsdf"><span
class="toc-section-number">2.2.3</span> La función de distribución de
dispersión bidireccional (BSDF)</a></li>
<li><a href="#reflectancia-hemisférica"><span
class="toc-section-number">2.2.4</span> Reflectancia
hemisférica</a></li>
</ul></li>
<li><a href="#modelos-ópticos-de-materiales"><span
class="toc-section-number">2.3</span> Modelos ópticos de materiales</a>
<ul>
<li><a href="#tipos-de-dispersión"><span
class="toc-section-number">2.3.1</span> Tipos de dispersión</a></li>
<li><a href="#reflexión"><span class="toc-section-number">2.3.2</span>
Reflexión</a>
<ul>
<li><a href="#reflexión-especular-perfecta"><span
class="toc-section-number">2.3.2.1</span> Reflexión especular
perfecta</a></li>
<li><a href="#reflexión-difusa-o-lambertiana"><span
class="toc-section-number">2.3.2.2</span> Reflexión difusa o
lambertiana</a></li>
<li><a href="#reflexión-especular-no-perfecta"><span
class="toc-section-number">2.3.2.3</span> Reflexión especular no
perfecta</a></li>
</ul></li>
<li><a href="#refracción"><span class="toc-section-number">2.3.3</span>
Refracción</a>
<ul>
<li><a href="#ley-de-snell"><span
class="toc-section-number">2.3.3.1</span> Ley de Snell</a></li>
<li><a href="#ecuaciones-de-fresnel"><span
class="toc-section-number">2.3.3.2</span> Ecuaciones de Fresnel</a></li>
<li><a href="#la-aproximación-de-schlick"><span
class="toc-section-number">2.3.3.3</span> La aproximación de
Schlick</a></li>
</ul></li>
<li><a href="#brdfs-basadas-en-modelos-de-microfacetas"><span
class="toc-section-number">2.3.4</span> BRDFs basadas en modelos de
microfacetas</a></li>
</ul></li>
<li><a href="#la-rendering-equation"><span
class="toc-section-number">2.4</span> La rendering equation</a></li>
</ul></li>
<li><a href="#métodos-de-monte-carlo"><span
class="toc-section-number">3</span> Métodos de Monte Carlo</a>
<ul>
<li><a href="#repaso-de-probabilidad"><span
class="toc-section-number">3.1</span> Repaso de probabilidad</a>
<ul>
<li><a href="#variables-aleatorias-discretas"><span
class="toc-section-number">3.1.1</span> Variables aleatorias
discretas</a></li>
<li><a href="#variables-aleatorias-continuas"><span
class="toc-section-number">3.1.2</span> Variables aleatorias
continuas</a></li>
<li><a href="#esperanza-y-varianza-de-una-variable-aleatoria"><span
class="toc-section-number">3.1.3</span> Esperanza y varianza de una
variable aleatoria</a></li>
<li><a href="#teoremas-importantes"><span
class="toc-section-number">3.1.4</span> Teoremas importantes</a></li>
<li><a href="#estimadores"><span class="toc-section-number">3.1.5</span>
Estimadores</a></li>
</ul></li>
<li><a href="#el-estimador-de-monte-carlo"><span
class="toc-section-number">3.2</span> El estimador de Monte Carlo</a>
<ul>
<li><a href="#monte-carlo-básico"><span
class="toc-section-number">3.2.1</span> Monte Carlo básico</a></li>
<li><a href="#integración-de-monte-carlo"><span
class="toc-section-number">3.2.2</span> Integración de Monte Carlo</a>
<ul>
<li><a href="#un-ejemplo-práctico-en-r"><span
class="toc-section-number">3.2.2.1</span> Un ejemplo práctico en
R</a></li>
</ul></li>
</ul></li>
<li><a href="#técnicas-de-reducción-de-varianza"><span
class="toc-section-number">3.3</span> Técnicas de reducción de
varianza</a>
<ul>
<li><a href="#muestreo-por-importancia"><span
class="toc-section-number">3.3.1</span> Muestreo por importancia</a>
<ul>
<li><a href="#muestreo-por-importancia-en-transporte-de-luz"><span
class="toc-section-number">3.3.1.1</span> Muestreo por importancia en
transporte de luz</a></li>
</ul></li>
<li><a href="#muestreo-por-importancia-múltiple"><span
class="toc-section-number">3.3.2</span> Muestreo por importancia
múltiple</a>
<ul>
<li><a
href="#muestreo-por-importancia-múltiple-en-transporte-de-luz"><span
class="toc-section-number">3.3.2.1</span> Muestreo por importancia
múltiple en transporte de luz</a></li>
</ul></li>
<li><a
href="#otras-técnicas-de-reducción-de-varianza-en-transporte-de-luz"><span
class="toc-section-number">3.3.3</span> Otras técnicas de reducción de
varianza en transporte de luz</a>
<ul>
<li><a href="#ruleta-rusa"><span
class="toc-section-number">3.3.3.1</span> Ruleta rusa</a></li>
<li><a
href="#next-event-estimation-o-muestreo-directo-de-fuentes-de-luz"><span
class="toc-section-number">3.3.3.2</span> Next event estimation, o
muestreo directo de fuentes de luz</a></li>
<li><a href="#quasi-monte-carlo"><span
class="toc-section-number">3.3.3.3</span> Quasi-Monte Carlo</a></li>
</ul></li>
</ul></li>
<li><a href="#escogiendo-puntos-aleatorios"><span
class="toc-section-number">3.4</span> Escogiendo puntos aleatorios</a>
<ul>
<li><a href="#método-de-la-transformada-inversa"><span
class="toc-section-number">3.4.1</span> Método de la transformada
inversa</a>
<ul>
<li><a href="#ejemplo-práctico-de-la-transformada-inversa-para-x2"><span
class="toc-section-number">3.4.1.1</span> Ejemplo práctico de la
transformada inversa para <span
class="math inline">\(x^2\)</span></a></li>
<li><a
href="#ejemplo-práctico-del-método-de-la-transformada-inversa-en-r"><span
class="toc-section-number">3.4.1.2</span> Ejemplo práctico del método de
la transformada inversa en R</a></li>
</ul></li>
<li><a href="#método-del-rechazo"><span
class="toc-section-number">3.4.2</span> Método del rechazo</a>
<ul>
<li><a href="#ejemplo-práctico-del-método-del-rechazo-en-r"><span
class="toc-section-number">3.4.2.1</span> Ejemplo práctico del método
del rechazo en R</a></li>
</ul></li>
</ul></li>
</ul></li>
<li><a href="#construyamos-un-path-tracer"><span
class="toc-section-number">4</span> ¡Construyamos un path tracer!</a>
<ul>
<li><a href="#el-algoritmo-de-path-tracing"><span
class="toc-section-number">4.1</span> El algoritmo de path tracing</a>
<ul>
<li><a href="#estimando-la-rendering-equation-con-monte-carlo"><span
class="toc-section-number">4.1.1</span> Estimando la rendering equation
con Monte Carlo</a></li>
<li><a href="#pseudocódigo-de-un-path-tracer"><span
class="toc-section-number">4.1.2</span> Pseudocódigo de un path
tracer</a></li>
<li><a href="#evitando-la-recursividad"><span
class="toc-section-number">4.1.3</span> Evitando la
recursividad</a></li>
</ul></li>
<li><a href="#requisitos-de-ray-tracing-en-tiempo-real"><span
class="toc-section-number">4.2</span> Requisitos de ray tracing en
tiempo real</a>
<ul>
<li><a href="#arquitecturas-de-gráficas"><span
class="toc-section-number">4.2.1</span> Arquitecturas de
gráficas</a></li>
<li><a href="#frameworks-y-api-de-ray-tracing-en-tiempo-real"><span
class="toc-section-number">4.2.2</span> Frameworks y API de ray tracing
en tiempo real</a></li>
</ul></li>
<li><a href="#setup-del-proyecto"><span
class="toc-section-number">4.3</span> Setup del proyecto</a>
<ul>
<li><a href="#un-vistazo-general-a-la-estructura"><span
class="toc-section-number">4.3.1</span> Un vistazo general a la
estructura</a></li>
<li><a href="#diagramas"><span class="toc-section-number">4.3.2</span>
Diagramas</a></li>
</ul></li>
<li><a href="#compilación-y-ejecución"><span
class="toc-section-number">4.4</span> Compilación y ejecución</a></li>
<li><a href="#estructuras-de-aceleración"><span
class="toc-section-number">4.5</span> Estructuras de aceleración</a>
<ul>
<li><a href="#botom-level-acceleration-structure-blas"><span
class="toc-section-number">4.5.1</span> Botom-Level Acceleration
Structure (BLAS)</a></li>
<li><a href="#top-level-acceleration-structure-tlas"><span
class="toc-section-number">4.5.2</span> Top-Level Acceleration Structure
(TLAS)</a></li>
</ul></li>
<li><a href="#la-ray-tracing-pipeline"><span
class="toc-section-number">4.6</span> La ray tracing pipeline</a>
<ul>
<li><a href="#descriptores-y-conceptos-básicos"><span
class="toc-section-number">4.6.1</span> Descriptores y conceptos
básicos</a></li>
<li><a href="#la-shader-binding-table"><span
class="toc-section-number">4.6.2</span> La Shader Binding Table</a></li>
<li><a href="#tipos-de-shaders"><span
class="toc-section-number">4.6.3</span> Tipos de shaders</a></li>
<li><a href="#traspaso-de-información-entre-shaders"><span
class="toc-section-number">4.6.4</span> Traspaso de información entre
shaders</a></li>
<li><a href="#creación-de-la-ray-tracing-pipeline"><span
class="toc-section-number">4.6.5</span> Creación de la ray tracing
pipeline</a></li>
</ul></li>
<li><a href="#materiales-y-objetos"><span
class="toc-section-number">4.7</span> Materiales y objetos</a></li>
<li><a href="#fuentes-de-luz"><span
class="toc-section-number">4.8</span> Fuentes de luz</a></li>
<li><a
href="#antialiasing-mediante-jittering-y-acumulación-temporal"><span
class="toc-section-number">4.9</span> Antialiasing mediante jittering y
acumulación temporal</a></li>
<li><a href="#corrección-de-gamma"><span
class="toc-section-number">4.10</span> Corrección de gamma</a></li>
</ul></li>
<li><a href="#análisis-de-rendimiento"><span
class="toc-section-number">5</span> Análisis de rendimiento</a>
<ul>
<li><a href="#usando-el-motor"><span
class="toc-section-number">5.1</span> Usando el motor</a>
<ul>
<li><a href="#cambio-de-escena"><span
class="toc-section-number">5.1.1</span> Cambio de escena</a></li>
</ul></li>
<li><a href="#path-tracing-showcase"><span
class="toc-section-number">5.2</span> Path tracing showcase</a>
<ul>
<li><a href="#materiales"><span class="toc-section-number">5.2.1</span>
Materiales</a></li>
<li><a href="#fuentes-de-luz-1"><span
class="toc-section-number">5.2.2</span> Fuentes de luz</a></li>
<li><a href="#iluminación-global"><span
class="toc-section-number">5.2.3</span> Iluminación global</a></li>
</ul></li>
<li><a href="#rendimiento"><span class="toc-section-number">5.3</span>
Rendimiento</a>
<ul>
<li><a href="#número-de-muestras"><span
class="toc-section-number">5.3.1</span> Número de muestras</a></li>
<li><a href="#profundidad-de-un-rayo"><span
class="toc-section-number">5.3.2</span> Profundidad de un rayo</a></li>
<li><a href="#acumulación-temporal"><span
class="toc-section-number">5.3.3</span> Acumulación temporal</a></li>
<li><a href="#resolución"><span class="toc-section-number">5.3.4</span>
Resolución</a></li>
<li><a href="#importance-sampling"><span
class="toc-section-number">5.3.5</span> Importance sampling</a></li>
</ul></li>
<li><a href="#comparativa-con-in-one-weekend"><span
class="toc-section-number">5.4</span> Comparativa con In One Weekend</a>
<ul>
<li><a href="#sobre-la-implementación-de-in-one-weekend"><span
class="toc-section-number">5.4.1</span> Sobre la implementación de In
One Weekend</a></li>
<li><a href="#tiempos-de-renderizado"><span
class="toc-section-number">5.4.2</span> Tiempos de renderizado</a>
<ul>
<li><a href="#por-número-de-muestras"><span
class="toc-section-number">5.4.2.1</span> Por número de
muestras</a></li>
<li><a href="#por-presupuesto-de-tiempo"><span
class="toc-section-number">5.4.2.2</span> Por presupuesto de
tiempo</a></li>
<li><a href="#conclusiones-de-la-comparativa"><span
class="toc-section-number">5.4.2.3</span> Conclusiones de la
comparativa</a></li>
</ul></li>
</ul></li>
</ul></li>
<li><a href="#conclusiones"><span class="toc-section-number">6</span>
Conclusiones</a>
<ul>
<li><a href="#posibles-mejoras"><span
class="toc-section-number">6.1</span> Posibles mejoras</a>
<ul>
<li><a href="#interfaces"><span class="toc-section-number">6.1.1</span>
Interfaces</a></li>
<li><a href="#nuevas-técnicas-de-reducción-de-ruido"><span
class="toc-section-number">6.1.2</span> Nuevas técnicas de reducción de
ruido</a></li>
<li><a href="#otras-mejoras-varias"><span
class="toc-section-number">6.1.3</span> Otras mejoras varias</a></li>
</ul></li>
</ul></li>
<li><a href="#el-presente-y-futuro-de-ray-tracing"><span
class="toc-section-number">7</span> El presente y futuro de Ray
Tracing</a>
<ul>
<li><a href="#denoising"><span class="toc-section-number">7.1</span>
<em>Denoising</em></a>
<ul>
<li><a href="#filtrado"><span class="toc-section-number">7.1.1</span>
Filtrado</a></li>
<li><a href="#machine-learning-y-técnicas-de-super-sampling"><span
class="toc-section-number">7.1.2</span> <em>Machine Learning</em> y
técnicas de <em>super sampling</em></a></li>
</ul></li>
<li><a href="#la-industria-del-videojuego"><span
class="toc-section-number">7.2</span> La industria del videojuego</a>
<ul>
<li><a href="#productos-comerciales"><span
class="toc-section-number">7.2.1</span> Productos comerciales</a></li>
<li><a href="#consolas-de-nueva-generación"><span
class="toc-section-number">7.2.2</span> Consolas de nueva
generación</a></li>
</ul></li>
<li><a href="#unreal-engine-5-y-lumen"><span
class="toc-section-number">7.3</span> Unreal Engine 5 y Lumen</a></li>
</ul></li>
<li><a href="#metodología-de-trabajo"><span
class="toc-section-number">8</span> Metodología de trabajo</a>
<ul>
<li><a href="#influencias"><span class="toc-section-number">8.1</span>
Influencias</a></li>
<li><a href="#ciclos-de-desarrollo"><span
class="toc-section-number">8.2</span> Ciclos de desarrollo</a></li>
<li><a href="#presupuesto"><span class="toc-section-number">8.3</span>
Presupuesto</a></li>
<li><a href="#diseño"><span class="toc-section-number">8.4</span>
Diseño</a>
<ul>
<li><a href="#bases-del-diseño"><span
class="toc-section-number">8.4.1</span> Bases del diseño</a></li>
<li><a href="#tipografías"><span class="toc-section-number">8.4.2</span>
Tipografías</a></li>
<li><a href="#paleta-de-colores"><span
class="toc-section-number">8.4.3</span> Paleta de colores</a></li>
</ul></li>
<li><a href="#flujo-de-trabajo-y-herramientas"><span
class="toc-section-number">8.5</span> Flujo de trabajo y
herramientas</a>
<ul>
<li><a href="#pandoc"><span class="toc-section-number">8.5.1</span>
Pandoc</a></li>
<li><a href="#figma"><span class="toc-section-number">8.5.2</span>
Figma</a></li>
<li><a href="#otros-programas"><span
class="toc-section-number">8.5.3</span> Otros programas</a></li>
</ul></li>
<li><a href="#github"><span class="toc-section-number">8.6</span>
Github</a>
<ul>
<li><a
href="#integración-continua-con-github-actions-y-github-pages"><span
class="toc-section-number">8.6.1</span> Integración continua con Github
Actions y Github Pages</a></li>
<li><a href="#issues-y-github-projects"><span
class="toc-section-number">8.6.2</span> Issues y Github
Projects</a></li>
<li><a href="#estilo-de-commits"><span
class="toc-section-number">8.6.3</span> Estilo de commits</a></li>
</ul></li>
</ul></li>
<li><a href="#glosario-de-términos"><span
class="toc-section-number">9</span> Glosario de términos</a>
<ul>
<li><a href="#notación"><span class="toc-section-number">9.1</span>
Notación</a></li>
<li><a href="#bases-de-ray-tracing"><span
class="toc-section-number">9.2</span> <span>Bases de Ray
Tracing</span></a></li>
<li><a href="#transporte-de-luz-1"><span
class="toc-section-number">9.3</span> <span>Transporte de
luz</span></a></li>
<li><a href="#métodos-de-monte-carlo-1"><span
class="toc-section-number">9.4</span> <span>Métodos de Monte
Carlo</span></a></li>
<li><a href="#construyamos-un-path-tracer-1"><span
class="toc-section-number">9.5</span> <span>Construyamos un path
tracer</span></a></li>
<li><a href="#análisis-de-rendimiento-1"><span
class="toc-section-number">9.6</span> <span>Análisis de
rendimiento</span></a></li>
</ul></li>
<li><a href="#bibliografía">Bibliografía</a></li>
</ul>
</nav>
<h1 class="unnumbered" id="sinopsis">Sinopsis</h1>
<p>En este trabajo se explorarán las técnicas modernas de síntesis de
imágenes físicamente realistas basadas en Path-Tracing en tiempo real.
Para ello, se utilizarán métodos de integración de Monte Carlo con el
fin de disminuir el tiempo de cómputo.</p>
<p>Se diseñará un software basado en la interfaz de programación de
aplicaciones gráficas Vulkan, utilizando como soporte un entorno de
desarrollo de Nvidia conocido como nvpro-samples. El software
implementará un motor gráfico basado en <em>path tracing</em>. Este
motor será capaz de renderizar numerosas escenas, cambiar los parámetros
del algoritmo path tracing y modificar las fuentes de iluminación en
tiempo de ejecución.</p>
<p>Con el fin de explorar cómo afectan diferentes métodos al ruido final
de la imagen, se estudiarán algunas técnicas de reducción de varianza
como muestreo directo de fuentes de iluminación, muestreo por
importancia o acumulación temporal. Además, el motor desarrollado se
comparará con una implementación del software creado en los libros de
<span class="citation" data-cites="Shirley2020RTW1">(<a
href="#ref-Shirley2020RTW1" role="doc-biblioref">Shirley
2020a</a>)</span> “Ray Tracing in One Weekend series”, la cual utiliza
exclusivamente la CPU. Se comporbarán las diferencias entre ambas
versiones, estudiando los puntos fuertes de cada una.</p>
<p><em>Palabras clave: raytracing, ray tracing, path tracing, métodos de
Monte Carlo, integración de Monte Carlo, transporte de luz, iluminación
global, Vulkan.</em></p>
<h1 class="unnumbered" id="a-brief-overview">A brief overview</h1>
<!-- LTeX: language=en -->
<p>To be able to capture a moment. Every human civilization in record
has always found a way to immortalise the idiosyncrasies of its sociecy.
We are not immune to this phenomenon, so it is only natural we try to
use the lastest technology to achieve the most faithful representation
of our world. Computers have the ability to produce realistic images and
astonishing simulations. Although there have been tremendous advances in
the field, many of the techniques we use today are still based on the
old days of computation, especially in 3D rendering.</p>
<p><strong>Rasterization</strong> is one of those methods. It allows us
to transform a virtual environment to a raster image, which is a set of
pixels. It is an extremelly fast method given how it works, which is
just a mapping of the scene geometry into a 2D plane. Its simplicity
makes it a great quick way of rendering an image, but it comes at the
expense of fidelity.</p>
<p>We can solve this issue with <strong>Ray tracing</strong>. Instead of
projecting virtual geometry into a plane, this algorithm tries to
simulate how light works by casting rays of light into the scene from
the camera. These rays then intersect with objects and collect
information about them. This has one key advantage over rasterization:
it manages to us parts of the scene which were not visible to the
camera.</p>
<p><strong>Path tracing</strong> takes this idea to the next level:
instead of stopping in the first intersection, it makes the rays bounce
freely through the scene. These rays gather information about object
materials, light sources and many more elements. Then, the algorithm
computes the color of the pixel by considering how much light those rays
have acummulated over their traversal. This manner of working results in
an improvement over ray tracing, which is often called global
illumination.</p>
<p><strong>Global illumination</strong> is a physical phenomenon related
to indirect lightning that occurs when photons bounce off of objects and
are reflected back into a room. In rasterization, it has been
traditionally faked by using methods such as ambient occlusion, light
maps and so on, since it is crucial in order to get a realistic image.
Path tracing naturally solves this issue given the physically accurate
nature of the algorithm.</p>
<p>The results it produces are unmatched. But they come with <strong>a
major problem: rendering times</strong>. Rendering times can be crucial
depending on the task at hand. Rasterization has been traditionally used
in real time applications, such as video games, while path tracing is
implemented in offline renderers, like the ones used for movie
productions. We then need to ask one question:</p>
<p><strong>Could it be possible to bring path tracing to real
time?</strong></p>
<p>In this project we will tackle this problem. We will build a
physically based rendering engine based on path tracing with modern
hardware in order to produce a software capable of producing realistic
images in mere milliseconds.</p>
<p>As one could imagine, this task it not an easy one. In order to
achieve it, we will need to understand multiple aspects of different
fields: from mathematics to computer imaging, with some light notions
from physics.</p>
<p><a href="#las-bases">Chapter 1</a> will set the <strong>fundamentals
of what ray tracing is</strong>, how a ray works, and how a ray
intersects with an object.</p>
<p>In <a href="#transporte-de-luz">chapter 2</a> we will study the
basics of <strong>radiometry and light transport</strong>, the field of
physics that deals with the interaction between light and matter. We
will introduce how photons work, how they are emitted from light sources
and how they travel through the environment. We will also need to
understand the mathematical abstractions in which we represent these
radiometric quantities. Then, we will need to analyse how light
interacts with matter, which, in a nutshell, can be described by the
bidirectional scattering distribution function of the material surface
and the outgoing direction of photons (since light can be either
scattered or transmitted). At the end of the chapter we will obtain the
most important equation in computer graphics: <strong>the rendering
equation</strong>, which can be described as</p>
<p><span class="math display">\[
L_o(p, \omega_o) = L_e(p, \omega_o) + \int_{H^2(\mathbf{n})}{f(p,
\omega_o \leftarrow \omega_i) L_i(p, \omega_i) \cos\theta_i\ d\omega_i}
\]</span></p>
<p>Since reality is an absurdly complex –yet simple– set of rules, we
will need to find a way of making the computations viable in real time.
That is why in <a href="#métodos-de-monte-carlo">chapter 3</a> we will
explore how random sampling can be used to generate approximations of
radiometric integrals. We will start with a brief summary of random
variables. Then, we will introduce <strong>Monte Carlo
techniques</strong>, which rely on sampling random variables from
certain distributions (usually the uniform between 0 and 1) and
averaging these samples to obtain the expected value of some other
distribution. This will allow us to compute integrals relatively easily
at the cost of some error. In order to improve these estimations, we
will study how different methods can be used to reduce the variance of
the estimator, such as (multiple) importance sampling, russian roulette,
or next-event estimation.</p>
<p>Once the theory is out of the way, it will be time to build an
application. <a href="#construyamos-un-path-tracer">Chapter 4</a> will
cover the <strong>implementation of the engine</strong>. We will first
introduce our main set of tools, which will be Vulkan graphics API and
Nvidia DesignWorks’ nvpro-samples framework <span class="citation"
data-cites="nvpro-samples">(<a href="#ref-nvpro-samples"
role="doc-biblioref">Nvidia 2022b</a>)</span>. Then, we will learn about
the structures that make real time ray tracing possible, such as the Top
and Bottom-Level Acceleration Structures (TLAS and BLAS respectively)
and the Shader Binding Table (SBT). We will also understand how the ray
tracing pipepline works, what types of shaders are there and how we can
use them to render a scene. Materials and light sources will also need
to be explained since they are a key component of the scene. Finally, we
will implement some techniques which allow us to reduce the noise of the
final image, such as gamma correction and temporal accumulation of
frames.</p>
<p>Now that we have a working engine, it is time to play with it. <a
href="#análisis-de-rendimiento">Chapter 5</a> will <strong>exhibit the
results of our work</strong>, comparing different scenes that showcase a
variety of physical phenomena. We will also analyse how it performs in
terms of image quality and frame time based on a set of parameters, like
ray depth, number of samples taken on each frame, and resolution.
Finally, we will <strong>compare our implementation</strong> with Peter
Shirley’s engine developed in <em>Ray Tracing In One Weekend</em> series
<span class="citation" data-cites="Shirley2020RTW1">(<a
href="#ref-Shirley2020RTW1" role="doc-biblioref">Shirley
2020a</a>)</span> <span class="citation"
data-cites="Shirley2020RTW2">(<a href="#ref-Shirley2020RTW2"
role="doc-biblioref">Shirley 2020b</a>)</span> <span class="citation"
data-cites="Shirley2020RTW1">(<a href="#ref-Shirley2020RTW1"
role="doc-biblioref">Shirley 2020a</a>)</span>.</p>
<p>We will end the project with the <a
href="#conclusiones">conclusions</a> that can be drawn from the results
of our work, as well as show what could be done to improve the engine in
the future. To summarize, we’ve found that the engine could improve its
sampling stategy. The images it produces are very noisy and need a
considerable amount of samples to become sharp. This is mainly due to
the weak light interface that has been implemented, since it doesn’t
take into account emissive materials found in the scene. Nevertheless,
the combination of how fast it can render a frame and the ability to mix
multiple accumulated frames over time manages to overcome this issue.
Other areas that could be further improved include the material
interface and the main engine class, which presents a high degree of
coupling.</p>
<hr>
<p>The contents of this document, the Vulkan project, an <em>In One
Weekend</em> implementation, and other utilities, can be found in the
following repository:</p>
<p><a
href="https://github.com/Asmilex/Raytracing">https://github.com/Asmilex/Raytracing</a></p>
<p><em>Keywords: raytracing, ray tracing, path tracing, Monte Carlo
methods, Monte Carlo integration, light transport, global illumination,
Vulkan.</em></p>
<h1 class="unnumbered" id="dedicatoria">Dedicatoria</h1>
<p>A mi familia por su apoyo constante y permitirme empezar una carrera
que ni siquiera sabía que quería.</p>
<p>A Blanca, Cristina, Jorge, José <em>“OC”</em>, Lucas, Mari, Marina,
Mapachana y Paula, Sergio por ayudarme con el contenido, feedback del
desarrollo y diseño de la documentación.</p>
<h1 class="unnumbered" id="introducción">Introducción</h1>
<p>Este trabajo puede visualizarse en la web <a
href="https://asmilex.github.io/Raytracing/">asmilex.github.io/Raytracing</a>
o en el <a
href="https://github.com/Asmilex/Raytracing/raw/main/docs/TFG.pdf">PDF</a>
disponible en el repositorio del trabajo <a
href="https://github.com/Asmilex/Raytracing">github.com/Asmilex/Raytracing</a>.
La página web contiene recursos adicionales como vídeos.</p>
<h2 class="unnumbered" id="nota-histórica">Nota histórica</h2>
<p>Ser capaces de capturar un momento.</p>
<p>Desde tiempos inmemoriales, este ha sido uno de los sueños de la
humanidad. La capacidad de retener lo que ven nuestros ojos comenzó con
simples pinturas ruprestres que nuestros ancestros dejaron enmarcadas en
las paredes de sus hogares.</p>
<p>Con el tiempo, la tecnología evolucionó; lo cual propició formas más
realistas de representar la realidad. El físico árabe Ibn al-Haytham, a
finales de los años 900, describió el efecto de la cámara oscura <span
class="citation" data-cites="pinhole">(<a href="#ref-pinhole"
role="doc-biblioref">Wikipedia 2022d</a>)</span>, un efecto óptico
mediante el cual se puede proyectar una imagen invertida en una pared. A
inicios del siglo XVIII, Nicéphore Niépce consiguió arreglar una imagen
capturada por las primeras cámaras <span class="citation"
data-cites="history-photography">(<a href="#ref-history-photography"
role="doc-biblioref">Wikipedia 2022j</a>)</span>. Era una impresión
primitiva, por supuesto; pero funcional. A finales de este siglo, sobre
los años 1890, la fotografía se extendió rápidamente en el espacio del
consumidor gracias a la compañía Kodak. Finalmente, a mediados del siglo
XX la fotografía digital, la cual simplificaría muchos de los problemas
de las cámaras tradicionales.</p>
<p>Una vez entró de lleno la era digital, los ordenadores personales se
volvieron una herramienta indispensable. Con ellos, los usuarios eran
capaces de mostrar imágenes en pantalla, que cambiaban bajo demanda.
Naturalmente, debido a nuestro afán por recrear el mundo, nos hicimos
una pregunta: ¿Podríamos <strong>simular la vida real</strong>?</p>
<p>Como era de esperar, este objetivo es complicado de lograr. Para
conseguirlo, hemos necesitado crear abstracciones de conceptos que nos
resultan naturales, como objetos, luces y seres vivos. <em>“Cosas”</em>
que un ordenador no entiende, y sin embargo, para nosotros
<em>funcionan</em>. Así, nació la geometría, los puntos de luces,
texturas, sombreados, y otros elementos de un escenario digital. Pero
estas abstracciones por sí mismas no son suficientes. Necesitamos
visualizarlas.</p>
<p>Para solventar este problema existen diferentes algoritmos. El más
primitivo es <strong>rasterización</strong>, una técnica utilizada para
convertir objetos tridimensionales de una escena en un conjunto de
píxeles. Proyectando acordemente el entorno a una cámara, conseguimos
colorear una región del espacio, de forma que en conjunto representan un
punto de vista de un mundo digital. Su simplicidad lo convierte en una
manera extraordinariamente rápida de conseguir una imagen. Sin embargo,
su gran inconveniente es la fidelidad. Debido a su naturaleza (que se
basa en una simple proyección), este algoritmo está extremadamente
limitado. Para aumentar el realismo del producto final, con el tiempo se
idearon métodos como <em>shadow mapping</em>, precómputo de luces, o
<em>reflection cubemaps</em>, los cuales intentan solventar el problema
subyacente de rasterización: conocer el entorno de la escena.</p>
<p>Como era de esperar, se buscaron vías alternativas a rasterización
para producir una imagen. La que más destacó fue <strong>ray
tracing</strong>. Su primer uso documentado data de los años 60, en un
artículo de Appel <span class="citation" data-cites="RT-history">(<a
href="#ref-RT-history" role="doc-biblioref">Freniere and Tourtellott
1997</a>)</span>. Parte de una idea increíblemente simple: consiste en
disparar un rayo desde una cámara para comprobar si un objeto está
ocluído. De esta forma, se resuelve el problema de conocer qué es lo que
se ve desde la cámara.</p>
<p>Un par de décadas más tarde, sobre 1980, comienzan a ser publicadas
imágenes hechas por ray tracing muy realistas. En estos años también se
experimenta un crecimiento en el número de publicaciones sobre cómo
hacer más rápido ray tracing. Uno de los puntos clave fue reducir el
tiempo requerido para calcular intersecciones con objetos, pues suponen
hasta el 95% del cómputo total. Kay y Kajiya publican un tipo de
estructura denominada <em>bounding box</em> que simplifica este problema
de manera considerable.</p>
<div id="fig:rt-original" class="fignos">
<figure>
<img loading="lazy" src="./img/00/RT%20prehistórico.jpg" style="width:40.0%"
alt="Figura 1: Ray tracing en los años 80. Fuente: (Whitted 1979)" />
<figcaption aria-hidden="true"><span>Figura 1:</span> Ray tracing en los
años 80. Fuente: <span class="citation" data-cites="Whitted1979AnII">(<a
href="#ref-Whitted1979AnII" role="doc-biblioref">Whitted
1979</a>)</span></figcaption>
</figure>
</div>
<p>En 1986 Kajiya introdujo la denominada <strong>rendering
equation</strong> <span class="citation" data-cites="kajiya">(<a
href="#ref-kajiya" role="doc-biblioref">Kajiya 1986</a>)</span>. Esta es
una ecuación que modela analíticamente la cantidad de luz de un cierto
basándose en las propiedades del material y la luz que llega a dicho
punto.</p>
<p><span class="math display">\[
L_o(p, \omega_o) = L_e(p, \omega_o) + \int_{H^2(\mathbf{n})}{f(p,
\omega_o \leftarrow \omega_i) L_i(p, \omega_i) \cos\theta_i\ d\omega_i}
\]</span></p>
<p>Debido a la complejidad de esta ecuación, se diseñó un algoritmo
denominado <strong>path tracing</strong>, el cual es capaz de estimar
numéricamente su valor. Su idea principal se basa en hacer rebotar rayos
por la escena una y otra vez, de forma que en cada impacto se adquiera
nueva información.</p>
<p>Los métodos de Monte Carlo proliferaron debido a su fundamento
teórico, el cual es idóneo para las diferentes formas de ray tracing.
Estas técnicas se basan en el uso de muestras de alguna distribución
para calcular un valor determinado. En este caso, aproximan el valor que
toma la integral de la rendering equation. Comenzaron siendo utilizados
en 1960 para el cálculo de la radiancia generada por los fotones en
simulaciones físicas, por lo que transicionaron fácilmente a ray
tracing.</p>
<p>A finales del siglo XX path tracing penetra de lleno en la industria.
Numerosas empresas comienzan a desarrollar motores de renderizado basado
en dicho algoritmo, abandonando así rasterización. Las productoras de
cine empiezan a utilizar exclusivamente medios digitales para crear
películas, una forma de crear arte nunca vista hasta la fecha.</p>
<p>La elegancia de path tracing reside en su naturaleza tan intuitiva.
Pues claro que la respuesta a “<em>¿Cómo simulamos fielmente una imagen
en un ordenador?</em>” es “<em>Representando la luz de forma
realista</em>”. Gracias a la física sabemos que los fotones emitidos por
las fuentes de iluminación se mueven por el espacio impactando en los
diferentes objetos. Y, como ocurre a menudo, la respuesta a muchos de
nuestros problemas ya existe en el mundo exterior. Aprendiendo sobre
cómo funciona nuestro alrededor nos permitirá modelar nuevos mundos a
nuestro gusto. De esta manera, podemos dejar atrás los <em>hacks</em>
que utilizábamos en rasterización; no habrá necesidad de falsificar los
efectos de iluminación, puesto que path tracing los solventa de manera
natural.</p>
<p>Aún con todos los avances del medio, <strong>el elefante en la sala
seguía siendo el rendimiento</strong>. Producir una única imagen podría
suponer horas de cómputo; incluso días. A diferencia del universo,
nosotros no podemos permitirnos el lujo de usar un elevado número de
fotones, ni hacer rebotar la luz tantas veces como queramos. Nos
pasaríamos una eternidad esperando. Y para ver una imagen en nuestra
pantalla necesitaremos estar vivos, claro. En la práctica, esto supuso
que no todos los medios pudieron pasarse a path tracing. Aquellas
industrias como la de los videojuegos, en las que se prioriza la rapidez
sobre fidelidad continuaron, tuvieron que continuar usando
rasterización. A fin de cuentas, solo disponen de unos escasos
milisegundos para renderizar una imagen.</p>
<p>Sin embargo, el paso del tiempo es imparable. Las pinturas rupestres
dieron paso al óleo sobre lienzo, mientras que las cámaras digitales
reemplazaron a las oscuras. Es natural esperar que, en algún momento,
rasterización se convierta en un algoritmo del pasado. Y ese momento es
la actualidad.</p>
<p>En 2018 Nvidia introdujo la arquitectura de tarjetas gráficas Turing
<span class="citation" data-cites="turing-arquitecture">(<a
href="#ref-turing-arquitecture" role="doc-biblioref">H. M. Nvidia Emmett
Kilgariff 2018</a>)</span>. Aunque por estos años ya existían
implementaciones de ray tracing en gráficas (tan temprano como <span
class="citation" data-cites="10.1145/566654.566640">(<a
href="#ref-10.1145/566654.566640" role="doc-biblioref">Purcell et al.
2002</a>)</span>, <span class="citation" data-cites="Ertl_theray">(<a
href="#ref-Ertl_theray" role="doc-biblioref">Ertl et al.
2022</a>)</span>), esta arquitectura presenta la capacidad de realizar
cómputos específicos de ray tracing acelerados por hardware en gráficas
de consumidor. Esto significa que <strong>path tracing se vuelve viable
en tiempo real</strong>. En lugar de horas, renderizar una imagen
costará milisegundos.</p>
<p>Se da el pistoletazo de salida a una nueva transición.</p>
<h2 class="unnumbered" id="objetivos-del-trabajo">Objetivos del
trabajo</h2>
<p>En este trabajo se estudiarán los <strong>fundamentos de ray tracing
y path tracing en tiempo real</strong>. Para conseguirlo, se han
propuesto los siguientes objetivos:</p>
<ul>
<li>Analizar los algoritmos modernos de visualización en 3D basados en
métodos de Monte Carlo, entre los que se encuentra path tracing.</li>
<li>Revisar de las técnicas de Monte Carlo, examinando puntos fuertes y
débiles de cada una. Se buscará minimizar tanto el error en la
reconstrucción de la imagen como el tiempo de ejecución.</li>
<li>Implementar dichos algoritmos en una tarjeta gráfica moderna
específicamente diseñada para acelerar los cálculos de ray tracing.</li>
<li>Diseñar e implementar de un software de síntesis de imágenes
realistas por path tracing y muestreo directo de fuentes de luz por
GPU.</li>
<li>Analizar el rendimiento del motor con respecto al tiempo de
ejecución y calidad de imagen.</li>
<li>Comparación del motor desarrollado con una implementación por
CPU.</li>
<li>Investigación de las técnicas modernas y sobre el futuro del
área.</li>
</ul>
<p>Afortunadamente, <strong>se ha conseguido realizar exitosamente cada
uno de los objetivos</strong>. Esta memoria cubrirá todo el trabajo que
ha sido necesario realizar para lograrlo.</p>
<h2 class="unnumbered" id="sobre-esta-memoria">Sobre esta memoria</h2>
<p>Esta memoria recapitulará todas las técnicas utilizadas para resolver
el problema propuesto. En los primeros capítulos, estudiaremos los
fundamentos teóricos, mientras que en los posteriores construiremos una
implementación de ray tracing, la cual analizaremos con detalle para
finalizar.</p>
<p>El <a href="#las-bases">capítulo 1</a> <strong>sentará las bases de
ray tracing</strong>: qué es un rayo exactamente, cómo podemos
representarlo matemáticamente y cuáles son las ecuaciones que nos
permiten modelar la propagación e impacto con diferentes objetos.</p>
<p>En el <a href="#transporte-de-luz">capítulo 2</a> introduciremos los
fundamentos de <strong>la radiometría y el transporte de luz</strong>,
el área de la física que se encarga de la interacción entre la luz y la
materia. Estudiaremos cómo funcionan los fotones, cómo son emitidos
desde las denominadas fuentes de iluminación, y cómo se propagan por el
medio. Para conseguirlo, necesitaremos construir ciertas abstracciones
que representen propiedades radiométricas. Entre las más importantes se
encuentran la potencia, la intensidad radiante y la radiancia. También
será necesario el concepto de ángulos sólidos, los cuales generalizan la
concepción clásica de ángulo planar.</p>
<p>Será entonces cuando aprendamos cómo interacciona la luz con la
materia. Esto nos llevará a crear una familia específica de funciones
denominadas <em>bidirectional distribution functions</em>, las cuales
describen como se reflejan los fotones cuando impactan con una
superficie. Asimismo, será importante conocer la dirección de salida de
estos, por lo que habrá que estudiar los fenómenos de reflexión y
refracción. Al final del capítulo obtendremos la ecuación del transporte
de luz o <em>rendering equation</em>.</p>
<p>Esta ecuación modela fielmente cuánta luz existe en un punto
dependiendo de su entorno. Sin embargo, es prácticamente imposible
resolverla analíticamente. Por ello, con el fin de poder realizar los
cálculos en tiempo real, en el <a
href="#métodos-de-monte-carlo">capítulo 3</a> exploraremos las
<strong>técnicas de Monte Carlo</strong>. Estas técnicas se basan en el
uso de muestreo aleatorio. A partir de promediar muestras de una
variable aleatoria seremos capaces de determinar, primero, la media de
una transformación de una v.a.; y después, el valor de una integral.
Esto nos permitirá estimar el valor de la ecuación del transporte de
luz.</p>
<p>Sin embargo, muestrear sin cabeza no producirá resultados
especialmente buenos. Por ello, comprobaremos cómo algunas técnicas
reducen la varianza del estimador de Monte Carlo; y con ello, el ruido
de la imagen final. Entre los métodos estudiados se encuentran el
muestreo (múltiple) por importancia, la ruleta rusa, el muestreo directo
de fuentes de iluminación o los métodos de Quasi-Monte Carlo. Todos
estos los acabaremos enfocando al área que estamos explorando.</p>
<p>Cuando hayamos acabado con la teoría física y matemática, será el
momento de producir la aplicación. El <a
href="#construyamos-un-path-tracer">capítulo 4</a> cubirá la
<strong>construcción de un motor de renderizado</strong> físicamente
realista. Presentaremos algunas herramientas clave en la resolución del
problema; entre las que se encuentran Vulkan, una interfaz de
programación de aplicaciones gráfica, y el framework para Vulkan Ray
tracing de Nvidia DesignWorks denominado <em>nvpro-samples</em> <span
class="citation" data-cites="nvpro-samples-tutorial">(<a
href="#ref-nvpro-samples-tutorial" role="doc-biblioref">Nvidia
2022a</a>)</span>.</p>
<p>Debido a la gran complejidad del problema, será necesario introducir
algunas estructuras clave que habilitan la ejecución de ray tracing en
tiempo real. Las dos más destacables son la <em>Top</em> y
<em>Bottom-Level Acceleration Structures</em> (TLAS y BLAS,
respectivamente), que albergan información sobre la geometría de una
escena; y la <em>Shader Binding Table</em> (SBT), una estructura que
permite seleccionar shaders dinámicamente a partir de la intersección de
los rayos.</p>
<p>Será entonces cuando hablemos de cómo funciona la <em>ray tracing
pipeline</em>, qué tipos de shaders existen y cómo podemos usarlos para
renderizar una escena. Además, hablaremos sobre cómo hemos representado
los materiales y las fuentes de iluminación. Finalmente, veremos algunas
técnicas adicionales que hemos usado para reducir el ruido de la imagen,
como la corrección de gamma y acumulación temporal de frames.</p>
<p>Una vez tengamos un motor funcional, será hora de jugar con él. El <a
href="#análisis-de-rendimiento">capítulo 5</a> mostrará los
<strong>resultados de nuestro trabajo</strong>. Visualizaremos algunos
fenómenos físicos que hemos estudiado a lo largo de la memoria. Estos
serán encapsulados en aproximadamente una docena de escenas, centrándose
cada una en cierta particularidad; como puede ser la iluminación global,
la refracción y reflexión, o el comportamiento de materiales
específicos.</p>
<p>También analizaremos cómo rinde el motor en términos de calidad
visual de imagen y tiempo de renderizado. Para ello, comprobaremos cómo
varían estas dos métricas al cambiar los parámetros del algoritmo path
tracing y las técnicas de reducción de ruido. Entre estos parámetros, se
encuentran el número de muestras del estimador de Monte Carlo, la
profunidad de rebotes de un rayo, la acumulación temporal y la
resolución.</p>
<p>Para poner en contexto el rendimiento, compararemos nuestra
implementación con el path tracer desarollado en los libros de Peter
Shirley <em>Ray Tracing In One Weekend</em> series <span
class="citation" data-cites="Shirley2020RTW1">(<a
href="#ref-Shirley2020RTW1" role="doc-biblioref">Shirley
2020a</a>)</span> <span class="citation"
data-cites="Shirley2020RTW2">(<a href="#ref-Shirley2020RTW2"
role="doc-biblioref">Shirley 2020b</a>)</span> <span class="citation"
data-cites="Shirley2020RTW1">(<a href="#ref-Shirley2020RTW1"
role="doc-biblioref">Shirley 2020a</a>)</span>.</p>
<p>Terminaremos el grueso del trabajo con el <a
href="#conclusiones">capítulo de conclusiones</a>. Reflexionaremos sobre
todo lo que hemos aprendido; desde los éxitos logrados hasta las
dificultades que presenta este complejo algoritmo. Además, explicaremos
cómo se podría mejorar este trabajo, analizando de qué pie cojea la
implementación.</p>
<p>En los anexos se puede encontrar contenido adicional. El <a
href="#el-presente-y-futuro-de-ray-tracing">primer anexo</a> cubre el
<strong>estado del arte</strong> del área –aunque este trabajo es
prácticamente estado del arte–. En el <a
href="#metodología-de-trabajo">segundo</a> hablaremos sobre <strong>cómo
se ha realizado este trabajo</strong>: desde las principales influencias
hasta el diseño gráfico; pasando por el presupeusto y los ciclos de
desarrollo. Por último, se ha incluído un glosario de <a
href="#glosario-de-términos">términos y conceptos</a> para facilitar la
lectura de este documento.</p>
<h2 class="unnumbered" id="principales-fuentes-consultadas">Principales
fuentes consultadas</h2>
<p>Aunque en la realización de este trabajo se han utilizado múltiples
fuentes de información, destacan una serie de libros por encima del
resto:</p>
<ul>
<li>La colección de libros digitales de Peter Shirley <em>Ray Tracing In
One Weekend</em> <span class="citation" data-cites="Shirley2020RTW1">(<a
href="#ref-Shirley2020RTW1" role="doc-biblioref">Shirley
2020a</a>)</span> <span class="citation"
data-cites="Shirley2020RTW2">(<a href="#ref-Shirley2020RTW2"
role="doc-biblioref">Shirley 2020b</a>)</span> <span class="citation"
data-cites="Shirley2020RTW3">(<a href="#ref-Shirley2020RTW3"
role="doc-biblioref">Shirley 2020c</a>)</span>. En esencia, han sido la
inspiración de todo este proyecto. Se han utilizado como introducción al
área y para implementar algunos de los métodos que veremos en futuras
secciones; así como comparativa final con nuestro motor. Esto significa
que aparecerán múltiples veces en la memoria.</li>
<li><em>Physically Based Rendering: From Theory to Implementation (3rd
ed.)</em> <span class="citation" data-cites="PBRT3e">(<a
href="#ref-PBRT3e" role="doc-biblioref">Pharr, Jakob, and Humphreys
2016</a>)</span>. Considerado por muchos como el santo grial de la
informática gráfica moderna. El capítulo “Transporte de luz” está
fielmente basado en el trabajo de este libro. Además, algunas de las
técnicas del capítulo “Técnicas de Monte Carlo” utilizan sus
contenidos.</li>
<li>La teoría de Monte Carlo ha sido sintetizada principalmente de
<em>Métodos de Monte Carlo</em> <span class="citation"
data-cites="metodos-monte-carlo">(<a href="#ref-metodos-monte-carlo"
role="doc-biblioref">Illana 2013</a>)</span> y de <em>Monte Carlo
Theory, Methods and Examples</em> <span class="citation"
data-cites="mcbook">(<a href="#ref-mcbook" role="doc-biblioref">Owen
2013</a>)</span>.</li>
<li><em>Ray Tracing Gems I</em> <span class="citation"
data-cites="Haines2019">(<a href="#ref-Haines2019"
role="doc-biblioref">Haines and Akenine-Möller 2019</a>)</span> y
<em>Ray Tracing Gems II</em> <span class="citation"
data-cites="Marrs2021">(<a href="#ref-Marrs2021"
role="doc-biblioref">Adam Marrs and Wald 2021</a>)</span> una colección
de artículos esenciales sobre Ray Tracing publicada por Nvidia. Una
enorme variedad de expertos en el medio han participado en estos dos
libros.</li>
<li>Aunque no han sido tan decisivos, existen muchos otros recursos que
han ayudado a solidificar y cohesionar el trabajo. Entre estos, se
encuentran libros como <em>Graphics Codex</em> <span class="citation"
data-cites="McGuire2018GraphicsCodex">(<a
href="#ref-McGuire2018GraphicsCodex" role="doc-biblioref">McGuire
2021</a>)</span> y <em>Realistic Ray Tracing</em> <span class="citation"
data-cites="ShirleyRRT">(<a href="#ref-ShirleyRRT"
role="doc-biblioref">Shirley and Morley 2003</a>)</span>; así como los
cursos <em>Computer Graphics and Imaging</em> <span class="citation"
data-cites="berkeley-cs184">(<a href="#ref-berkeley-cs184"
role="doc-biblioref">Berkeley cs184 2022</a>)</span>, <em>Fundamentals
of Computer Graphics</em> <span class="citation"
data-cites="pellacini-marschner-2017">(<a
href="#ref-pellacini-marschner-2017" role="doc-biblioref">Fabio
Pellacini 2022</a>)</span>, y <em>Lecture Rendering</em> <span
class="citation" data-cites="tuwien">(<a href="#ref-tuwien"
role="doc-biblioref">Zsolnai-Fehér 2022</a>)</span>. Además, la
recolección de técnicas y estudios de Alain Galvan han supuesto un gran
refuerzo <span class="citation" data-cites="alain-API">(<a
href="#ref-alain-API" role="doc-biblioref">Galvan 2022a</a>)</span>
<span class="citation" data-cites="alain-materials">(<a
href="#ref-alain-materials" role="doc-biblioref">Galvan
2022b</a>)</span> <span class="citation" data-cites="alain-debug">(<a
href="#ref-alain-debug" role="doc-biblioref">Galvan 2022c</a>)</span>
<span class="citation" data-cites="alain-denoising">(<a
href="#ref-alain-denoising" role="doc-biblioref">Galvan
2022d</a>)</span> <span class="citation"
data-cites="alain-filtering">(<a href="#ref-alain-filtering"
role="doc-biblioref">Galvan 2022e</a>)</span>.</li>
</ul>
<h1 data-number="1" id="las-bases"><span
class="header-section-number">1</span> Las bases</h1>
<p>Empecemos por definir lo que es un rayo.</p>
<p>Un rayo <span class="citation" data-cites="Shirley2020RTW1">(<a
href="#ref-Shirley2020RTW1" role="doc-biblioref">Shirley
2020a</a>)</span> es una función <span class="math inline">\(P(t) =
\mathbf{\mathbf{o}} + t\mathbf{d}\)</span>, donde <span
class="math inline">\(\mathbf{\mathbf{o}}\)</span> el origen, un punto
del espacio afín; <span class="math inline">\(\mathbf{d}\)</span> un
vector libre, y <span class="math inline">\(t \in \mathbb{R}\)</span>.
Podemos considerarlo una interpolación entre dos puntos en el espacio,
donde <span class="math inline">\(t\)</span> controla la posición en la
que nos encontramos.</p>
<p>Por ejemplo, si <span class="math inline">\(t = 0\)</span>,
obtendremos el origen. Si <span class="math inline">\(t = 1\)</span>,
obtendremos el punto correspondiente a la dirección. Usando valores
negativos vamos <em>hacia atrás</em>.</p>
<div id="fig:rayo_basico" class="fignos">
<figure>
<img loading="lazy" src="./img/01/Rayo%20básico.png" style="width:70.0%"
alt="Figura 2: El parámetro t nos permite controlar los puntos del rayo" />
<figcaption aria-hidden="true"><span>Figura 2:</span> El parámetro <span
class="math inline">\(t\)</span> nos permite controlar los puntos del
rayo</figcaption>
</figure>
</div>
<p>Dado que estos puntos estarán generalmente en <span
class="math inline">\(\mathbb{R}^3\)</span>, podemos escribirlo como</p>
<p><span id="eq:rayo" class="eqnos"><span class="math display">\[
P(t) = (o_x, o_y, o_z) + t (d_x, d_y, d_z)
\]</span><span class="eqnos-number">(1)</span></span></p>
<p>Estos rayos los <em>dispararemos</em> a través de una cámara virtual,
que estará enfocando a la escena. De esta forma, los haremos rebotar con
los objetos que se encuentren en el camino del rayo. a este proceso lo
llamaremos <strong>ray casting</strong>.</p>
<div id="fig:ray_casting" class="fignos">
<figure>
<img loading="lazy" src="./img/01/Ray%20casting.png" style="width:80.0%"
alt="Figura 3: Diagrama de ray casting" />
<figcaption aria-hidden="true"><span>Figura 3:</span> Diagrama de ray
casting</figcaption>
</figure>
</div>
<p>Generalmente, nos quedaremos con el primer objeto que nos encontremos
en su camino. Aunque, a veces, nos interesará saber todos con los que se
encuentre.</p>
<p>Cuando un rayo impacta con un objeto, adquirirá parte de las
propiedades lumínicas del punto de impacto. Por ejemplo, cuánta luz
proporciona la lámpara que tiene encima la esfera de la figura
anterior.</p>
<p>Una vez recojamos la información que nos interese, aplicaremos otro
raycast desde el nuevo punto de impacto, escogiendo una nueva dirección
determinada. Esta dirección dependerá del tipo de material del objeto.
Y, de hecho, algunos serán capaces de invocar varios rayos.</p>
<p>Por ejemplo, los espejos reflejan la luz casi de forma perfecta;
mientras que otros elementos como el agua o el cristal reflejan
<em>y</em> refractan luz, así que necesitaremos generar dos nuevos
raycast.</p>
<p>Usando suficientes rayos obtendremos la imagen de la escena. a este
proceso de <strong>ray casting recursivo</strong> es lo que se conoce
como ray tracing.</p>
<p>Como este proceso puede continuar indefinidamente, tendremos que
controlar la profundidad de la recursión. a mayor profundidad, mayor
calidad de imagen; pero también, mayor tiempo de ejecución.</p>
<h2 data-number="1.1" id="eligiendo-direcciones"><span
class="header-section-number">1.1</span> Eligiendo direcciones</h2>
<p>Una de las partes más importantes de ray tracing, y a la que quizás
dedicaremos más tiempo, es a la elección de la dirección.</p>
<p>Hay varios factores que entran en juego a la hora de decidir qué
hacemos cuando impactamos con un nuevo objeto:</p>
<ol type="1">
<li><strong>¿Cómo es la superficie del material?</strong> a mayor
rugosidad, mayor aleatoriedad en la dirección. Por ejemplo, no es lo
mismo el asfalto de una carretera que una lámina de aluminio
impecable.</li>
<li><strong>¿Cómo de fiel es nuestra geometría?</strong></li>
<li><strong>¿Dónde se encuentran las luces en la escena?</strong>
Dependiendo de la posición, nos interesará muestrear la luz con mayor
influencia.</li>
</ol>
<p>Estas cuestiones las exploraremos a fondo en las siguientes
secciones.</p>
<h2 data-number="1.2" id="intersecciones-rayo---objeto"><span
class="header-section-number">1.2</span> Intersecciones rayo -
objeto</h2>
<p>Como dijimos al principio del capítulo, representaremos un rayo
como</p>
<p><span class="math display">\[
\begin{aligned}
P(t) &amp; = (o_x, o_y, o_z) + t (d_x, d_y, d_z) = \\
&amp; = (o_x + t d_x, o_y + t d_y, o_y + t d_z)
\end{aligned}
\]</span></p>
<p>Por ejemplo, tomando <span class="math inline">\(\mathbf{\mathbf{o}}
= (1, 3, 2), \mathbf{d} = (1, 2, 1)\)</span>:</p>
<ul>
<li>Para <span class="math inline">\(t = 0\)</span>, <span
class="math inline">\(P(t) = (1, 3, 2)\)</span>.</li>
<li>Para <span class="math inline">\(t = 1\)</span>, <span
class="math inline">\(P(t) = (1, 3, 2) + (1, 2, 1) = (2, 5,
3)\)</span>.</li>
</ul>
<p>Nos resultará especialmente útil limitar los valores que puede tomar
<span class="math inline">\(t\)</span>. Restringiremos los posibles
puntos del dominio de forma que <span class="math inline">\(t \in
[t_{min}, t_{max})\)</span>, con <span class="math inline">\(t_{min}
&lt; t_{max}\)</span>. En general, nos interesará separarnos de las
superficies un pequeño pero no despreciable <span
class="math inline">\(\varepsilon\)</span> para evitar intersecar con el
origen.</p>
<div id="fig:limites_rayo" class="fignos">
<figure>
<img loading="lazy" src="./img/01/Límites%20de%20un%20rayo.png"
alt="Figura 4: Separarnos un poquito del origen evitará que el rayo interseque con la superficie desde la que proviene." />
<figcaption aria-hidden="true"><span>Figura 4:</span> Separarnos un
poquito del origen evitará que el rayo interseque con la superficie
desde la que proviene.</figcaption>
</figure>
</div>
<p>Una de las principales cuestiones que debemos hacernos es saber
cuándo un rayo impacta con una superficie. Lo definiremos
analíticamente.</p>
<h3 data-number="1.2.1" id="superficies-implícitas"><span
class="header-section-number">1.2.1</span> Superficies implícitas</h3>
<p>Generalmente, cuando hablemos de superficies, nos referiremos
superficies diferenciables <span class="citation"
data-cites="wikipedia-contributors-2022O">(<a
href="#ref-wikipedia-contributors-2022O" role="doc-biblioref">Wikipedia
2022h</a>)</span>, pues nos interesará conocer el vector normal en cada
punto.</p>
<p>Una superficie implícita es una superficie en un espacio euclidiano
definida como</p>
<p><span class="math display">\[
F(x, y, z) = 0
\]</span></p>
<p>Esta ecuación implícita define una serie de puntos del espacio <span
class="math inline">\(\mathbb{R}^3\)</span> que se encuentran en la
superficie. Por ejemplo, la esfera se define como <span
class="math inline">\(x^2 + y^2 + z^2 - 1 = 0\)</span>.</p>
<p>Consideremos una superficie <span class="math inline">\(S\)</span> y
un punto regular <span
class="math inline">\(\mathbf{\mathbf{p}}\)</span> de ella; es decir, un
punto tal que el gradiente de <span class="math inline">\(F\)</span> en
<span class="math inline">\(\mathbf{\mathbf{p}}\)</span> no es 0. Se
define el vector normal <span class="math inline">\(\mathbf{n}\)</span>
a la superficie en ese punto como</p>
<p><span id="eq:normal_superficie" class="eqnos"><span
class="math display">\[
\mathbf{n} = \nabla F(\mathbf{p}) = \left( \frac{\partial
F(\mathbf{p})}{\partial x}, \frac{\partial F(\mathbf{p})}{\partial y},
\frac{\partial F(\mathbf{p})}{\partial z}\right )
\]</span><span class="eqnos-number">(2)</span></span></p>
<blockquote>
<p>TODO: dibujo de la normal a una superficie.</p>
</blockquote>
<p>Dado un punto <span class="math inline">\(\mathbf{q} \in
\mathbb{R}^3\)</span>, queremos saber dónde interseca un rayo <span
class="math inline">\(P(t)\)</span>. Es decir, para qué <span
class="math inline">\(t\)</span> se cumple que <span
class="math inline">\(F(P(t)) = 0 \iff F(\mathbf{\mathbf{o}} +
t\mathbf{d}) = 0\)</span>.</p>
<p>Este tipo de superficies suele requerir un cálculo numérico iterativo
por lo general, pero algunos objetos presentan expresiones sencillas que
permiten una resolución analítica.</p>
<p>Este es el caso de los planos. Para comprobarlo, tomemos un punto
<span class="math inline">\(\mathbf{q}_0\)</span> de un cierto plano y
un vector normal a la superficie <span
class="math inline">\(\mathbf{n}\)</span>. La ecuación implícita del
plano será <span class="citation" data-cites="ShirleyRRT">(<a
href="#ref-ShirleyRRT" role="doc-biblioref">Shirley and Morley
2003</a>)</span></p>
<p><span class="math display">\[
F(Q) = (\mathbf{q} - \mathbf{q}_0) \cdot \mathbf{n} = 0
\]</span></p>
<p>Si pinchamos nuestro rayo en la ecuación,</p>
<p><span class="math display">\[
\begin{aligned}
F(P(t)) &amp; = (P(t) - \mathbf{q}_0) \cdot \mathbf{n} \\
        &amp; = (\mathbf{\mathbf{o}} + t\mathbf{d} - \mathbf{q}_0) \cdot
\mathbf{n} = 0 \\
\end{aligned}
\]</span></p>
<p>Resolviendo para <span class="math inline">\(t\)</span>, esto se da
si</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{\mathbf{o}} \cdot \mathbf{n} + t\mathbf{d} \cdot \mathbf{n} -
\mathbf{q}_0 \cdot \mathbf{n} &amp; = 0 &amp; \iff \\
t\mathbf{d} \cdot \mathbf{n} &amp; = \mathbf{q}_0 \cdot \mathbf{n} -
\mathbf{\mathbf{o}} \cdot \mathbf{n} &amp; \iff \\
t &amp; = \frac{\mathbf{q}_0 \cdot \mathbf{n} - \mathbf{\mathbf{o}}
\cdot \mathbf{n}}{\mathbf{d} \cdot \mathbf{n}}
\end{aligned}
\]</span></p>
<p>Es decir, hemos obtenido el único valor de <span
class="math inline">\(t\)</span> para el cual el rayo toca la
superficie.</p>
<p>Debemos tener en cuenta el caso para el cual <span
class="math inline">\(d \cdot \mathbf{n} = 0\)</span>. Esto solo se da
si la dirección y el vector normal a la superficie son paralelos.</p>
<blockquote>
<p>TODO: dibujo de dos rayos con un plano: uno corta a la superficie,
mientras que el otro es paralelo.</p>
</blockquote>
<h3 data-number="1.2.2" id="superficies-paramétricas"><span
class="header-section-number">1.2.2</span> Superficies paramétricas</h3>
<p>Otra forma de definir una superficie en el espacio es mediante un
subconjunto <span class="math inline">\(S \subset \mathbb{R}^2\)</span>
y una serie de funciones, <span class="math inline">\(f, g, h: S
\rightarrow \mathbb{R}^3\)</span>, de forma que</p>
<p><span class="math display">\[
(x, y, z) = \left( f(u, v), g(u, v), h(u, v) \right) \\
\]</span></p>
<blockquote>
<p>En informática gráfica, hacemos algo similar cuando mapeamos una
textura a una superficie. Se conoce como <strong>UV
mapping</strong>.</p>
</blockquote>
<p>Demos un par de ejemplos de superficies paramétricas:</p>
<ul>
<li>El grafo de una función <span class="math inline">\(f: S \rightarrow
\mathbb{R}^3\)</span>, <span class="math display">\[
G(f) = \left\{(x, y, f(x, y)) \,\middle|\, (x, y) \in S \right\}
\]</span> define una superficie diferenciable siempre que <span
class="math inline">\(f\)</span> también lo sea.</li>
<li>Usando coordenadas esféricas <span class="math inline">\((r, \theta,
\phi)\)</span>, podemos parametrizar la esfera como <span
class="math inline">\((x, y, z) = (\cos\phi\sin\theta,
\sin\phi\sin\theta, \cos\theta)\)</span></li>
</ul>
<blockquote>
<p>TODO añadir imagen de coordenadas esféricas. U otro capítulo con
coordenadas.</p>
<p>NOTE: estoy usando (radial, polar, azimuthal). <span
class="math inline">\(\theta\)</span> corresponde con la apertura con
respecto a la vertical</p>
</blockquote>
<p>El vector normal <span class="math inline">\(\mathbf{n}\)</span> a la
superficie en un punto <span class="math inline">\((u, v)\)</span> del
dominio viene dado por</p>
<p><span class="math display">\[
\mathbf{n}(u, v) =
        \left( \frac{\partial f}{\partial u}, \frac{\partial g}{\partial
u}, \frac{\partial h}{\partial u} \right)
                \times
        \left( \frac{\partial f}{\partial v}, \frac{\partial g}{\partial
v}, \frac{\partial h}{\partial v} \right)
\]</span></p>
<p>El punto de intersección de una superficie paramétrica con un rayo
viene dado por aquellos puntos de la superifice <span
class="math inline">\((u, v)\)</span> para los que</p>
<p><span class="math display">\[
\begin{aligned}
\begin{cases}
o_x + td_x = f(u, v) \\
o_y + td_y = g(u, v) \\
o_z + td_z = h(u, v)
\end{cases}
\end{aligned}
\]</span></p>
<p>Con <span class="math inline">\(t \in \mathbb{R}\)</span>. Es posible
que el rayo no impacte en ningún punto. En ese caso, el sistema de
ecuaciones no tendría solución. Otra posibilidad es que intersequen en
varios puntos.</p>
<p>Por regla general este tipo de superficies no permiten un cálculo
fácil del punto de intersección, lo cual hace que se evitan en ray
tracing. En su lugar se opta por otro tipo de estructuras como las <a
href="#intersecciones-con-triángulos">mallas de triángulos</a>.</p>
<h3 data-number="1.2.3" id="intersecciones-con-esferas"><span
class="header-section-number">1.2.3</span> Intersecciones con
esferas</h3>
<p>Estudiemos ahora cómo intersecan una esfera con nuestro rayo. Una
esfera de centro <span class="math inline">\(\mathbf{c}\)</span> y radio
<span class="math inline">\(r\)</span> viene dada por aquellos puntos
<span class="math inline">\(\mathbf{p} = (x, y, z)\)</span> que
cumplen</p>
<p><span class="math display">\[
(\mathbf{p} - \mathbf{c}) \cdot (\mathbf{p} - \mathbf{c}) = r^2
\]</span></p>
<p>Podemos reescribir esta ecuación en términos de sus coordenadas para
obtener</p>
<p><span class="math display">\[
(x - c_x)^2 + (y - c_y)^2 + (z - c_z)^2 = r^2
\]</span></p>
<p>Veamos para qué valores de <span class="math inline">\(t\)</span> de
nuestro rayo se cumple esa ecuación:</p>
<p><span class="math display">\[
\begin{aligned}
(P(t) - \mathbf{c}) \cdot (P(t) - \mathbf{c}) &amp; = r^2 &amp; \iff \\
(\mathbf{o} + t\mathbf{d} - \mathbf{c}) \cdot (\mathbf{o} + t\mathbf{d}
- \mathbf{c}) &amp; = r^2 &amp; \iff \\
\end{aligned}
\]</span></p>
<p>Aplicando las propiedades del producto escalar de la conmutatividad
(<span class="math inline">\(a \cdot \mathbf{b} = \mathbf{b} \cdot
a\)</span>) y la distributiva (<span class="math inline">\(a \cdot
(\mathbf{b} + \mathbf{c}) = a \cdot \mathbf{b} + a \cdot
\mathbf{c}\)</span>), podemos escribir</p>
<p><span class="math display">\[
\begin{aligned}
((\mathbf{o} - \mathbf{c}) + t\mathbf{d}) \cdot ((\mathbf{o} -
\mathbf{c}) + t\mathbf{d}) &amp; = r^2 &amp; \iff \\
(\mathbf{o} - \mathbf{c})^2 + 2 \cdot (\mathbf{o} - \mathbf{c}) \cdot
t\mathbf{d} + (t\mathbf{d})^2 &amp; = r^2 &amp; \iff \\
d^2t^2 + 2 d \cdot (\mathbf{o} - \mathbf{c})t + (\mathbf{o} -
\mathbf{c})^2 - r^2 &amp; = 0 &amp; \iff \\
\end{aligned}
\]</span></p>
<p>Así que tenemos una ecuación de segundo grado. Resolviéndola, nos
salen nuestros puntos de intersección:</p>
<p><span class="math display">\[
t = \frac{
    - d \cdot (\mathbf{o} - \mathbf{c}) \pm \sqrt{(d \cdot (\mathbf{o} -
\mathbf{c}))^2 - 4 (\mathbf{d}^2)((\mathbf{o} - \mathbf{c})^2 - r^2)}
}{
    2 \mathbf{d}^2
}
\]</span></p>
<p>Debemos distinguir tres casos, atiendiendo al valor que toma el
discriminante <span class="math inline">\(\Delta = \small{(\mathbf{d}
\cdot (\mathbf{o} - \mathbf{c}))^2 - 4 (\mathbf{d}^2)((\mathbf{o} -
\mathbf{c})^2 - r^2)}\)</span>:</p>
<ol type="1">
<li>Si <span class="math inline">\(\Delta &lt; 0\)</span>, <span
class="math inline">\(\sqrt{\Delta} \notin \mathbb{R}\)</span>, y el
rayo no impacta con la esfera</li>
<li>Si <span class="math inline">\(\Delta = 0\)</span>, el rayo impacta
en un punto, que toma el valor <span class="math inline">\(t = \frac{-d
\cdot (\mathbf{o} - \mathbf{c})}{2 d \cdot d}\)</span>. En esta
situación, decimos que el rayo es tangente a la esfera.</li>
<li>Si <span class="math inline">\(\Delta &gt; 0\)</span>, existen dos
soluciones. En ese caso, el rayo atraviesa la esfera.</li>
</ol>
<div id="fig:interseccion_esfera" class="fignos">
<figure>
<img loading="lazy" src="./img/01/Intersección%20rayo%20-%20esfera.png"
style="width:60.0%"
alt="Figura 5: Puntos de intersección con una esfera." />
<figcaption aria-hidden="true"><span>Figura 5:</span> Puntos de
intersección con una esfera.</figcaption>
</figure>
</div>
<p>Para estos dos últimos, si consideramos <span
class="math inline">\(t_0\)</span> cualquier solución válida, el vector
normal resultante viene dado por</p>
<p><span class="math display">\[
\mathbf{n} = 2 (P(t_0) - \mathbf{c})
\]</span></p>
<p>o, normalizando,</p>
<p><span class="math display">\[
\hat{\mathbf{n}} = \frac{(P(t_0) - \mathbf{c})}{r}
\]</span></p>
<h3 data-number="1.2.4" id="intersecciones-con-triángulos"><span
class="header-section-number">1.2.4</span> Intersecciones con
triángulos</h3>
<p>Este tipo de intersecciones serán las más útiles en nuestro path
tracer. Generalmente, nuestras geometrías estarán compuestas por mallas
de triángulos, así que conocer dónde impacta nuestro rayo será clave.
Empecemos por la base:</p>
<p>Un triángulo viene dado por tres puntos, <span
class="math inline">\(a, \mathbf{b}\)</span>, y <span
class="math inline">\(\mathbf{c}\)</span>; correspondientes a sus
vértices. Para evitar casos absurdos, supongamos que estos puntos son
afinmente independientes; es decir, que no están alineados.</p>
<h4 data-number="1.2.4.1" id="coordenadas-baricéntricas"><span
class="header-section-number">1.2.4.1</span> Coordenadas
baricéntricas</h4>
<p>Podemos describir los puntos contenidos en el plano que forman estos
vertices mediante <strong>coordenadas baricéntricas</strong>. Este
sistema de coordenadas expresa cada punto del plano como una combinación
convexa de los vértices <span class="math inline">\(\mathbf{a},
\mathbf{b}, \mathbf{c}\)</span> <span class="citation"
data-cites="wikipedia-contributors-2022G">(<a
href="#ref-wikipedia-contributors-2022G" role="doc-biblioref">Wikipedia
2022i</a>)</span>. Es decir, que para cada punto <span
class="math inline">\(\mathbf{p}\)</span> del triángulo existen <span
class="math inline">\(\alpha, \beta\)</span> y <span
class="math inline">\(\gamma\)</span> tales que <span
class="math inline">\(\alpha + \beta + \gamma = 1\)</span> y</p>
<p><span class="math display">\[
\mathbf{p} = \alpha a + \beta \mathbf{b} + \gamma \mathbf{c}
\]</span></p>
<blockquote>
<p>TODO: triángulo con coordenadas baricéntricas.</p>
</blockquote>
<p>Debemos destacar que existen dos grados de libertad debido a la
restricción de que las coordenadas sumen 1.</p>
<p>Una propiedad de estas coordenadas que nos puede resultar útil es que
un punto <span class="math inline">\(\mathbf{p}\)</span> está contenido
en el triángulo si y solo si <span class="math inline">\(0 &lt; \alpha,
\beta, \gamma &lt; 1\)</span>.</p>
<p>Esta propiedad y la restricción de que sumen 1 nos da una cierta
intuición de cómo funcionan. Podemos ver las coordenadas baricéntricas
como la contribución de los vértices a un punto <span
class="math inline">\(\mathbf{p}\)</span>. Por ejemplo, si <span
class="math inline">\(\alpha = 0\)</span>, eso significa que el punto
viene dado por <span class="math inline">\(\beta \mathbf{b} + \gamma
\mathbf{c}\)</span>; es decir, una combinación lineal de <span
class="math inline">\(\mathbf{b}\)</span> y <span
class="math inline">\(\mathbf{c}\)</span>. Se encuentra en la recta que
generan.</p>
<p>Por proponer otro ejemplo, si alguna de las coordenadas fuera mayor
que 1, eso significaría que el punto estaría más allá del triángulo.</p>
<blockquote>
<p>TODO: dibujo con explicación de cómo funciona (libreta Shinrin -
Yoku)</p>
</blockquote>
<h4 data-number="1.2.4.2" id="calculando-la-intersección"><span
class="header-section-number">1.2.4.2</span> Calculando la
intersección</h4>
<p>Podemos eliminar una de las varibales escribiendo <span
class="math inline">\(\alpha = 1 - \beta - \gamma\)</span>, lo que nos
dice</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{p} &amp; = (1 - \beta - \gamma) \mathbf{a} + \beta \mathbf{b} +
\gamma \mathbf{c} \\
  &amp; = \mathbf{a} + (\mathbf{b} - \mathbf{a}) \beta + (\mathbf{c} -
\mathbf{a}) \gamma
\end{aligned}
\]</span></p>
<p>bajo la restricción</p>
<p><span id="eq:beta_gamma" class="eqnos"><span class="math display">\[
\begin{aligned}
\beta + \gamma &amp; &lt; 1 \\
0 &amp; &lt; \beta          \\
0 &amp; &lt; \gamma
\end{aligned}
\]</span><span class="eqnos-number">(3)</span></span></p>
<p>Un rayo <span class="math inline">\(P(t) = \mathbf{o} +
t\mathbf{d}\)</span> impactará en un punto del triángulo si se
cumple</p>
<p><span class="math display">\[
P(t) = \mathbf{o} + t\mathbf{d} = \mathbf{a} + (\mathbf{b} - \mathbf{a})
\beta + (\mathbf{c} - \mathbf{a}) \gamma
\]</span></p>
<p>cumpliendo [<a href="#eq:beta_gamma">3</a>]. Podemos expandir la
ecuación anterior en sus coordenadas para obtener</p>
<p><span class="math display">\[
\begin{aligned}
o_x + td_x &amp; = a_x + (b_x - a_x) \beta + (c_x - a_x) \gamma \\
o_y + td_y &amp; = a_y + (b_y - a_y) \beta + (c_y - a_y) \gamma \\
o_z + td_z &amp; = a_z + (b_z - a_z) \beta + (c_z - a_z) \gamma \\
\end{aligned}
\]</span></p>
<p>Reordenamos:</p>
<p><span class="math display">\[
\begin{aligned}
(a_x - b_x) \beta + (a_x - c_x) \gamma+ td_x &amp; = a_x - o_x \\
(a_y - b_y) \beta + (a_y - c_y) \gamma+ td_y &amp; = a_y - o_y \\
(a_z - b_z) \beta + (a_z - c_z) \gamma+ td_z &amp; = a_z - o_z
\end{aligned}
\]</span></p>
<p>Lo que nos permite escribir el sistema en forma de ecuación:</p>
<p><span class="math display">\[
\begin{pmatrix}
        a_x - b_x &amp; a_x - c_x &amp; d_x \\
        a_y - b_y &amp; a_y - c_y &amp; d_y \\
        a_z - b_z &amp; a_z - c_z &amp; d_z
\end{pmatrix}
\begin{pmatrix}
        \beta \\ \gamma \\ t
\end{pmatrix}
=
\begin{pmatrix}
        a_x - o_x \\ a_y - o_y \\ a_z - o_z
\end{pmatrix}
\]</span></p>
<p>Calcular rápidamente la solución a un sistema de ecuaciones lineales
es un problema habitual. En <span class="citation"
data-cites="ShirleyRRT">(<a href="#ref-ShirleyRRT"
role="doc-biblioref">Shirley and Morley 2003</a>)</span> se utiliza la
regla de Cramer para hacerlo, esperando que el compilador optimice las
variables intermedias creadas. Nosotros no nos tendremos que preocupar
de esto en particular, ya que el punto de impacto lo calculará la GPU
gracias a las herramientras aportadas por KHR <span class="citation"
data-cites="vulkan">(<a href="#ref-vulkan" role="doc-biblioref">The
Khronos Vulkan Working Group 2022</a>, Ray Traversal)</span>.</p>
<p>Para obtener el vector normal, podemos hacer el producto vectorial de
dos vectores que se encuentren en el plano del triángulo. como, por
convención, los vértices se guardan en sentido antihorario visto desde
fuera del objeto, entonces</p>
<p><span class="math display">\[
\mathbf{n} = (\mathbf{b} - \mathbf{a}) \times (\mathbf{c} - \mathbf{a})
\]</span></p>
<h1 data-number="2" id="transporte-de-luz"><span
class="header-section-number">2</span> Transporte de luz</h1>
<p>En este capítulo estudiaremos las bases de la radiometría. Esta área
de la óptica nos proporcionará una serie de herramientas con las cuales
podremos responder a la pregunta <em>cuánta luz existe en un
punto</em>.</p>
<h2 data-number="2.1" id="introducción-a-la-radiometría"><span
class="header-section-number">2.1</span> Introducción a la
radiometría</h2>
<p>Antes de comenzar a trabajar, necesitamos conocer <em>qué
entendemos</em> por luz. Aunque hay muchas formas de trabajar con ella
(a fin de cuentas, todavía seguimos discutiendo sobre qué es exactamente
la luz <a href="#fn1" class="footnote-ref" id="fnref1"
role="doc-noteref"><sup>1</sup></a>), nosotros nos quedaremos con la
definición clásica y algunas pinceladas de la cuántica. Nos será
suficiente utilizar el concepto de fotón.</p>
<p><strong>Un fotón</strong> es aquella particula emitida por una fuente
de iluminación. Estos fotones tienen una posición, una dirección de
propagación y una longitud de onda <span
class="math inline">\(\lambda\)</span> <span class="citation"
data-cites="ShirleyRRT">(<a href="#ref-ShirleyRRT"
role="doc-biblioref">Shirley and Morley 2003</a>)</span>; así como una
velocidad <span class="math inline">\(c\)</span> que depende del índice
de refracción del medio, <span class="math inline">\(n\)</span>. La
unidad de medida de <span class="math inline">\(\lambda\)</span> es el
nanómetro (<span class="math inline">\(\text{nm}\)</span>).</p>
<blockquote>
<p><strong>Nota</strong>:(ción) cuando usemos un paréntesis tras una
ecuación, dentro denotaremos sus unidades de medida.</p>
</blockquote>
<p>Necesitaremos también definir qué es <strong>la frecuencia</strong>,
<span class="math inline">\(f\)</span>. Su utilidad viene del hecho de
que, cuando la luz cambia de medio al propagarse, la frecuencia se
mantiene constante.</p>
<p><span id="eq:frecuencia" class="eqnos"><span class="math display">\[
f = \frac{c}{\lambda} (\text{Hz})
\]</span><span class="eqnos-number">(4)</span></span></p>
<p>Un fotón tiene asociada una <strong>carga de energía</strong>,
denotada por <span class="math inline">\(Q\)</span>:</p>
<p><span id="eq:carga_de_energia" class="eqnos"><span
class="math display">\[
Q = hf = \frac{hc}{\lambda} (\text{J})
\]</span><span class="eqnos-number">(5)</span></span></p>
<p>donde <span class="math inline">\(h = 6.62607004 \times 10^{-34}
\text{J} \cdot \text{s}\)</span> es la constante de Plank y <span
class="math inline">\(c = 299 792 458 \text{m/s}\)</span> la velocidad
de la luz.</p>
<p>En realidad, <strong>todas estas cantidades deberían tener un
subíndice <span class="math inline">\(\lambda\)</span></strong>, puesto
que dependen de la longitud de onda. La energía de un fotón <span
class="math inline">\(Q\)</span>, por ejemplo, debería denotarse <span
class="math inline">\(Q_\lambda\)</span>. Sin embargo, en la literatura
de informática gráfica, <strong>se ha optado por omitirla</strong>.
¡Tenlo en cuenta a partir de aquí!</p>
<h3 data-number="2.1.1" id="potencia"><span
class="header-section-number">2.1.1</span> Potencia</h3>
<p>A partir de la energía <span class="math inline">\(Q\)</span>,
podemos estimar la cantidad total de energía que pasa por una región del
espacio por unidad de tiempo. A esta tasa la llamaremos
<strong>potencia</strong>, o <strong>flujo radiante</strong> <span
class="math inline">\(\Phi\)</span> <span class="citation"
data-cites="PBRT3e">(<a href="#ref-PBRT3e" role="doc-biblioref">Pharr,
Jakob, and Humphreys 2016</a>, Radiometry)</span>. Esta medida nos
resultará más útil que la energía total, puesto que nos permite estimar
la energía en un instante:</p>
<p><span id="eq:potencia" class="eqnos"><span class="math display">\[
\Phi = \lim_{\Delta t \to 0}{\frac{\Delta Q}{\Delta t}} = \frac{dQ}{dt}
(J/s)
\]</span><span class="eqnos-number">(6)</span></span></p>
<p>Su unidad es julios por segundo, comúnmente denotado vatio
(<em>watts</em>, <span class="math inline">\(\text{W}\)</span>). También
se utiliza el lumen. Podemos encontrar la energía total en un periodo de
tiempo <span class="math inline">\([t_0, t_1]\)</span> integrando el
flujo radiante:</p>
<p><span class="math display">\[
Q = \int_{t_0}^{t_1}{\Phi(t)dt}
\]</span></p>
<h3 data-number="2.1.2" id="irradiancia"><span
class="header-section-number">2.1.2</span> Irradiancia</h3>
<p>La <strong>irradiancia</strong> o <strong>radiancia emitida</strong>
es la potencia por unidad de área que recibe una región de una
superficie o un punto de la misma. Si <span
class="math inline">\(A\)</span> es el área de dicha superficie, la
irradiancia se define como</p>
<p><span id="eq:irradiancia" class="eqnos"><span class="math display">\[
E = \frac{\Phi}{A} (\text{W/m}^2)
\]</span><span class="eqnos-number">(7)</span></span></p>
<div id="fig:irradiancia" class="fignos">
<figure>
<img loading="lazy" src="./img/02/Irradiancia.png"
alt="Figura 6: La irradiancia es la potencia por metro cuadrado incidente en una superficie. Es proporcional al coseno del ángulo entre la dirección de la luz y la normal a la superficie." />
<figcaption aria-hidden="true"><span>Figura 6:</span> La irradiancia es
la potencia por metro cuadrado incidente en una superficie. Es
proporcional al coseno del ángulo entre la dirección de la luz y la
normal a la superficie.</figcaption>
</figure>
</div>
<p>Ahora que tenemos la potencia emitida en una cierta área, nos surge
una pregunta: ¿y en un cierto punto <span
class="math inline">\(p\)</span>?.</p>
<p>Tomando límites en la expresión anterior, encontramos la
respuesta:</p>
<p><span id="eq:irradiancia_punto" class="eqnos"><span
class="math display">\[
E(p) = \lim_{\Delta A \to 0}{\frac{\Delta \Phi}{\Delta A}} =
\frac{d\Phi}{dA} (\text{W/m}^2)
\]</span><span class="eqnos-number">(8)</span></span></p>
<p>De la misma manera que con la potencia, integrando <span
class="math inline">\(E(p)\)</span> podemos obtener el flujo
radiante:</p>
<p><span class="math display">\[
\Phi = \int_{A}{E(p)dp}
\]</span></p>
<p>El principal problema de la irradiancia es que <em>no nos dice nada
sobre las direcciones</em> desde las que ha llegado la luz.</p>
<h3 data-number="2.1.3" id="ángulos-sólidos"><span
class="header-section-number">2.1.3</span> Ángulos sólidos</h3>
<p>Con estas tres unidades básicas, nos surge una pregunta muy natural:
<em>¿cómo mido cuánta luz llega a una superficie?</em></p>
<p>Para responder a esta pregunta, necesitaremos los <strong>ángulos
sólidos</strong>. Son la extensión de los ángulos bidimensionales a los
que estamos acostumbrados (llamados técnicamente <strong>ángulos
planares</strong>).</p>
<p>Ilustremos el sentido de estos ángulos: imaginemos que tenemos un
cierto objeto en dos dimensiones delante de nosotros, a una distancia
desconocida. ¿Sabríamos cuál es su tamaño, solo con esta información? Es
más, si entrara otro objeto en la escena, ¿podríamos distinguir cuál de
ellos es más grande?</p>
<p>Parece difícil responder a estas preguntas. Sin embargo, sí que
podemos determinar <em>cómo de grandes nos parecen</em> desde nuestro
punto de vista. Para ello, describimos una circunferencia de radio <span
class="math inline">\(r\)</span> alrededor nuestra. Si trazamos un par
de líneas desde nuestra posición a las partes más alejadas de este
objeto, y las cortamos con nuestra circunferencia, obtendremos un par de
puntos inscritos en ella. Pues bien, al arco que encapsulan dichos
puntos le vamos a hacer corresponder un cierto ángulo: el ángulo
planar.</p>
<div id="fig:angulo_planar" class="fignos">
<figure>
<img loading="lazy" src="./img/02/Ángulo%20planar.png" style="width:85.0%"
alt="Figura 7: La idea intuitiva de un ángulo planar" />
<figcaption aria-hidden="true"><span>Figura 7:</span> La idea intuitiva
de un ángulo planar</figcaption>
</figure>
</div>
<p>Llevando esta idea a las tres dimensiones es como conseguimos el
concepto de <strong>ángulo sólido</strong>. Si en dos dimensiones
teníamos una circunferencia, aquí tendremos una esfera. Cuando generemos
las rectas proyectantes hacia el volumen, a diferencia de los ángulos
planares, se inscribirá un área en la esfera. La razón entre dicha área
<span class="math inline">\(A\)</span> y el cuadrado del radio <span
class="math inline">\(r\)</span> nos dará un ángulo sólido:</p>
<p><span id="eq:angulo_solido_omega" class="eqnos"><span
class="math display">\[
\sigma = \frac{A}{r^2} \text{(sr)}
\]</span><span class="eqnos-number">(9)</span></span></p>
<p>Los denotaremos por <span class="math inline">\(\sigma\)</span>,
aunque también se pueden encontrar en la literatura como <span
class="math inline">\(\Omega\)</span>. Su unidad de medida es el
estereorradián (<span class="math inline">\(\text{sr}\)</span>). Se
tiene que <span class="math inline">\(\sigma \in [0, 4\pi]\)</span>. Un
esteorradián corresponde a una superficie con área <span
class="math inline">\(r^2\)</span>: <span class="math inline">\(1
\text{sr} = \frac{r^2}{r^2}\)</span>.</p>
<div id="fig:angulo_solido" class="fignos">
<figure>
<img loading="lazy" src="./img/02/Ángulo%20sólido.png" style="width:80.0%"
alt="Figura 8: Un ángulo sólido es la razón entre el área proyectada y el cuadrado del radio" />
<figcaption aria-hidden="true"><span>Figura 8:</span> Un ángulo sólido
es la razón entre el área proyectada y el cuadrado del
radio</figcaption>
</figure>
</div>
<p>Usaremos <span class="math inline">\(\omega\)</span> para representar
<strong>vectores dirección unitarios en la esfera</strong> alrededor de
un punto <span class="math inline">\(p\)</span>.</p>
<div id="fig:xkcd_1276" class="fignos">
<figure>
<img loading="lazy" src="./img/02/xkcd_1276.png" style="width:60.0%"
alt="Figura 9: Como de costumbre, hay un XKCD relevante (Munroe n.d.)" />
<figcaption aria-hidden="true"><span>Figura 9:</span> Como de costumbre,
hay un XKCD relevante <span class="citation" data-cites="xkcd-size">(<a
href="#ref-xkcd-size" role="doc-biblioref">Munroe
n.d.</a>)</span></figcaption>
</figure>
</div>
<p>Puesto que estamos trabajando con esferas, nos resultará muy cómodo
emplear coordenadas esféricas. Para un cierto punto de coordenadas <span
class="math inline">\((x, y, z)\)</span> de la esfera unitaria, se tiene
que</p>
<p><span id="eq:coordenadas_esféricas" class="eqnos"><span
class="math display">\[
\begin{aligned}
    \begin{cases}
        x = \sin\theta\cos\phi \\
        y = \sin\theta\sin\phi \\
        z = \cos\theta
    \end{cases}
\end{aligned}
\]</span><span class="eqnos-number">(10)</span></span></p>
<p>A <span class="math inline">\(\theta\)</span> se le denomina ángulo
polar, mientras que a <span class="math inline">\(\phi\)</span> se le
llama acimut. Imaginémonos un punto en la esfera de radio <span
class="math inline">\(r\)</span> ubicado en una posición <span
class="math inline">\((r, \theta, \phi)\)</span>. Queremos calcular un
área chiquitita <span class="math inline">\(dA_h\)</span>, de forma que
el ángulo sólido asociado a dicha área debe ser <span
class="math inline">\(d\sigma\)</span>. Así, <span
class="math inline">\(d\sigma = \frac{dA_h}{r^2}\)</span>. Si
proyectamos el área, obtenemos <span
class="math inline">\(d\theta\)</span> y <span
class="math inline">\(d\phi\)</span>: pequeños cambios en los ángulos
que nos generan nuestra pequeña área <span class="citation"
data-cites="berkeley-cs184">(<a href="#ref-berkeley-cs184"
role="doc-biblioref">Berkeley cs184 2022</a>, Radiometry &amp;
Photometry)</span>.</p>
<p><span class="math inline">\(dA_h\)</span> debe tener dos lados <span
class="math inline">\(lado_1\)</span> y <span
class="math inline">\(lado_2\)</span>. Podemos hallar <span
class="math inline">\(lado_1\)</span> si lo trasladamos al eje <span
class="math inline">\(z\)</span> de nuevo. Así, <span
class="math inline">\(lado_1 = r \sin d\theta\)</span>. De la misma
manera, <span class="math inline">\(lado_2 = r d\theta\)</span>.</p>
<blockquote>
<p>TODO: foto que explique todo esto, porque si no, no hay quien se
entere. Quizás me sirva la de
https://cs184.eecs.berkeley.edu/public/sp22/lectures/lec-11-radiometry-and-photometry/lec-11-radiometry-and-photometry.pdf,
p.16 siempre que adapte <span class="math inline">\(\phi\)</span>.</p>
</blockquote>
<p>Poniendo estos valores en <span
class="math inline">\(d\sigma\)</span>:</p>
<p><span id="eq:d_omega" class="eqnos"><span class="math display">\[
\begin{aligned}
d\sigma &amp; = \frac{dA_h}{r^2} = \frac{lado_1 lado_2}{r^2} = \\
        &amp; = \frac{r \sin\theta\ d\phi\ r\ d\theta}{r^2} = \\
        &amp; = \sin\theta\ d\theta\ d\phi
\end{aligned}
\]</span><span class="eqnos-number">(11)</span></span></p>
<p>¡Genial! Acabamos de añadir un recurso muy potente a nuestro
inventario. Esta expresión nos permitirá convertir integrales sobre
ángulos sólidos en integrales sobre ángulos esféricos. Por ejemplo, este
sería el caso de la esfera.</p>
<p>Supongamos que queremos encontrar el ángulo sólido correspondiente al
conjunto de todas las direcciones sobre la esfera <span
class="math inline">\(\mathbb{S}^2\)</span>. Se tiene que</p>
<p><span class="math display">\[
\begin{aligned}
\sigma &amp; = \int_{\mathbb{S}^2}{d\omega} = \int_{0}^{2\pi}
\int_{0}^{\pi} {\sin\theta\ d\theta d\phi} = \\
       &amp; = 4\pi \text{ sr}
\end{aligned}
\]</span></p>
<p>Además, esto nos dice que un hemisferio de la esfera corresponde a
<span class="math inline">\(2\pi\)</span> estereorradianes.</p>
<p>En la práctica es muy común considerar el ángulo sólido asociado a
una dirección en la esfera <span class="math inline">\(\omega\)</span>,
el cual se denota por <span class="math inline">\(d\omega\)</span>. A
partir de este punto usaremos esa notación.</p>
<h3 data-number="2.1.4" id="intensidad-radiante"><span
class="header-section-number">2.1.4</span> Intensidad radiante</h3>
<p>Los ángulos sólidos nos proporcionan una variedad de herramientas
nuevas considerable. Gracias a ellos, podemos desarrollar algunos
conceptos nuevos. Uno de ellos es la <strong>intensidad
radiante</strong>.</p>
<p>Imaginémonos un pequeñito punto de luz encerrado en una esfera, el
cual emite fotones en todas direcciones (es decir, su ángulo sólido es
el de la esfera, <span class="math inline">\(4\pi\)</span>). Nos
gustaría medir cuánta energía pasa por la esfera. Podríamos entonces
definir</p>
<p><span class="math display">\[
I = \frac{\Phi}{4\pi} \text{(W/sr)}
\]</span></p>
<p>Si en vez de utilizar toda la esfera, <em>cerramos</em> el ángulo lo
máximo posible, nos estaríamos restringiendo a un área extremadamente
pequeña, lo cual nos proporcionaría la densidad angular de flujo
radiante:</p>
<p><span id="eq:intensidad_radiante" class="eqnos"><span
class="math display">\[
I = \lim_{\Delta\omega \to 0}{\frac{\Delta\Phi}{\Delta\omega}} =
\frac{d\Phi}{d\omega}
\]</span><span class="eqnos-number">(12)</span></span></p>
<p>De la misma manera que con los conceptos anteriores, podemos volver a
la potencia integrando sobre un conjunto de direcciones <span
class="math inline">\(\Omega\)</span> de la esfera:</p>
<p><span class="math display">\[
\Phi = \int_{\Omega}{I(\omega)d\omega}
\]</span></p>
<h3 data-number="2.1.5" id="radiancia"><span
class="header-section-number">2.1.5</span> Radiancia</h3>
<p>Finalmente, llegamos a uno de los conceptos más importantes de esta
sección, el cual es una extensión de la radiancia emitida teniendo en
cuenta la dirección de la luz. La <strong>radiancia espectral</strong>
(o radiancia a secas<a href="#fn2" class="footnote-ref" id="fnref2"
role="doc-noteref"><sup>2</sup></a>) <span class="citation"
data-cites="PBRT3e">(<a href="#ref-PBRT3e" role="doc-biblioref">Pharr,
Jakob, and Humphreys 2016</a>, Radiometry)</span>, denotada por <span
class="math inline">\(L(p, \omega)\)</span>, es la irradiancia por
unidad de ángulo sólido <span class="math inline">\(d\omega\)</span>
asociado a una dirección <span
class="math inline">\(\omega\)</span>:</p>
<p><span id="eq:radiancia_irradiancia" class="eqnos"><span
class="math display">\[
L(p, \omega) = \lim_{\Delta\omega \to 0}{\frac{\Delta
E_\omega(p)}{\Delta\omega}} = \frac{dE_\omega(p)}{d\omega}
\]</span><span class="eqnos-number">(13)</span></span></p>
<p>Expandiendo esta expresión, se tiene que la radiancia también es la
potencia que pasa por un punto <span class="math inline">\(p\)</span> y
viaja en una dirección <span class="math inline">\(\omega\)</span> por
unidad de área (perpendicular a <span
class="math inline">\(\omega\)</span>) alrededor de <span
class="math inline">\(p\)</span>, por unidad de ángulo sólido <span
class="math inline">\(d\omega\)</span>:</p>
<p><span id="eq:radiancia_flujo" class="eqnos"><span
class="math display">\[
L(p, \omega) = \frac{d^2\Phi(p, \omega)}{d\omega\ dA^\bot} =
\frac{d^2\Phi(p, \omega)}{d\omega\ dA\ \cos\theta}
\]</span><span class="eqnos-number">(14)</span></span></p>
<p>donde <span class="math inline">\(dA^\bot\)</span> es el área
proyectada por <span class="math inline">\(dA\)</span> en una hipotética
superficie perpendicular a <span
class="math inline">\(\omega\)</span>:</p>
<blockquote>
<p>TODO: figura similar a pbr figura 5.10
https://www.pbr-book.org/3ed-2018/Color_and_Radiometry/Radiometry</p>
<p>TODO: foto como la de
https://cs184.eecs.berkeley.edu/public/sp22/lectures/lec-11-radiometry-and-photometry/lec-11-radiometry-and-photometry.pdf,
página 10.</p>
</blockquote>
<p>Cuando un rayo impacta en una superficie, <span
class="math inline">\(L\)</span> puede tomar valores muy diferentes en
un lado y otro de esta. A fin de cuentas, necesitamos distinguir entre
los fotones que llegan a la superficie y los que salen. Para
solucionarlo podemos distinguir entre la radiancia que llega a un punto
–la incidente–, y la saliente.</p>
<p>A la <strong>radiancia incidente o entrante</strong> la llamaremos
<span class="math inline">\(L_i(p, \omega)\)</span>, mientras que la
<strong>radiancia saliente</strong> se denomina <span
class="math inline">\(L_o(p, \omega)\)</span>.</p>
<p>Es importante destacar que <span
class="math inline">\(\omega\)</span> apunta <em>hacia fuera</em> de la
superficie. Para <span class="math inline">\(L_o(p, \omega)\)</span> se
tiene que la radiancia viaja en el sentido de <span
class="math inline">\(\omega\)</span>, mientras que para <span
class="math inline">\(L_i(p, \omega)\)</span> se tiene que la radiancia
viaja en el sentido contrario a <span
class="math inline">\(\omega\)</span>; es decir, hacia el punto <span
class="math inline">\(p\)</span>.</p>
<blockquote>
<p>TODO:
https://cs184.eecs.berkeley.edu/public/sp22/lectures/lec-11-radiometry-and-photometry/lec-11-radiometry-and-photometry.pdf,
p.36</p>
</blockquote>
<p>Una propiedad a tener en cuenta es que, si cogemos un punto <span
class="math inline">\(p\)</span> del espacio donde no existe ninguna
superifcie, <span class="math inline">\(L_o(p, \omega) = L_i(p, -\omega)
= L(p, \omega)\)</span></p>
<p>La importancia de la radiancia se debe a un par de propiedades:</p>
<p>La primera de ellas es que, dado <span
class="math inline">\(L\)</span>, podemos calcular cualquier otra unidad
básica mediante integración. Además, <strong>su valor se mantiene
constante en rayos que viajan en el vacío en línea recta</strong> <span
class="citation" data-cites="pellacini-marschner-2017">(<a
href="#ref-pellacini-marschner-2017" role="doc-biblioref">Fabio
Pellacini 2022</a>)</span>. Esto último hace que resulte muy natural
usarla en un ray tracer.</p>
<p>Veamos por qué ocurre esto:</p>
<blockquote>
<p>TODO:
https://pellacini.di.uniroma1.it/teaching/graphics17b/lectures/12_pathtracing.pdf,
página 18.</p>
</blockquote>
<p>Consideremos dos superficies ortogonales entre sí, <span
class="math inline">\(S_1\)</span> y <span
class="math inline">\(S_2\)</span> separadas una distancia <span
class="math inline">\(r\)</span>. Debido a la conservación de la
energía, cualquier fotón que salga de una superficie y se encuentre bajo
el ángulo sólido de la otra debe llegar impactar en dicha superficie
opuesta.</p>
<p>Por tanto:</p>
<p><span class="math display">\[
d^2\Phi_1 = d^2\Phi_2
\]</span></p>
<p>Sustituyendo en la expresión de la radiancia [<a
href="#eq:radiancia_flujo">14</a>], y teniendo en cuenta que son
ortogonales (lo que nos dice que <span class="math inline">\(\cos\theta
= 1\)</span>):</p>
<p><span class="math display">\[
L_1 d\omega_1 dA_1 = L_2 d\omega_2 dA_2
\]</span></p>
<p>Por construcción, podemos cambiar los ángulos sólidos:</p>
<p><span class="math display">\[
L_1 \frac{dA_2}{r^2} dA_1 = L_2 \frac{dA_1}{r^2} dA_2
\]</span></p>
<p>Lo que finalmente nos dice que <span class="math inline">\(L_1 =
L_2\)</span>, como queríamos ver.</p>
<h3 data-number="2.1.6" id="integrales-radiométricas"><span
class="header-section-number">2.1.6</span> Integrales radiométricas</h3>
<p>En esta sección, vamos a explorar las nuevas herramientas que nos
proporciona la radiancia. Veremos también cómo integrar ángulos sólidos,
y cómo simplificar dichas integrales.</p>
<h4 data-number="2.1.6.1"
id="una-nueva-expresión-de-la-irradiancia-y-el-flujo"><span
class="header-section-number">2.1.6.1</span> Una nueva expresión de la
irradiancia y el flujo</h4>
<p>Como dijimos al final de <a href="#irradiancia">la sección de la
irradiancia</a>, esta medida no tiene en cuenta las direcciones desde
las que llegaba la luz. A diferencia de esta, la radiancia sí que las
utiliza. Dado que una de las ventajas de la radiancia es que nos permite
obtener el resto de medidas radiométricas, ¿por qué no desarrollamos una
nueva expresión de la irradiancia?</p>
<p>Para obtener cuánta luz llega a un punto, debemos acumular la
radiancia incidente que nos llega desde cualquier dirección.</p>
<blockquote>
<p>TODO: dibujo como el de la libreta roja. Me lo mandé por Telegram,
por si no lo encuentro</p>
</blockquote>
<p>Dado un punto <span class="math inline">\(p\)</span> que se encuentra
en una superficie con normal <span
class="math inline">\(\mathbf{n}\)</span> en dicho punto, la irradiancia
se puede expresar como <span class="citation" data-cites="PBRT3e">(<a
href="#ref-PBRT3e" role="doc-biblioref">Pharr, Jakob, and Humphreys
2016</a>, Working with Radiometric Integrals)</span></p>
<p><span id="eq:E_abs_cos" class="eqnos"><span class="math display">\[
E(p, \mathbf{n}) = \int_{\mathbb{S}^2}{L_i(p, \omega) \left\lvert
cos\theta \right\rvert d\omega}
\]</span><span class="eqnos-number">(15)</span></span></p>
<p>Siendo <span class="math inline">\(\Omega\)</span> un subcojunto de
direcciones de la esfera <span
class="math inline">\(\mathbb{S}^2\)</span>. El término <span
class="math inline">\(\cos\theta\)</span> aparece en la integral debido
a la derivada del área proyectada, <span
class="math inline">\(dA^\bot\)</span>. <span
class="math inline">\(\theta\)</span> es el ángulo entre la dirección
<span class="math inline">\(\omega\)</span> y la normal <span
class="math inline">\(\mathbf{n}\)</span>.</p>
<p>Generalmente, la irradiancia se calcula únicamente en el hemisferio
de direcciones asociado a la normal en el punto, <span
class="math inline">\(H^2(\mathbf{n})\)</span>.</p>
<p>Podemos eliminar el <span class="math inline">\(\cos\theta\)</span>
de la integral mediante una pequeña transformación: proyectando el
ángulo sólido sobre el disco alrededor del punto <span
class="math inline">\(p\)</span> con normal <span
class="math inline">\(\mathbf{n}\)</span>, obtenemos una expresión más
sencilla: como <span class="math inline">\(d\omega^\bot = \left\lvert
\cos\theta \right\rvert d\omega\)</span>, entonces</p>
<p><span class="math display">\[
\begin{aligned}
    E(p, \mathbf{n}) = \int_{H^2(\mathbf{n})}{L_i(p, \omega)
d\omega^\bot}
\end{aligned}
\]</span></p>
<p>Usando lo que aprendimos sobre la derivada de los ángulos sólidos [<a
href="#eq:d_omega">11</a>], se puede reescribir la ecuación anterior
como</p>
<p><span class="math display">\[
E(p, \mathbf{n}) = \int_{0}^{2\pi}\int_{0}^{\pi/2}{L_i(p, \theta, \phi)
\cos\theta\ \sin\theta\ d\theta\ d\phi}
\]</span></p>
<p>Si integramos esta expresión para todos los puntos de una superficie
<span class="math inline">\(A\)</span>, obtenemos la potencia total
saliente de esa superficie en todas las direcciones:</p>
<p><span class="math display">\[
\begin{aligned}
    \Phi &amp; = \int_{A}\int_{H^2(\mathbf{n})}{L_o(p, \omega)
\cos\theta\ d\omega dA} = \\
         &amp; = \int_{A}\int_{H^2(\mathbf{n})}{L_o(p, \omega)
d\omega^\bot dA}
\end{aligned}
\]</span></p>
<blockquote>
<p>TODO: a lo mejor merece la pena hacer un ejemplo sobre los diferentes
tipos de luz, como en
https://cs184.eecs.berkeley.edu/public/sp22/lectures/lec-11-radiometry-and-photometry/lec-11-radiometry-and-photometry.pdf
p.41? O a lo mejor un capítulo para hablar de luces en general.</p>
</blockquote>
<h4 data-number="2.1.6.2" id="integrando-sobre-área"><span
class="header-section-number">2.1.6.2</span> Integrando sobre área</h4>
<p>Una herramienta más que nos vendrá bien será la capacidad de
convertir integrales sobre direcciones en integrales sobre área. Hemos
hecho algo similar en las secciones anteriores, así que no perdemos nada
por generalizarlo.</p>
<p>Considera un punto <span class="math inline">\(p\)</span> sobre una
superficie con normal en dicho punto <span
class="math inline">\(\mathbf{n}\)</span>. Supongamos que tenemos una
pequeña área <span class="math inline">\(dA\)</span> con normal <span
class="math inline">\(\mathbf{n_{dA}}\)</span>. Sea <span
class="math inline">\(\theta\)</span> el ángulo entre <span
class="math inline">\(\mathbf{n}\)</span> y <span
class="math inline">\(\mathbf{n_{dA}}\)</span>, y <span
class="math inline">\(r\)</span> la distancia entre <span
class="math inline">\(p\)</span> y <span
class="math inline">\(dA\)</span>.</p>
<p>Entonces, la relación entre la diferencial de un ángulo sólido y la
de un área es</p>
<p><span id="eq:diferencial_angulo_solido" class="eqnos"><span
class="math display">\[
d\omega = \frac{dA\cos\theta}{r^2}
\]</span><span class="eqnos-number">(16)</span></span></p>
<blockquote>
<p>TODO: figura como la de pbr book 5.16.</p>
</blockquote>
<p>Esto nos permite, por ejemplo, expandir algunas expresiones como la
de la irradiancia [<a href="#eq:E_abs_cos">15</a>] si partimos de un
cuadrilátero <span class="math inline">\(dA\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
    E(p, \mathbf{n}) &amp; = \int_{\mathbb{S}^2}{L_i(p, \omega)
\left\lvert \cos\theta \right\rvert d\omega} = \\
                     &amp; = \int_{A}{L\cos\theta\
\frac{\cos\theta_o}{r^2}dA}
\end{aligned}
\]</span></p>
<p>siendo <span class="math inline">\(\theta_o\)</span> el ángulo de la
radiancia de salida de la superficie del cuadrilátero.</p>
<h3 data-number="2.1.7" id="fotometría-y-radiometría"><span
class="header-section-number">2.1.7</span> Fotometría y radiometría</h3>
<blockquote>
<p>TODO: hablar sobre las diferencias. Hay información útil en
01_lights.pdf, p.43</p>
</blockquote>
<h2 data-number="2.2" id="dispersión-de-luz"><span
class="header-section-number">2.2</span> Dispersión de luz</h2>
<p>Cuando la luz impacta en una superficie, ocurren un par de sucesos:
parte de los fotones se reflejan saliendo disparados hacia alguna
dirección, mientras que otros se absorben. La forma en la que se
comportan depende de cómo sea la superficie. Específicamente, del
material del que esté hecha.</p>
<p>En informática gráfica se consideran tres tipos principales de
dispersión de luz: <strong>dispersión y reflexión en
superficies</strong> (<em>surface scattering</em>), <strong>dispersión
volumétrica</strong> (<em>volumetric scattering</em>) y
<strong>dispersión bajo superficie</strong> (<em>subsurface
scattering</em>)</p>
<p>En este capítulo vamos a modelar la primera. Estudiaremos qué es lo
que ocurre cuando los fotones alcanzan una superficie, en qué dirección
se reflejan, y cómo cambia el comportamiento dependiendo de las
propiedades del material.</p>
<h3 data-number="2.2.1"
id="la-función-de-distribución-de-reflectancia-bidireccional-brdf"><span
class="header-section-number">2.2.1</span> La función de distribución de
reflectancia bidireccional (BRDF)</h3>
<p>La <strong>función de distribución de reflectancia
bidireccional</strong> (en inglés, <em>bidirectional reflectance
distribution function</em>, BRDF) <span class="citation"
data-cites="PBRT3e">(<a href="#ref-PBRT3e" role="doc-biblioref">Pharr,
Jakob, and Humphreys 2016</a>, Surface Reflection)</span> describe cómo
la luz se refleja en una superficie opaca. Se encarga de informarnos
sobre cuánta radiancia sale en dirección <span
class="math inline">\(\omega_o\)</span> debido a la radiancia incidente
desde la dirección <span class="math inline">\(\omega_i\)</span>,
partiendo de un punto <span class="math inline">\(p\)</span> en una
superficie con normal <span class="math inline">\(\mathbf{n}\)</span>.
Depende de la longitud de onda <span
class="math inline">\(\lambda\)</span>, pero, como de costumbre, la
omitiremos.</p>
<blockquote>
<p><strong>Intuición</strong>: <em>¿cuál es la probabilidad de que,
habiéndome llegado un fotón desde <span
class="math inline">\(\omega_i\)</span>, me salga disparado hacia <span
class="math inline">\(\omega_o\)</span>?</em></p>
</blockquote>
<blockquote>
<p>TODO: esquema como el de pbr fig. 5.18, o como
https://pellacini.di.uniroma1.it/teaching/graphics17b/lectures/12_pathtracing.pdf
p.20</p>
</blockquote>
<p>Si consideramos <span class="math inline">\(\omega_i\)</span> como un
cono diferencial de direcciones, la irradiancia diferencial en <span
class="math inline">\(p\)</span> viene dada por</p>
<p><span class="math display">\[
dE(p, \omega_i) = L_i(p, \omega_i) \cos\theta_i\ d\omega_i
\]</span></p>
<p>Debido a esta irradiancia, una pequeña parte de radiancia saldrá en
dirección <span class="math inline">\(\omega_o\)</span>, proporcional a
la irradiancia:</p>
<p><span class="math display">\[
dL_o(p, \omega_o) \propto dE(p, \omega_i)
\]</span></p>
<p>Si lo ponemos en forma de cociente, sabremos exactamente cuál es la
proporción de luz. A este cociente lo llamaremos <span
class="math inline">\(f_r(p, \omega_o \leftarrow \omega_i)\)</span>; la
función de distribución de reflectancia bidireccional:</p>
<p><span class="math display">\[
f_r(p, \omega_o \leftarrow \omega_i) = \frac{dL_o(p, \omega_o)}{dE(p,
\omega_i)} = \frac{dL_o(p, \omega_o)}{L_i(p, \omega_i) \cos\theta_i\
d\omega_i} (\text{sr}^{-1})
\]</span></p>
<blockquote>
<p><strong>Nota</strong>(ción): dependiendo de la fuente que estés
leyendo, es posible que te encuentres una integral algo diferente. Por
ejemplo, en tanto en Wikipedia como en <span class="citation"
data-cites="ShirleyRRT">(<a href="#ref-ShirleyRRT"
role="doc-biblioref">Shirley and Morley 2003</a>)</span> se integra con
respecto a los ángulos de salida <span
class="math inline">\(\omega_o\)</span>, en vez de los incidentes.</p>
<p>Aquí, usaremos la notación de integrar con respecto a los incidentes,
como se hace en <span class="citation" data-cites="PBRT3e">(<a
href="#ref-PBRT3e" role="doc-biblioref">Pharr, Jakob, and Humphreys
2016</a>)</span>.</p>
</blockquote>
<p>Las BRDF físicamente realistas tienen un par de propiedades
importantes:</p>
<ol type="1">
<li><strong>Reciprocidad</strong>: para cualquier par de direcciones
<span class="math inline">\(\omega_i\)</span>, <span
class="math inline">\(\omega_o\)</span>, se tiene que <span
class="math inline">\(f_r(p, \omega_i \leftarrow \omega_o) =\ \)</span>
<span class="math inline">\(f_r(p, \omega_o \leftarrow
\omega_i)\)</span>.</li>
<li><strong>Conservación de la energía</strong>: La energía reflejada
tiene que ser menor o igual que la incidente:</li>
</ol>
<p><span class="math display">\[
\int_{H^2(\mathbf{n})}{f_r(p, \omega_o \leftarrow \omega_i)
\cos\theta_i\ d\omega_i} \leq 1
\]</span></p>
<h3 data-number="2.2.2"
id="la-función-de-distribución-de-transmitancia-bidireccional-btdf"><span
class="header-section-number">2.2.2</span> La función de distribución de
transmitancia bidireccional (BTDF)</h3>
<p>Si la BRDF describe cómo se refleja la luz, la <em>bidirectional
transmittance distribution function</em> (abreviada BTDF) nos informará
sobre la transmitancia; es decir, cómo se comporta la luz cuando
<em>entra</em> en un medio. Generalmente serán dos caras de la misma
moneda: cuando la luz impacta en una superficie, parte de ella, se
reflejará, y otra parte se transmitirá.</p>
<p>Puedes imaginarte la BTDF como una función de reflectancia del
hemisferio opuesto a donde se encuentra la normal de la superficie.</p>
<p>Denotaremos a la BTDF por</p>
<p><span class="math display">\[
f_t(p, \omega_o \leftarrow \omega_i)
\]</span></p>
<p>Al contrario que en la BRDF, <span
class="math inline">\(\omega_o\)</span> y <span
class="math inline">\(\omega_i\)</span> se encuentran en hemisferios
diferentes.</p>
<h3 data-number="2.2.3"
id="la-función-de-distribución-de-dispersión-bidireccional-bsdf"><span
class="header-section-number">2.2.3</span> La función de distribución de
dispersión bidireccional (BSDF)</h3>
<p>Convenientemente, podemos unir la BRDF y la BTDF en una sola
expresión, llamada <strong>la función de distribución de dispersión
bidireccional</strong> (<em>bidirectional scattering distribution
function</em>, BSDF). A la BSDF la denotaremos por</p>
<p><span class="math display">\[
f(p, \omega_o \leftarrow \omega_i)
\]</span></p>
<blockquote>
<p><strong>Nota</strong>(ción): también se suele utilizar BxDF en vez de
BSDF.</p>
</blockquote>
<p>Usando esta definición, podemos obtener</p>
<p><span class="math display">\[
dL_o(p, \omega_o) = f(p, \omega_o \leftarrow \omega_i) L_i(p, \omega_i)
\left\lvert \cos\theta_i \right\rvert d\omega_i
\]</span></p>
<p>Esto nos deja a punto de caramelo una nueva expresión de la
randiancia en términos de la randiancia incidente en un punto <span
class="math inline">\(p\)</span>. Integrando la expresión anterior,
obtenemos</p>
<p><span id="eq:scattering_equation" class="eqnos"><span
class="math display">\[
L_o(p, \omega_o) = \int_{\mathbb{S}^2}{f(p, \omega_o \leftarrow
\omega_i)L_i(p, \omega_i)\left\lvert \cos\theta_i \right\rvert
d\omega_i}
\]</span><span class="eqnos-number">(17)</span></span></p>
<p>siendo <span class="math inline">\(\mathbb{S}^2\)</span> la
esfera.</p>
<blockquote>
<p><strong>Intuición:</strong> <em>la BSDF son todas las posibles
direcciones en las que puede salir disparada la luz.</em></p>
</blockquote>
<p>Esta forma de expresar la radiancia es muy importante. Generalmente
se le suele llamar la <em>ecuación de dispersión</em> (<em>scattering
equation</em>, en inglés). Dado que es una integral muy importante,
seguramente tengamos que evaluarla repetidamente. ¡Los métodos de Monte
Carlo nos vendrán de perlas! Más adelante hablaremos de ella.</p>
<p>Las BSDFs tienen unas propiedades interesantes:</p>
<ul>
<li><strong>Positividad</strong>: como los fotones no se pueden reflejar
“negativamente”, <span class="math inline">\(f(p, \omega_o \leftarrow
\omega_i) \ge 0\)</span>.</li>
<li><strong>Reciprocidad de Helmotz:</strong> se puede invertir la
dirección de un rayo: <span class="math inline">\(f(p, \omega_o
\leftarrow \omega_i) = f(p, \omega_i \leftarrow \omega_o)\)</span>.</li>
<li><strong>White furnace test</strong>: Toda la luz incidente debe ser
reflejada cuando la reflectividad de la superficie es 1.</li>
<li><strong>Conservación de la energía</strong>: todos los fotones que
llegan a la superficie deben ser reflejados o absorbidos. Es decir, no
se emite ningún fotón nuevo:</li>
</ul>
<p><span class="math display">\[
\int_{H^2(\mathbf{n})}{f(p, \omega_o \leftarrow \omega_i) \cos\theta_i\
d\omega_i} \le 1\ \forall \omega_o
\]</span></p>
<h3 data-number="2.2.4" id="reflectancia-hemisférica"><span
class="header-section-number">2.2.4</span> Reflectancia hemisférica</h3>
<p>Puede ser útil tomar el comportamiento agregado de las BRDFs y las
BTDFs y reducirlo un cierto valor que describa su comportamiento general
de dispersión. Sería algo así como un resumen de su distribución. Para
conseguirlo, vamos a introducir dos nuevas funciones:</p>
<p>El <strong>albedo</strong> <span class="citation"
data-cites="Szirmay-Kalos00monte-carlomethods">(<a
href="#ref-Szirmay-Kalos00monte-carlomethods"
role="doc-biblioref">Szirmay-Kalos 2000</a>)</span>, también conocido
como la <strong>reflectancia hemisférica-direccional</strong>
(<em>hemispherical-directional reflectance</em>) <span class="citation"
data-cites="PBRT3e">(<a href="#ref-PBRT3e" role="doc-biblioref">Pharr,
Jakob, and Humphreys 2016</a>, Reflection Models, Basic
Interface)</span> describe la radiancia saliente en la dirección <span
class="math inline">\(\omega_o\)</span>, supuesto que la radiancia
entrante desde cualquier dirección es constante e igual a la unidad:</p>
<p><span class="math display">\[
\rho_{hd}(\omega_o) = \int_{H^2(n)}{f_r(p, \omega_o \leftarrow \omega_i)
\left\lvert \cos\theta_i \right\rvert\ d\omega_i}
\]</span></p>
<p>Por otra parte, la <strong>reflectancia
hemisférica-hemisférica</strong> (<em>hemispherical-hemispherical
reflectance</em>) es un valor espectral que nos proporciona el ratio de
luz incidente reflejada por una superficie, suponiendo que llega la
misma luz desde todas direcciones:</p>
<p><span class="math display">\[
\rho_{hh} = \frac{1}{\pi} \int_{H^2(n)} \int_{H^2(n)}{f_r(p, \omega_o
\leftarrow \omega_i) \left\lvert \cos\theta_o\ \cos\theta_i
\right\rvert\ d\omega_o\ d\omega_i}
\]</span></p>
<h2 data-number="2.3" id="modelos-ópticos-de-materiales"><span
class="header-section-number">2.3</span> Modelos ópticos de
materiales</h2>
<p>En la práctica, cada superficie tendrá una BSDF característica. Esto
hace que la luz adquiera una dirección particular al incidir en cada
punto de esta. En esta sección, vamos a tratar algunas BSDFs
particulares e introduciremos las fórmulas fundamentales que se usan en
los modelos de materiales (también conocidos como modelos de
<em>shading</em>).</p>
<p>Los tipos de materiales que vamos a tratar son las básicos. Entre
ellos, se encuentran la difusa lambertiana, materiales dieléctricos,
espejos y algunas BSDFs compuestas. Un repertorio de implementaciones se
encuentra en el repositorio de BRDFs de <span class="citation"
data-cites="disney-brdfs">(<a href="#ref-disney-brdfs"
role="doc-biblioref">Walt Disney Animation Studios 2019</a>)</span>.</p>
<h3 data-number="2.3.1" id="tipos-de-dispersión"><span
class="header-section-number">2.3.1</span> Tipos de dispersión</h3>
<p>Prácticamente toda superficie, en mayor o menor medida, refleja parte
de la luz incidente. Otros tipos de materiales reflejan y refractan a la
vez, como puede ser un espejo o el agua.</p>
<div id="fig:refraccion" class="fignos">
<figure>
<img loading="lazy" src="./img/02/Reflexión%20y%20refracción.png" style="width:50.0%"
alt="Figura 10: Reflexión y refracción de luz (Haines 2021, 106)." />
<figcaption aria-hidden="true"><span>Figura 10:</span> Reflexión y
refracción de luz <span class="citation"
data-cites="GemsII-Reflexion">(<a href="#ref-GemsII-Reflexion"
role="doc-biblioref">Haines 2021, 106</a>)</span>.</figcaption>
</figure>
</div>
<blockquote>
<p>TODO: cambiar por foto propia</p>
</blockquote>
<p>En esencia, los reflejos se pueden clasificar en cuatro grandes tipos
<span class="citation" data-cites="McGuire2018GraphicsCodex">(<a
href="#ref-McGuire2018GraphicsCodex" role="doc-biblioref">McGuire
2021</a>, Materials)</span>:</p>
<ul>
<li><strong>Difusos</strong> (<em>Diffuse</em>): esparcen la luz en
todas direcciones casi equiprobablemente. Por ejemplo, la tela y el
papel son materiales difusos.</li>
<li><strong>Especulares brillantes</strong> (<em>Glossy specular</em>):
la distribución de luz se asemeja a un cono. La chapa de un coche es un
material especular brillante.</li>
<li><strong>Especulares perfectos</strong> (<em>Perfect specular</em>):
en esencia, son espejos. El ángulo de salida de la luz es muy pequeño,
por lo que reflejan casi a la perfección lo que les llega.</li>
<li><strong>Retrorreflectores</strong> (<em>Retro reflective</em>): la
luz se refleja en dirección contraria a la de llegada. Esto es lo que
sucede a la luna.</li>
</ul>
<p>Ten en cuenta que es muy difícil encontrar objetos físicos que imiten
a la perfección un cierto modelo. Suelen recaer en un híbrido entre dos
o más modelos.</p>
<p>Fijado un cierto modelo, la función de distribución de reflectancia,
BRDF, puede ser <strong>isotrópica</strong> o
<strong>anisotrópica</strong>. Los materiales isotrópicos son aquellos
cuya BRDF <span class="math inline">\(f_r(p, \omega_o \leftarrow
\omega_i)\)</span> no cambia cuando se rotan los vectores <span
class="math inline">\(\omega_o, \omega_i\)</span> un mismo ángulo
alrededor de <span class="math inline">\(p\)</span>. La mayor parte de
los materiales son de este tipo.</p>
<p>Por el contrario, si la BRDF sí cambia cuando se aplica dicha
rotación, entonces nos encontramos ante un material anisotrópico. Esto
suele ocurrir cuando tratamos con superficies pulidas con surcos.
Algunos ejemplos son los discos de vinilo y los metales pulidos en una
dirección.</p>
<h3 data-number="2.3.2" id="reflexión"><span
class="header-section-number">2.3.2</span> Reflexión</h3>
<p>Primero, tratemos con materiales que únicamente reflejan luz; es
decir, su BSDF es una BRDF.</p>
<h4 data-number="2.3.2.1" id="reflexión-especular-perfecta"><span
class="header-section-number">2.3.2.1</span> Reflexión especular
perfecta</h4>
<p>Para un material especular perfecto (es decir, espejos), la dirección
reflejada <span class="math inline">\(\omega_o\)</span> dado un rayo
incidente <span class="math inline">\(\omega_i\)</span> es <span
class="citation"
data-cites="GemsII-Reflexion PBRT3e McGuire2018GraphicsCodex">(<a
href="#ref-GemsII-Reflexion" role="doc-biblioref">Haines 2021, 105</a>;
<a href="#ref-PBRT3e" role="doc-biblioref">Pharr, Jakob, and Humphreys
2016</a>, Specular Reflection and Transmission; <a
href="#ref-McGuire2018GraphicsCodex" role="doc-biblioref">McGuire
2021</a>, Materials)</span>:</p>
<p><span class="math display">\[
\omega_o = 2 \mathbf{n} (\omega_i \cdot \mathbf{n}) -\omega_i
\]</span></p>
<p>siendo <span class="math inline">\(\mathbf{n}\)</span> la normal en
el punto incidente. Con esta expresión, se necesita que <span
class="math inline">\(\mathbf{n}\)</span> esté normalizado. Para los
otros dos vectores no es necesario; la dirección de salida tendrá la
misma norma que la de entrada.</p>
<p>La BRDF de un material especular perfecto se describe en términos de
la proporción de radiancia reflejada hacia <span
class="math inline">\(\omega_o\)</span> dependiente del vector incidente
<span class="math inline">\(\omega_i\)</span>, la cual puede viene
descrita por <span class="math inline">\(\rho_{hd}(\omega_i)\)</span>.
Se da la relación</p>
<p><span class="math display">\[
\rho_{hd}(\omega_i) = \int_{\mathbb{S}^2} {f_r(p, \omega_o \leftarrow
\omega_i)\ d\omega_o}
\]</span></p>
<p>Puesto que en los espejos perfectos no se pierde energía,
necesariamente <span class="math inline">\(\rho_{hd} = 1\)</span>.</p>
<p>Debemos tener en cuenta que la probabilidad de que un rayo tenga una
dirección diferente a la del reflejo es 0, por lo que</p>
<p><span class="math display">\[
f_r(p, \omega_o \leftarrow \omega_i) = 0 \qquad \forall\  \omega_0 \ne 2
\mathbf{n} (\omega_i \cdot \mathbf{n}) -\omega_i
\]</span></p>
<p>La función de densidad evaluada en el vector reflejado es
problemática. Está claro que debe integrar <span
class="math inline">\(\rho_{hd}\)</span>, pero el ángulo sólido en el
que se integra tiene medida cero, pues toda la radiancia se refleja
hacia una única dirección. Esto significa que</p>
<p><span class="math display">\[
f_r(\omega_o \leftarrow \omega_i ) = \frac{\rho_{hd}(\omega_i)}{0
\left\lvert \omega_i \cdot \mathbf{n} \right\rvert}
\]</span></p>
<p>Podemos solucionar este problema utilizando una Delta de Dirac,
obteniendo finalmente la BRDF de los materiales especulares
perfectos:</p>
<p><span id="eq:brdf_especular_perfecto" class="eqnos"><span
class="math display">\[
\begin{aligned}
f_r(\omega_o \leftarrow \omega_i ) &amp; = \frac{\delta(\omega_o,
\omega_i) \rho_{hd}(\omega_i)}{\left\lvert \omega_i \cdot \mathbf{n}
\right\rvert} = \\
    &amp; = \frac{\delta(\omega_o, \omega_i) \left\lvert \omega_i \cdot
\mathbf{n} \right\rvert k_r}{\left\lvert \omega_i \cdot \mathbf{n}
\right\rvert}
\end{aligned}
\]</span><span class="eqnos-number">(18)</span></span></p>
<p>siendo <span class="math inline">\(\rho_{hd}(\omega_i) = \left\lvert
\omega_i \cdot \mathbf{n} \right\rvert k_r\)</span> el albedo, con <span
class="math inline">\(k_r\)</span> el coeficiente de reflectividad, cuyo
valor se encuentra entre 0 y 1, dependiendo de la energía que se
pierda.</p>
<h4 data-number="2.3.2.2" id="reflexión-difusa-o-lambertiana"><span
class="header-section-number">2.3.2.2</span> Reflexión difusa o
lambertiana</h4>
<p>Este es uno de los modelos más sencillos. Es conocido también como el
modelo lambertiano. Se asume que la superficie es completamente difusa,
lo cual implica que la luz se refleja en todas direcciones
equiprobablemente, independientemente del punto de vista del observador
<span class="citation" data-cites="McGuire2018GraphicsCodex">(<a
href="#ref-McGuire2018GraphicsCodex" role="doc-biblioref">McGuire
2021</a>, Materials)</span>. Esto significa que</p>
<p><span class="math display">\[
f_r(\omega_o \leftarrow \omega_i) = \frac{\rho_{hd}}{\pi}
\]</span></p>
<p>El albedo <span class="math inline">\(0 \le \rho_{hd} \le 1\)</span>
es la reflectividad de la superficie. El denominador es aquel
coeficiente tal que se normalice la BRDF:</p>
<p><span class="math display">\[
\int_{\mathbb{S}^2} {\max(\mathbf{n} \cdot \omega_i, 0)\ d\omega_i} =
\pi
\]</span></p>
<p>En la práctica no se utiliza mucho, pues está muy limitado.</p>
<h4 data-number="2.3.2.3" id="reflexión-especular-no-perfecta"><span
class="header-section-number">2.3.2.3</span> Reflexión especular no
perfecta</h4>
<h5 data-number="2.3.2.3.1" id="phong"><span
class="header-section-number">2.3.2.3.1</span> Phong</h5>
<p>El modelo de Phong se basa en la observación de que, cuando el punto
de vista se alinea con la dirección del vector de luz reflejado,
aparecen puntos muy iluminados, lo que se conoce como resaltado
especular.</p>
<p>Su BRDF es</p>
<p><span class="math display">\[
f_r(p, \omega_o \leftarrow \omega_i) = \frac{\alpha + 2}{2\pi} \max{(0,
\omega_i \cdot \omega_o)}^\alpha
\]</span></p>
<p>donde <span class="math inline">\(\alpha\)</span> es índice del
brillo del material, y <span class="math inline">\(\omega_o\)</span> el
vector reflejado teniendo en cuenta <span
class="math inline">\(\omega_i\)</span> y <span
class="math inline">\(\mathbf{n}\)</span>. El coeficiente <span
class="math inline">\(\frac{\alpha + 2}{2\pi}\)</span> se utiliza para
normalizarla <span class="citation" data-cites="brdf-bp">(<a
href="#ref-brdf-bp" role="doc-biblioref">Giesen 2009</a>)</span></p>
<p>Este modelo de iluminación se suele usar en rasterización y no es
común encontrarlo en ray tracing físicamente realista. En su versión de
rasterización hace falta definir los coeficientes ambientales y difusos.
Aquí solo hemos optado por el especular.</p>
<h5 data-number="2.3.2.3.2" id="blinn---phong"><span
class="header-section-number">2.3.2.3.2</span> Blinn - Phong</h5>
<p>Este es una pequeña modificación al de Phong. En vez de usar el
vector reflejado de luz, se define un vector unitario entre el
observador y la luz, <span class="math inline">\(\mathbf{h} =
\frac{\omega + \mathbf{l}}{\left\lVert \omega + \mathbf{l}
\right\rVert}\)</span>. Resulta más fácil calcularlo. Además, este
modelo es más realista.</p>
<p><span class="math display">\[
f_r(p, \omega_o \leftarrow \omega_i) = \frac{8 \pi(2^{-\alpha/2} +
\alpha)}{(\alpha + 2)(\alpha + 4)} \max\{0, \mathbf{h} \cdot
\mathbf{n}\}^\alpha
\]</span></p>
<p>De la misma manera que en Phong, <span class="math inline">\(\frac{8
\pi(2^{-\alpha/2} + \alpha)}{(\alpha + 2)(\alpha + 4)}\)</span> es el
coeficiente de normalización de la BRDF <span class="citation"
data-cites="brdf-bp">(<a href="#ref-brdf-bp" role="doc-biblioref">Giesen
2009</a>)</span>.</p>
<h3 data-number="2.3.3" id="refracción"><span
class="header-section-number">2.3.3</span> Refracción</h3>
<p>Algunos materiales permiten que la luz los atraviese –conocido como
transmisión–. En estos casos, decimos que se produce un cambio en el
medio. Para conocer cómo de rápido viajan los fotones a través de ellos,
se utiliza un valor denominado <strong>índice de refracción</strong>,
usualmente denotado por <span class="math inline">\(\eta\)</span>:</p>
<p><span class="math display">\[
\eta = \frac{c}{\nu}
\]</span></p>
<p>siendo <span class="math inline">\(c\)</span> la velocidad de la luz
en el vacío y <span class="math inline">\(\nu\)</span> la velocidad de
fase del medio, la cual depende de la longitud de onda. Sin embargo,
como hemos comentado varias veces, no tendremos en cuenta la longitud de
onda en nuestro ray tracer, por lo que no nos tenemos que preocupar de
esto.</p>
<p>Algunos materiales como el aire tienen un índice de refracción <span
class="math inline">\(\eta_{\text{aire}} = 1.0003\)</span>, mientras que
el del agua vale <span class="math inline">\(\eta_{\text{agua}} =
1.333\)</span>, y el del cristal vale <span
class="math inline">\(\eta_{\text{cristal}} = 1.52\)</span>.</p>
<h4 data-number="2.3.3.1" id="ley-de-snell"><span
class="header-section-number">2.3.3.1</span> Ley de Snell</h4>
<p>La <strong>ley de Snell</strong> nos proporciona una ecuación muy
sencilla que relaciona el cambio de un medio con índice de refracción
<span class="math inline">\(\eta_1\)</span> a otro con índice de
refracción <span class="math inline">\(\eta_2\)</span>:</p>
<p><span id="eq:ley_snell" class="eqnos"><span class="math display">\[
\eta_1 \sin\theta_1 = \eta_2 \sin\theta_2
\]</span><span class="eqnos-number">(19)</span></span></p>
<p>siendo <span class="math inline">\(\theta_1\)</span> y <span
class="math inline">\(\theta_2\)</span> los ángulos de entrada y salida
respectivamente.</p>
<p>Usualmente, los índices de refración son conocidos, así como el
ángulo de incidencia <span class="math inline">\(\theta_1\)</span>, por
lo que podremos calcular el ángulo del vector refractado con
facilidad:</p>
<p><span class="math display">\[
\theta_2 = \arcsin{\left(\frac{\eta_1}{\eta_2}\sin\theta_2\right)}
\]</span></p>
<p>Cuando cambiamos de un medio con índice de refracción <span
class="math inline">\(\eta_1\)</span> a otro con <span
class="math inline">\(\eta_2 &lt; \eta_1\)</span>, podemos encontrarnos
ante un caso de <strong>reflexión interna total</strong>.
Analíticamente, lo que ocurre es que</p>
<p><span class="math display">\[
\sin\theta_2 = \frac{\eta_1}{\eta_2}\sin\theta_1 &gt; 1
\]</span></p>
<p>lo cual no puede ocurrir. Se denomina el ángulo crítico a aquel <span
class="math inline">\(\theta_1\)</span> para la cual <span
class="math inline">\(\frac{\eta_1}{\eta_2}\sin\theta_1 &gt;
1\)</span>:</p>
<p><span class="math display">\[
\theta_1 = \arcsin{\left(\frac{\eta_2}{\eta_1}\right)}
\]</span></p>
<p>Por ejemplo, si un haz de luz viaja desde un cristal hacia un cuerpo
de agua, entonces <span class="math inline">\(\theta_1 =
\arcsin{(1.333/1.52)} \approx 1.06\)</span> radianes <span
class="math inline">\(= 61.04\textdegree\)</span>.</p>
<p>Lo que ocurre en estos casos es que, en vez de pasar al segundo
medio, los fotones vuelven al primero; creando un reflejo como si de un
espejo se tratara.</p>
<div id="fig:reflexion_interna_total" class="fignos">
<figure>
<img loading="lazy" src="./img/02/Reflexión%20interna%20total.jpg" style="width:80.0%"
alt="Figura 11: Como el ángulo de incidencia es considerablemente alto, por la parte de arriba la luz no puede atravesar el agua. Esto hace que podamos ver el edificio de enfrente. En el centro vemos refractado el suelo. Y, sin embargo, en la parte inferior, ¡observamos luz solar y el edificio de nuevo!" />
<figcaption aria-hidden="true"><span>Figura 11:</span> Como el ángulo de
incidencia es considerablemente alto, por la parte de arriba la luz no
puede atravesar el agua. Esto hace que podamos ver el edificio de
enfrente. En el centro vemos refractado el suelo. Y, sin embargo, en la
parte inferior, ¡observamos luz solar y el edificio de
nuevo!</figcaption>
</figure>
</div>
<p>El vector refractado <span class="math inline">\(\omega_t\)</span>
puede conseguirse a partir de la siguiente expresión <span
class="citation" data-cites="McGuire2018GraphicsCodex">(<a
href="#ref-McGuire2018GraphicsCodex" role="doc-biblioref">McGuire
2021</a>)</span>:</p>
<p><span class="math display">\[
\omega_t =
    - \frac{\eta_1}{\eta_2}\left(\omega_i - (\omega_i \cdot
\mathbf{n})\mathbf{n}\right)
    - \left(
        \sqrt{1 - \left(\frac{\eta_1}{\eta_2}\right)^2 \left(1 -
(\omega_i \cdot \mathbf{n}^2)\right)}
      \right) \cdot \mathbf{n}
\]</span></p>
<h4 data-number="2.3.3.2" id="ecuaciones-de-fresnel"><span
class="header-section-number">2.3.3.2</span> Ecuaciones de Fresnel</h4>
<p>Aquellos materiales que refractan y reflejan luz (como el agua de la
foto anterior) no pueden generar energía de la nada; por lo que la
combinación de ambos efectos debe ser proporcional a la luz incidente.
Es decir, una fracción de luz es reflejada, y otra es refractada. Las
<strong>ecuaciones de Fresnel</strong> nos permiten conocer esta
cantidad.</p>
<p>La proporción de luz reflejada desde un rayo que viaja por un medio
con índice de refracción <span class="math inline">\(\eta_1\)</span> y
ángulo de incidencia <span class="math inline">\(\theta_1\)</span> a
otro medio con índice de refracción <span
class="math inline">\(\eta_2\)</span> es <span class="citation"
data-cites="GemsII-Fresnel">(<a href="#ref-GemsII-Fresnel"
role="doc-biblioref">Majercik 2021, 109</a>)</span>:</p>
<p><span id="eq:fresnel_equations" class="eqnos"><span
class="math display">\[
\begin{aligned}
    R_s &amp; = \left\lvert \frac
        {\eta_1 \cos\theta_1 - \eta_2 \sqrt{1 - \left(
\frac{\eta_1}{\eta_2}\sin\theta_1\right) ^2}}
        {\eta_1 \cos\theta_1 + \eta_2 \sqrt{1 - \left(
\frac{\eta_1}{\eta_2}\sin\theta_1\right) ^2}}
     \right\rvert^2 \\
    R_p &amp; = \left\lvert \frac
        {\eta_1 \sqrt{1 - \left(
\frac{\eta_1}{\eta_2}\sin\theta_1\right)^2} - \eta_2\cos\theta_1}
        {\eta_1 \sqrt{1 - \left(
\frac{\eta_1}{\eta_2}\sin\theta_1\right)^2} + \eta_2\cos\theta_1}
     \right\rvert^2
\end{aligned}
\]</span><span class="eqnos-number">(20)</span></span></p>
<p>donde los subíndices <span class="math inline">\(s\)</span> y <span
class="math inline">\(p\)</span> denotan la polarización de la luz:
<span class="math inline">\(s\)</span> es perpendicular a la dirección
de propagación, mientras que <span class="math inline">\(p\)</span> es
paralela.</p>
<p>Generalmente en los ray tracers se simula luz no polarizada, así que
se deben promediar ambos valores. Por lo tanto, se debe usar el valor
<span class="math inline">\(R\)</span> definido de la siguiente
manera:</p>
<p><span id="eq:fresnel_equation" class="eqnos"><span
class="math display">\[
R = \frac{R_s + R_p}{2}
\]</span><span class="eqnos-number">(21)</span></span></p>
<h4 data-number="2.3.3.3" id="la-aproximación-de-schlick"><span
class="header-section-number">2.3.3.3</span> La aproximación de
Schlick</h4>
<p>Como podemos imaginarnos, calcular las expresiones de Fresnel [<a
href="#eq:fresnel_equations">20</a>] no es precisamente barato. En la
práctica, todo el mundo utiliza una aproximación creada por Schlick, la
cual funciona sorprendentemente bien. Viene dada por</p>
<p><span id="eq:schlick_aprox" class="eqnos"><span
class="math display">\[
R(\theta_1) = R_0 + (1 - R_0)(1 - \cos\theta_1)^5
\]</span><span class="eqnos-number">(22)</span></span></p>
<p>siendo <span class="math inline">\(R_0 = R(0)\)</span>; es decir, el
valor que toma <span class="math inline">\(R\)</span> cuando el rayo
incidente es paralelo al medio. Su valor es</p>
<p><span class="math display">\[
R_0 = \left(\frac{\eta_1 - \eta_2}{\eta_1 + \eta_2}\right)^2
\]</span></p>
<p>Esta aproximación es 32 veces más rápida de calcular que las
ecuaciones de Fresnel, generando un error medio inferior al 1% <span
class="citation"
data-cites="https://doi.org/10.1111/1467-8659.1330233">(<a
href="#ref-https://doi.org/10.1111/1467-8659.1330233"
role="doc-biblioref">Schlick 1994</a>)</span></p>
<h3 data-number="2.3.4"
id="brdfs-basadas-en-modelos-de-microfacetas"><span
class="header-section-number">2.3.4</span> BRDFs basadas en modelos de
microfacetas</h3>
<p>La mayor parte de las superficies no son puramente lisas a nivel
microscópico. En la mayoría de materiales existen microfacetas, las
cuales producen que la luz no se refleje de forma especular. Esto hace
que la luz pueda salir en direcciones muy diferentes dependiendo de la
rugosidad de la superficie. Este tipo de materiales suele necesitar una
función que caracterice la distribución de las normales de las
microfacetas (denotada por <span
class="math inline">\(\mathbf{n_f}\)</span>).</p>
<p>Un modelo que utiliza esta idea es el de <strong>Oren-Nayar</strong>,
el cual describe la reflexión difusa de una superficie. Se basa en la
idea de que las superficies rugosas aparentan ser más brillantes
conforme la dirección de la luz se aproxima a la dirección de la vista.
La BRDF de Oren-Nayar es <span class="citation" data-cites="PBRT3e">(<a
href="#ref-PBRT3e" role="doc-biblioref">Pharr, Jakob, and Humphreys
2016</a>, Microfacet Models)</span></p>
<p><span class="math display">\[
f_r(p, \omega_o \leftarrow \omega_i) = \frac{\rho}{\pi} \left(A +
B\max{(0, \cos(\phi_i - \phi_o))}\sin\alpha\tan\beta\right)
\]</span></p>
<p>donde, si <span class="math inline">\(\theta\)</span> está en
radianes,</p>
<p><span class="math display">\[
\begin{aligned}
A      &amp; = 1 - \frac{\theta^2}{2(\theta^2 + 0.33)} \\
B      &amp; = \frac{0.45 \theta^2}{\theta^2 + 0.09}   \\
\alpha &amp; = \max{(\theta_i, \theta_o)}              \\
\beta  &amp; = \min{(\theta_i, \theta_o)}
\end{aligned}
\]</span></p>
<p>En la actualidad, el modelo más utilizado de microfacetas es el
<strong>GGX</strong>. Motores modernos como Unreal Engine 4 y Unity lo
utilizan en sus pipelines físicamente realistas. La BRDF se define de la
siguiente manera <span class="citation"
data-cites="McGuire2018GraphicsCodex">(<a
href="#ref-McGuire2018GraphicsCodex" role="doc-biblioref">McGuire
2021</a>)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
f_r(p, \omega_o, \omega_i) &amp; = \frac
    {
        D(p, \omega_h) F(p, \omega_i, \omega_o) G_{1}(p, \omega_i,
\omega_h) G_{1}(p, \omega_o, \omega_h)
    }
    {
        4\pi (\mathbf{n} \cdot \omega_i) (\mathbf{n} \cdot \omega_o)
    } = \\
    &amp; = \frac
    {
        4 \alpha^2_p \left(F_0 + (1 - F_0)(1 - \max{(0, \omega_h \cdot
\omega_i})^5\right)(\mathbf{n} \cdot \omega_i)(\mathbf{n} \cdot
\omega_o)
    }
    {
        \pi \left(1 + (\alpha^2_p - 1) \cdot (\mathbf{n} \cdot
\omega_h)^2\right)^2
        A_1
        A_2
    }
\end{aligned}
\]</span></p>
<p>siendo</p>
<p><span class="math display">\[
\begin{aligned}
A_1 &amp; = \left((\mathbf{n} \cdot \omega_i) + \sqrt{\alpha^2_p + (1 -
\alpha^2_p)(\mathbf{n} \cdot \omega_i)^2}\right) \\
A_2 &amp; = \left((\mathbf{n} \cdot \omega_o) + \sqrt{\alpha^2_p + (1 -
\alpha^2_p)(\mathbf{n} \cdot \omega_o)^2}\right)
\end{aligned}
\]</span></p>
<p>y donde</p>
<ul>
<li><span class="math inline">\(\omega_h\)</span> es el vector
intermedio a <span class="math inline">\(\omega_o\)</span> y <span
class="math inline">\(\omega_i\)</span>, calculado como <span
class="math inline">\((\omega_i + \omega_o)/\left\lVert \omega_i +
\omega_o \right\rVert\)</span>.</li>
<li><span class="math inline">\(\alpha\)</span> es el coeficiente de
rugosidad de la microfaceta, <span class="math inline">\(\alpha \in [0,
1]\)</span>.</li>
<li><span class="math inline">\(F_0\)</span> es la reflectancia medida
en un ángulo de incidencia <span class="math inline">\(\theta_i =
0\)</span>.</li>
</ul>
<h2 data-number="2.4" id="la-rendering-equation"><span
class="header-section-number">2.4</span> La rendering equation</h2>
<p>Y, finalmente, tras esta introducción de los principales conceptos
radiométricos, llegamos a la ecuación más importante de todo este
trabajo: la <strong>rendering equation</strong>; también llamada la
<strong>ecuación del transporte de luz</strong>.</p>
<blockquote>
<p><strong>Nota</strong>(ción): esta vez no traduciré el concepto. Es
cierto que afea un poco la escritura teniendo en cuenta que esto es un
texto en castellano. Sin embargo, la otra opción es inventarme una
traducción que nadie usa.</p>
</blockquote>
<p>Antes de comenzar, volvamos a plantear de nuevo la situación: nos
encontramos observando desde nuestra pantalla una escena virtual
mediante la cámara. Queremos saber qué color tomará un pixel específico.
Para conseguirlo, dispararemos rayos desde nuestro punto de vista hacia
el entorno, haciendo que reboten en los objetos. Cuando un rayo impacte
en una superficie, adquirirá parte de las propiedades del material del
objeto. Además, de este rayo surgirán otros nuevos (un rayo dispersado y
otro refractado), que a su vez repetirán el proceso. La información que
se obtiene a partir de estos caminos de rayos nos permitirá darle color
al píxel. Con dicha ecuación, describiremos analíticamente cómo ocurre
esto.</p>
<p>Un último concepto más: denotemos por <span
class="math inline">\(L_e(p, \omega_o)\)</span> a <strong>la radiancia
producida por los materiales emisivos</strong>. En esencia, estos
materiales son fuentes de luz, pues emiten radiancia por sí mismos.</p>
<p>La <em>rendering equation</em> viene dada por la siguiente
expresión:</p>
<p><span id="eq:rendering_equation" class="eqnos"><span
class="math display">\[
L_o(p, \omega_o) = L_e(p, \omega_o) + \int_{H^2(\mathbf{n})}{f(p,
\omega_o \leftarrow \omega_i) L_i(p, \omega_i) \cos\theta_i\ d\omega_i}
\]</span><span class="eqnos-number">(23)</span></span></p>
<p>Para hacerla operativa en términos computacionales podemos
transformarla un poco. Bien, partamos de la ecuación de para la
radiancia reflejada:</p>
<p><span class="math display">\[
L_o(p, \omega_o) = \int_{H^2(\mathbf{n})}{f(p, \omega_o \leftarrow
\omega_i) L_i(p, \omega_i) \cos\theta_i\ d\omega_i}
\]</span></p>
<p>Vamos a buscar expresar la radiancia incidente en términos de la
radiancia reflejada. Para ello, usamos la propiedad de que la radiancia
a lo largo de un rayo no cambia.</p>
<p>Si a una superficie le llega un fotón desde alguna parte, debe ser
porque <em>“alguien”</em> ha tenido que emitirlo. El fotón
necesariamente ha llegado a partir de un rayo. La propiedad nos dice que
la radiancia no ha podido cambiar en el camino.</p>
<p>Pues bien, consideremos una función <span class="math inline">\(r:
\mathbb{R}^3 \times \Omega \to \mathbb{R}^3\)</span> tal que, dado un
punto <span class="math inline">\(p\)</span> y una dirección <span
class="math inline">\(\omega\)</span>, devuelve el siguiente punto de
impacto en una superficie. En esencia, es una función de <em>ray
casting</em> <span class="citation"
data-cites="pellacini-marschner-2017">(<a
href="#ref-pellacini-marschner-2017" role="doc-biblioref">Fabio
Pellacini 2022</a>, Path Tracing)</span>.</p>
<p>Esta función nos permite expresar el punto anterior de la siguiente
forma:</p>
<p><span class="math display">\[
L_i(p, \omega) = L_o(r(p, \omega), -\omega)
\]</span></p>
<p>Esto nos permite cambiar la expresión de <span
class="math inline">\(L_i\)</span> en la integral anterior:</p>
<p><span class="math display">\[
L_o(p, \omega_o) = \int_{H^2(\mathbf{n})}{f(p, \omega_o \leftarrow
\omega_i) L_o(r(p, \omega_i), -\omega_i) \cos\theta_i\ d\omega_i}
\]</span></p>
<p>Finalmente, la radiancia total vendrá dada por la suma de la
radiancia emitida y la reflejada:</p>
<p><span class="math display">\[
L(p, \omega_o) = L_e(p, \omega_o) + \int_{H^2(\mathbf{n})}{f(p, \omega_o
\leftarrow \omega_i) L_o(r(p, \omega_i), -\omega_i) \cos\theta_i\
d\omega_i}
\]</span></p>
<p>Y con esto, ¡hemos obtenido la <em>rendering equation</em>!</p>
<p>Si quieres ver gráficamente cómo funciona, te recomiendo pasarte por
<span class="citation" data-cites="arneback-2019">(<a
href="#ref-arneback-2019" role="doc-biblioref">Arnebäck
2019</a>)</span>. Es un vídeo muy intuitivo.</p>
<iframe width="784" height="441" src="https://www.youtube-nocookie.com/embed/eo_MTI-d28s" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
<p>Si nos paramos a pensar, la ecuación de reflexión es muy similar a la
de renderizado. Sin embargo, hay un par de matices que las hacen muy
diferentes:</p>
<ul>
<li>La ecuación de reflexión describe cómo se comporta la luz reflejada
en un cierto punto. Es decir, tiene un ámbito local. Además, para
calcular la radiancia reflejada, se necesita conocer la radiancia
incidente.</li>
<li>La <em>rendering equation</em> calcula las condiciones globales de
la luz. Además, no se conocen las radiancias de salida.</li>
</ul>
<p>Este último matiz es importante. Para renderizar una imagen, se
necesita calcular la radiancia de salida para aquellos puntos visibles
desde nuestra cámara.</p>
<h1 data-number="3" id="métodos-de-monte-carlo"><span
class="header-section-number">3</span> Métodos de Monte Carlo</h1>
<p>Como vimos en el capítulo anterior, la clave para conseguir una
imagen en nuestro ray tracer es calcular la cantidad de luz en un punto
de la escena. Para ello, necesitamos hallar la radiancia en dicha
posición mediante la <em>rendering equation</em>. Sin embargo, es
<em>muy</em> difícil resolverla; tanto computacional como
analíticamente. Por ello, debemos atacar el problema desde otro punto de
vista.</p>
<p>Las técnicas de Monte Carlo nos permitirán aproximar el valor que
toman las integrales mediante una estimación. Utilizando muestreo
aleatorio para evaluar puntos de una función, seremos capaces de obtener
un resultado suficientemente bueno.</p>
<p>Una de las propiedades que hacen interesantes a este tipo de métodos
es la <strong>independencia del ratio de convergencia y la
dimensionalidad del integrando</strong>. Sin embargo, conseguir un mejor
rendimiento tiene un precio a pagar. Dadas <span
class="math inline">\(n\)</span> muestras, la convergencia a la solución
correcta tiene un orden de <span
class="math inline">\(\mathcal{O}\left(n^{-1/2}\right) =
\mathcal{O}\left(\frac{1}{\sqrt{n}}\right)\)</span>. Es decir, para
reducir el error a la mitad, necesitaríamos 4 veces más muestras.</p>
<p>En este capítulo veremos los fundamentos de la integración de Monte
Carlo, cómo muestrear distribuciones específicas y métodos para afinar
el resultado final.</p>
<h2 data-number="3.1" id="repaso-de-probabilidad"><span
class="header-section-number">3.1</span> Repaso de probabilidad</h2>
<p>Antes de comenzar a fondo, necesitaremos unas nociones de variable
aleatoria para poder entender la integración de Monte Carlo, por lo que
vamos a hacer un breve repaso.</p>
<p>Una <strong>variable aleatoria</strong> <span
class="math inline">\(X\)</span> (v.a.) es, esencialmente, una regla que
asigna un valor numérico a cada posibilidad de un proceso de azar.
Formalmente, es una función definida en un espacio de probabilidad <span
class="math inline">\((\Omega, \mathcal{A}, P)\)</span> asociado a un
experimento aleatorio:</p>
<p><span class="math display">\[
X: \Omega \rightarrow \mathbb{R}
\]</span></p>
<p>A <span class="math inline">\(\Omega\)</span> lo conocemos como
espacio muestral (conjunto de todas las posibilidades), <span
class="math inline">\(\mathcal{A}\)</span> es una <span
class="math inline">\(\sigma\)</span>-álgebra de subconjuntos de <span
class="math inline">\(\Omega\)</span> que refleja todas las
posibilidades de eventos aleatorios, y <span
class="math inline">\(P\)</span> es una función probabilidad, que asigna
a cada evento una probabilidad.</p>
<p>Una variable aleatoria <span class="math inline">\(X\)</span> puede
clasificarse atendiendo a cómo sea su rango <span
class="math inline">\(R_X = \left\{x \in \mathbb{R} \,\middle|\, \exists
\omega \in \Omega \text{ tal que } X(\omega) = x \right\}\)</span>: en
discreta o continua, dependiendo de si <span
class="math inline">\(X\)</span> toma valores en un conjunto numerable o
no numerable.</p>
<h3 data-number="3.1.1" id="variables-aleatorias-discretas"><span
class="header-section-number">3.1.1</span> Variables aleatorias
discretas</h3>
<p>Las v.a. discretas son aquellas cuyo rango es un conjunto
discreto.</p>
<p>Para comprender mejor cómo funcionan, pongamos un ejemplo:
Consideremos un experimento en el que lanzamos dos dados, anotando lo
que sale en cada uno. Los posibles valores que toman serán <span
class="citation" data-cites="galvin-no-date">(<a
href="#ref-galvin-no-date" role="doc-biblioref">Galvin
n.d.</a>)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
\Omega = \{ &amp; (1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6),  \\
   &amp; (2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (2, 6),  \\
   &amp; (3, 1), (3, 2), (3, 3), (3, 4), (3, 5), (3, 6),  \\
   &amp; (4, 1), (4, 2), (4, 3), (4, 4), (4, 5), (4, 6),  \\
   &amp; (5, 1), (5, 2), (5, 3), (5, 4), (5, 5), (5, 6),  \\
   &amp; (6, 1), (6, 2), (6, 3), (6, 4), (6, 5), (6, 6)   \}
\end{aligned}
\]</span></p>
<p>Cada resultado tiene la misma probabilidad de ocurrir (claro está, si
el dado no está trucado). Como hay <span
class="math inline">\(36\)</span> posibilidades, la probabilidad de
obtener un cierto valor es de <span
class="math inline">\(\frac{1}{36}\)</span>.</p>
<p>La v.a. <span class="math inline">\(X\)</span> denotará la suma de
los valores obtenidos en cada uno. Así, por ejemplo, si al lanzar los
dados hemos obtenido <span class="math inline">\((1, 3)\)</span>, <span
class="math inline">\(X\)</span> tomará el valor <span
class="math inline">\(4\)</span>. En total, <span
class="math inline">\(X\)</span> puede tomar todos los valores
comprendidos entre <span class="math inline">\(2\)</span> y <span
class="math inline">\(12\)</span>. Cada pareja no está asociada a un
único valor de <span class="math inline">\(X\)</span>. Por ejemplo,
<span class="math inline">\((1, 2)\)</span> suma lo mismo que <span
class="math inline">\((2, 1)\)</span>. Esto nos lleva a preguntarnos…
¿Cuál es la probabilidad de que <span class="math inline">\(X\)</span>
adquiera un cierto valor?</p>
<p>La <strong>función masa de probabilidad</strong> nos permite conocer
la probabilidad de que <span class="math inline">\(X\)</span> tome un
cierto valor <span class="math inline">\(x\)</span>. Se denota por <span
class="math inline">\(P(X = x)\)</span>.</p>
<p>También se suele usar <span class="math inline">\(p_X(x)\)</span> o,
directamente <span class="math inline">\(p(x)\)</span>, cuando no haya
lugar a dudas. Sin embargo, en este trabajo reservaremos este nombre a
otro tipo de funciones.</p>
<blockquote>
<p><strong>Nota</strong>(ción): Cuando <span
class="math inline">\(X\)</span> tenga una cierta función masa de
probabilidad, escribiremos <span class="math inline">\(X \sim
p_X\)</span></p>
</blockquote>
<p>En este ejemplo, la probabilidad de que <span
class="math inline">\(X\)</span> tome el valor <span
class="math inline">\(4\)</span> es</p>
<p><span class="math display">\[
\begin{aligned}
P(X = 4) &amp; = \sum{\small{\text{nº parejas que suman 4}} \cdot
\small{\text{probabilidad de que salga la pareja}}} \\
         &amp; = 3 \cdot \frac{1}{36} = \frac{1}{12}
\end{aligned}
\]</span></p>
<p>Las parejas serían <span class="math inline">\((1, 3), (2,
2)\)</span> y <span class="math inline">\((3, 1)\)</span>.</p>
<p>Por definición, si el conjunto de valores que puede tomar <span
class="math inline">\(X\)</span> es <span class="math inline">\(\{x_1,
\dots, x_n\}\)</span>, la función masa de probabilidad debe cumplir
que</p>
<p><span class="math display">\[
\sum_{i = 1}^{n}{P\left[ X = x_i \right]} = 1
\]</span></p>
<p>Siendo <span class="math inline">\(P\left[ X = x_i \right] \ge
0\)</span>, pues la f.m.p. es no negativa.</p>
<p>Muchas veces nos interesará conocer la probabilidad de que <span
class="math inline">\(X\)</span> se quede por debajo o igual que un
cierto valor <span class="math inline">\(x\)</span> (de hecho, podemos
caracterizar distribuciones aleatorias gracias a esto). Para ello,
usamos la <strong>función de distribución</strong>:</p>
<p><span class="math display">\[
F_X(x) = P(X \le x) = \sum_{\substack{k \in \mathbb{R} \\ k \le x}}{P(X
= k)}
\]</span></p>
<p>Es una función continua por la derecha y monótona no decreciente.
Además, se cumple que <span class="math inline">\(0 \le F_X(x) \le
1\)</span> y <span class="math inline">\(\lim_{x \to -\infty}{F_X} =
0\)</span>, <span class="math inline">\(\lim_{x \to \infty}{F_X} =
1\)</span>.</p>
<p>En nuestro ejemplo, si consideramos <span class="math inline">\(x =
3\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
F_X(x) &amp; = \sum_{i = 1}^{3}{P(X = i)} = P(X = 1) + P(X = 2) + P(X =
3) \\
       &amp; = \frac{1}{36} + \frac{2}{36} + \frac{3}{36} = \frac{1}{12}
\end{aligned}
\]</span></p>
<h3 data-number="3.1.2" id="variables-aleatorias-continuas"><span
class="header-section-number">3.1.2</span> Variables aleatorias
continuas</h3>
<p>Este tipo de variables aleatorias tienen un rango no numerable; es
decir, el conjunto de valores que puede tomar abarca un intervalo de
números.</p>
<p>Un ejemplo podría ser la altura de una persona.</p>
<p>Si en las variables aleatorias discretas teníamos funciones masa de
probabilidad, aquí definiremos las <strong>funciones de densidad de
probabilidad</strong> (o simplemente, funciones de densidad). La idea es
la misma: nos permite conocer la probabilidad de que nuestra variable
aleatoria tome un cierto valor del espacio muestral.</p>
<p>Es importante mencionar que, aunque <em>la probabilidad de que la
variable aleatoria tome un valor específico</em> es <span
class="math inline">\(0\)</span>, ya que nos encontramos en un conjunto
no numerable, sí que podemos calcular la probabilidad de que se
encuentre entre dos valores. Por tanto, si la función de densidad es
<span class="math inline">\(f_X\)</span>, entonces</p>
<p><span class="math display">\[
P(a \le X \le b) = \int_{a}^{b}{f_X(x)dx}
\]</span></p>
<p>La función de densidad tiene dos características importantes:</p>
<ol type="1">
<li><span class="math inline">\(f_X\)</span> es no negativa; esto es,
<span class="math inline">\(f_X(x) \ge 0\ \forall x \in
\mathbb{R}\)</span></li>
<li><span class="math inline">\(f_X\)</span> integra uno en todo <span
class="math inline">\(\mathbb{R}\)</span>:</li>
</ol>
<p><span class="math display">\[
\int_{-\infty}^{\infty}{f_X(x)\ dx} = 1
\]</span></p>
<p>Estas dos propiedades caracterizan a una función de densidad; es
decir, toda función <span class="math inline">\(f: \mathbb{R}
\rightarrow \mathbb{R}\)</span> no negativa e integrable tal que <span
class="math inline">\(\int_{-\infty}^{\infty}{f_X(x)\ dx} = 1\)</span>
es la función de densidad de alguna variable continua.</p>
<blockquote>
<p>Intuitivamente, podemos ver esta última propiedad como <em>si
acumulamos todos los valores que puede tomar la variable aleatoria, la
probabilidad de que te encuentres en el conjunto debe ser 1</em></p>
</blockquote>
<p>Una de las variables aleatorias que más juego nos darán en el futuro
será la <strong>v.a. con distribución uniforme en <span
class="math inline">\([0, 1)\)</span></strong>. La denotaremos <span
class="math inline">\(\Xi \sim \mathcal{U}\left([0, 1)\right)\)</span>.
La probabilidad de que <span class="math inline">\(\Xi\)</span> tome un
valor es constante, por lo que podemos definir su función de densidad
como</p>
<p><span class="math display">\[
f_\Xi(\xi) = \left\{  \begin{array}{llc}
                  1 &amp; \text{si } \xi \in [0, 1) \\
                  0 &amp; \text{en otro caso.}
                  \end{array}
         \right.
\]</span></p>
<p>La probabilidad de <span class="math inline">\(\Xi\)</span> tome un
valor entre dos elementos <span class="math inline">\(a, b \in [0,
1)\)</span> es</p>
<p><span class="math display">\[
P(\Xi \in [a, b]) = \int_{a}^{b}{1dx} = b - a
\]</span></p>
<p>Como veremos más adelante, definiendo correctamente una función de
densidad conseguiremos mejorar el rendimiento del path tracer.</p>
<p>La función de distribución <span
class="math inline">\(F_X(x)\)</span> podemos definirla como:</p>
<p><span class="math display">\[
F_X(x) = P(X \le x) = \int_{-\infty}^{x}{f_X(t)dt}
\]</span></p>
<p>Es decir, dado un <span class="math inline">\(x\)</span>, ¿cuál sería
la probabilidad de que <span class="math inline">\(X\)</span> se quede
por debajo de <span class="math inline">\(x\)</span>?</p>
<p>Al igual que ocurre en el caso discreto, <span
class="math inline">\(F_X\)</span> toma valores entre 0 y 1 (<span
class="math inline">\(0 \le F_X(x) \le 1\)</span>) y sus límites
laterales coinciden con las cotas anteriores (<span
class="math inline">\(\lim_{x \to -\infty}{F_X} = 0\)</span>, <span
class="math inline">\(\lim_{x \to \infty}{F_X} = 1\)</span>).</p>
<p>Además, se cumple que <span class="math inline">\(0 \le F_X \le
1(x)\)</span> y <span class="math inline">\(\lim_{x \to -\infty}{F_X} =
0\)</span>, <span class="math inline">\(\lim_{x \to \infty}{F_X} =
1\)</span>.</p>
<p>El Teorema Fundamental del Cálculo nos permite relacionar función de
distribución y función de densidad directamente:</p>
<p><span class="math display">\[
f_X(x) = \frac{dF_X(x)}{dx}
\]</span></p>
<h3 data-number="3.1.3"
id="esperanza-y-varianza-de-una-variable-aleatoria"><span
class="header-section-number">3.1.3</span> Esperanza y varianza de una
variable aleatoria</h3>
<p>La <strong>esperanza de una variable aleatoria</strong>, denotada
<span class="math inline">\(E\left[ X \right]\)</span>, es una
generalización de la media ponderada. Nos informa del <em>valor
esperado</em> de dicha variable aleatoria.</p>
<p>En el caso de las variables discretas, se define como</p>
<p><span class="math display">\[
E\left[ X \right] = \sum_{i}{x_i p_i}
\]</span></p>
<p>donde <span class="math inline">\(x_i\)</span> son los posibles
valores que puede tomar la v.a., y <span
class="math inline">\(p_i\)</span> la probabilidad asociada a cada uno
de ellos; es decir, <span class="math inline">\(p_i = P[X =
x_i]\)</span></p>
<p>Para una variable aleatoria continua real, la esperanza viene dada
por</p>
<p><span class="math display">\[
E\left[ X \right] = \int_{-\infty}^{\infty}{x f_X(x) dx}
\]</span></p>
<p>Pongamos un par de ejemplos del cálculo de la esperanza. En el <a
href="#variables-aleatorias-discretas">ejemplo de las variables
discretas</a>, la esperanza venía dada por</p>
<p><span class="math display">\[
E\left[ X \right] = \sum_{i = 2}^{12}{i P[X = i]} = 2\frac{1}{36} + 3
\frac{2}{36} + \dots + 12 \frac{1}{36} = 7
\]</span></p>
<p>Para variables aleatorias uniformes en <span
class="math inline">\((a, b)\)</span> (es decir, <span
class="math inline">\(X \sim \mathcal{U}(a, b)\)</span>), la esperanza
es</p>
<p><span class="math display">\[
E\left[ X \right] = \int_{a}^{b}{x \frac{1}{b - a}dx} = \frac{a + b}{2}
\]</span></p>
<p>La esperanza tiene unas cuantas propiedades que nos resultarán muy
útiles. Estas son:</p>
<ul>
<li><strong>Linealidad</strong>:
<ul>
<li>Si <span class="math inline">\(X, Y\)</span> son dos v.a., <span
class="math inline">\(E\left[ X + Y \right] = E\left[ X \right] +
E\left[ Y \right]\)</span></li>
<li>Si <span class="math inline">\(a\)</span> es una constante, <span
class="math inline">\(X\)</span> una v.a., entonces <span
class="math inline">\(E\left[ aX \right] = aE\left[ X
\right]\)</span></li>
<li>Análogamente, para ciertas <span class="math inline">\(X_1, \dots,
X_k\)</span>, <span class="math inline">\(E\left[ \sum_{i = 1}^{k}{X_i}
\right] = \sum_{i = 1}^{k}{E\left[ X_i \right]}\)</span></li>
</ul></li>
</ul>
<p>Estas propiedades no necesitan que las variables aleatorias sean
independientes, lo cual supondrá un punto clave para las técnicas de
Monte Carlo.</p>
<p>Será habitual encontrarnos con el problema de que no conocemos la
distribución de una variable aleatoria <span
class="math inline">\(Y\)</span>. Sin embargo, si encontramos una
transformación medible de una variable aleatoria <span
class="math inline">\(X\)</span> de forma que obtengamos <span
class="math inline">\(Y\)</span> (esto es, <span
class="math inline">\(\exists g\)</span> función medible tal que <span
class="math inline">\(g(X) = Y\)</span>), entonces podemos calcular la
esperanza de <span class="math inline">\(Y\)</span> fácilmente. Esta
propiedad hará que las variables aleatorias con distribución uniforme
adquieran muchísima importancia. Generar números aleatorios en <span
class="math inline">\([0, 1)\)</span> es muy fácil, así <a
href="#método-de-la-transformada-inversa">que obtendremos otras vv.aa a
partir de <span class="math inline">\(\xi\)</span></a>.</p>
<p>Otra medida muy útil de una variable aleatoria es <strong>la
varianza</strong>. Nos permitirá medir cómo de dispersa es la
distribución con respecto a su media. La denotaremos como <span
class="math inline">\(Var[X]\)</span>, y se define como</p>
<p><span class="math display">\[
Var[X] = E\left[ (X - E\left[ X \right])^2 \right]
\]</span></p>
<p>Si desarrollamos esta definición, podemos conseguir una expresión
algo más agradable:</p>
<p><span class="math display">\[
\begin{aligned}
   Var[X] &amp; = E\left[ (X - E\left[ X \right])^2 \right] = \\
          &amp; = E\left[ X^2 + E\left[ X \right]^2 - 2XE\left[ X
\right] \right] = \\
          &amp; = E\left[ X^2 \right] + E\left[ X \right]^2 - 2E\left[ X
\right]E\left[ X \right] = \\
          &amp; = E\left[ X^2\right] - E\left[X \right]^2
\end{aligned}
\]</span></p>
<p>Hemos usado que <span class="math inline">\(E\left[ E\left[ X \right]
\right] = E\left[ X \right]\)</span> y la linealidad de la
esperanza.</p>
<p>Enunciemos un par de propiedades que tiene, similares a la de la
esperanza:</p>
<ul>
<li>La varianza saca constantes al cuadrado: <span
class="math inline">\(Var[aX] = a^2Var[X]\)</span></li>
<li><span class="math inline">\(Var[X + Y] =\)</span> <span
class="math inline">\(Var[X] + Var[Y] + 2Cov[X, Y]\)</span>, donde <span
class="math inline">\(Cov[X, Y]\)</span> es la covarianza de <span
class="math inline">\(X\)</span> y <span
class="math inline">\(Y\)</span>.
<ul>
<li>En el caso en el que <span class="math inline">\(X\)</span> e <span
class="math inline">\(Y\)</span> sean incorreladas (es decir, la
covarianza es <span class="math inline">\(0\)</span>), <span
class="math inline">\(Var[X + Y] =\)</span> <span
class="math inline">\(Var[X] + Var[Y]\)</span>.</li>
</ul></li>
</ul>
<p>La varianza nos será útil a la hora de medir el error cometido por
una estimación de Monte Carlo.</p>
<h3 data-number="3.1.4" id="teoremas-importantes"><span
class="header-section-number">3.1.4</span> Teoremas importantes</h3>
<p>Además de las anteriores propiedades, existen una serie de teoremas
esenciales que necesitaremos más adelante:</p>
<p><strong>Ley del estadístico insconciente</strong> (<em>Law of the
unconscious statistician</em>, o LOTUS): dada una variable aleatoria
<span class="math inline">\(X\)</span> y una función medible <span
class="math inline">\(g\)</span>, la esperanza de <span
class="math inline">\(g(X)\)</span> se puede calcular como</p>
<p><span id="eq:LOTUS" class="eqnos"><span class="math display">\[
E\left[ g(X) \right] = \int_{-\infty}^{\infty}{g(x) f_X(x) dx}
\]</span><span class="eqnos-number">(24)</span></span></p>
<p><strong>Ley (fuerte) de los grandes números</strong>: dada una
muestra de <span class="math inline">\(N\)</span> valores <span
class="math inline">\(X_1, \dots, X_N\)</span> de una variable aleatoria
<span class="math inline">\(X\)</span> con esperanza <span
class="math inline">\(E\left[ X \right] = \mu\)</span>,</p>
<p><span class="math display">\[
P\left[ \lim_{N \to \infty}{\frac{1}{N} \sum_{i = 1}^{N}{X_i}} =
\mu  \right] = 1
\]</span></p>
<p>Usando que <span class="math inline">\(\bar{X}_N = \frac{1}{N}
\sum_{i = 1}^{N}{X_i}\)</span>, esta ley se suele escribir como</p>
<p><span id="eq:ley_numeros_grandes" class="eqnos"><span
class="math display">\[
P\left[ \lim_{N \to \infty}{\bar{X}_N} = \mu  \right] = 1
\]</span><span class="eqnos-number">(25)</span></span></p>
<p>Este teorema es especialmente importante. En esencia, nos dice que
cuando repetimos muchas veces un experimento, al promediar los
resultados obtendremos una esperanza muy cercana a la esperanza
real.</p>
<p><strong>Teorema Central del Límite (CLT) para variables idénticamente
distribuidas</strong> <span class="citation" data-cites="mcbook">(<a
href="#ref-mcbook" role="doc-biblioref">Owen 2013</a>, capítulo
2)</span>: Sean <span class="math inline">\(X_1, \dots, X_N\)</span>
muestras aleatorias simples de una variable aleatoria <span
class="math inline">\(X\)</span> con esperanza <span
class="math inline">\(E\left[ X \right] = \mu\)</span> y varianza <span
class="math inline">\(Var[X] = \sigma^2\)</span>. Sea</p>
<p><span class="math display">\[
Z_N = \frac{\sum_{i = 1}^{N}{X_i - N\mu}}{\sigma \sqrt{N}}
\]</span></p>
<p>Entonces, la variable aleatoria <span
class="math inline">\(Z_N\)</span> converge hacia una función de
distribución normal estándar cuando <span
class="math inline">\(N\)</span> es suficientemente grande:</p>
<p><span id="eq:CLT" class="eqnos"><span class="math display">\[
\lim_{N \to \infty}{P[Z_N \le z]} = \int_{-\infty}^{z}{\frac{1}{\sqrt{2
\pi}} e^{- \frac{x^2}{2}}dx}
\]</span><span class="eqnos-number">(26)</span></span></p>
<h3 data-number="3.1.5" id="estimadores"><span
class="header-section-number">3.1.5</span> Estimadores</h3>
<p>A veces, no podremos conocer de antemano el valor que toma un cierto
parámetro de una distribución. Sin embargo, conocemos el tipo de
distribución que nuestra variable aleatoria <span
class="math inline">\(X\)</span> sigue. Los estimadores nos
proporcionarán una forma de calcular el posible valor de esos parámetros
a partir de una muestra de <span class="math inline">\(X\)</span>.</p>
<p>Sea <span class="math inline">\(X\)</span> una variablea aleatoria
con distribución perteneciente a una familia de distribuciones
paramétricas <span class="math inline">\(X \sim F \in \left\{F(\theta)
\,\middle|\, \theta \in \Theta \right\}\)</span>. <span
class="math inline">\(\Theta\)</span> es el conjunto de valores que
puede tomar el parámetro. Buscamos una forma de determinar el valor de
<span class="math inline">\(\theta\)</span>.</p>
<p>Diremos que <span class="math inline">\(T(X_1, \dots, X_N)\)</span>
es <strong>un estimador de <span
class="math inline">\(\theta\)</span></strong> si <span
class="math inline">\(T\)</span> toma valores en <span
class="math inline">\(\Theta\)</span>.</p>
<p>A los estimadores de un parámetro los solemos denotar con <span
class="math inline">\(\hat{\theta}\)</span>.</p>
<p>Como vemos, la definición no es muy restrictiva. Únicamente le
estamos pidiendo a la función de la muestra que pueda tomar valores
viables para la distribución.</p>
<p>Se dice que un estimador <span class="math inline">\(T(X_1, \dots,
X_N)\)</span> es <strong>insesgado</strong> (o centrado en el parámetro
<span class="math inline">\(\theta\)</span>) si</p>
<p><span class="math display">\[
E\left[ T(X_1, \dots, X_n) \right] = \theta\quad \forall \theta \in
\Theta
\]</span></p>
<p>Naturalmente, decimos que un estimador <span
class="math inline">\(T(X_1, \dots, X_N)\)</span> está
<strong>sesgado</strong> si <span class="math inline">\(E\left[ T(X_1,
\dots, X_N) \right] \not = \theta\)</span>.</p>
<h2 data-number="3.2" id="el-estimador-de-monte-carlo"><span
class="header-section-number">3.2</span> El estimador de Monte
Carlo</h2>
<p>Tras este breve repaso de probabilidad, estamos en condiciones de
definir el estimador de Monte Carlo. Primero, vamos con su versión más
sencilla.</p>
<h3 data-number="3.2.1" id="monte-carlo-básico"><span
class="header-section-number">3.2.1</span> Monte Carlo básico</h3>
<p>Los estimadores de Monte Carlo nos permiten hallar la esperanza de
una variable aleatoria, digamos, <span class="math inline">\(Y\)</span>,
sin necesidad de calcular explícitamente su valor. Para ello, tomamos
<span class="math inline">\(N\)</span> muestras <span
class="math inline">\(Y_1, \dots, Y_N\)</span> de <span
class="math inline">\(Y\)</span>, cuya media vale <span
class="math inline">\(\mu\)</span>. Entonces, el estimador de <span
class="math inline">\(\mu\)</span> <span class="citation"
data-cites="mcbook">(<a href="#ref-mcbook" role="doc-biblioref">Owen
2013</a>, capítulo 2)</span> es:</p>
<p><span id="eq:mc_simple" class="eqnos"><span class="math display">\[
\hat\mu_N = \frac{1}{N} \sum_{i = 1}^{N}{Y_i}
\]</span><span class="eqnos-number">(27)</span></span></p>
<p>La intuición del estimador es, esencialmente, la misma que la del
teorema central del límite. Lo que buscamos es una forma de calcular el
valor promedio de un cierto suceso aleatorio, pero lo único que podemos
usar son muestras de su variable aleatoria. Promediando esas muestras,
sacamos información de la distribución. En este caso, la media.</p>
<p>En cualquier caso, la existencia de este estimador viene dada por la
ley de los grandes números (tanto débil como fuerte [<a
href="#eq:ley_numeros_grandes">25</a>]). Si <span
class="math inline">\(\mu = E\left[ Y \right]\)</span>, se tiene que</p>
<p><span class="math display">\[
\lim_{N \to \infty}P\left[ \left\lvert \hat\mu_N - \mu \right\rvert \le
\varepsilon \right] = 1 \quad \forall\ \varepsilon &gt; 0
\]</span></p>
<p>o utilizando la ley de los números grandes,</p>
<p><span class="math display">\[
\lim_{N \to \infty}P\left[ \left\lvert \hat\mu_N - \mu \right\rvert = 0
\right] = 1
\]</span></p>
<p>Haciendo la esperanza de este estimador, vemos que</p>
<p><span class="math display">\[
\begin{aligned}
E\left[ \hat\mu_N \right] &amp; = E\left[ \frac{1}{N} \sum_{i =
1}^{N}{Y_i}\right] = \frac{1}{N} E\left[\sum_{i = 1}^{N}{Y_i} \right] \\
             &amp; = \frac{1}{N} \sum_{i = 1}^{N}{E\left[ Y_i \right]} =
\frac{1}{N} \sum_{i = 1}^{N}{\mu} = \\
             &amp; = \mu
\end{aligned}
\]</span></p>
<p>Por lo que el estimador es insesgado. Además, se tiene que la
varianza es</p>
<p><span class="math display">\[
E\left[ (\hat\mu_N - \mu)^2 \right] = \frac{\sigma^2}{N}
\]</span></p>
<p>Un ejemplo clásico de estimador de Monte Carlo es calcular el valor
de <span class="math inline">\(\pi\)</span>. Se puede hallar integrando
una función que valga <span class="math inline">\(1\)</span> en el
interior de la circunferencia de radio unidad y <span
class="math inline">\(0\)</span> en el exterior:</p>
<p><span class="math display">\[
\begin{aligned}
f = \begin{cases}
      1 &amp; \text{si } x^2 + y^2 \le 1 \\
      0 &amp; \text{en otro caso}
    \end{cases} \Longrightarrow \pi = \int_{-1}^{1} \int_{-1}^{1}{f(x,
y)}\ dxdy
\end{aligned}
\]</span></p>
<p>Para usar el estimador de [<a href="#eq:mc_integral">28</a>],
necesitamos saber la probabilidad de obtener un punto dentro de la
circunferencia.</p>
<p>Bien, consideremos que una circunferencia de radio <span
class="math inline">\(r\)</span> se encuentra inscrita en un cuadrado.
El área de la circunferencia es <span class="math inline">\(\pi
r^2\)</span>, mientras que la del cuadrado es <span
class="math inline">\((2r)^2 = 4r^2\)</span>. Por tanto, la probabilidad
de obtener un punto dentro de la circunferencia es <span
class="math inline">\(\frac{\pi r^2}{4r^2} = \frac{\pi}{4}\)</span>.
Podemos tomar <span class="math inline">\(p(x, y) =
\frac{1}{4}\)</span>, de forma que</p>
<p><span class="math display">\[
\pi \approx \frac{4}{N} \sum_{i = 1}^{N}{f(x_i, y_i)}, \text{  con }
(x_i, y_i) \sim \mathcal{U}(\small{[-1, 1] \times [-1, 1]})
\]</span></p>
<h3 data-number="3.2.2" id="integración-de-monte-carlo"><span
class="header-section-number">3.2.2</span> Integración de Monte
Carlo</h3>
<p>Generalmente nos encontraremos en la situación en la que <span
class="math inline">\(Y = f(X)\)</span>, donde <span
class="math inline">\(X \in S \subset \mathbb{R}^d\)</span> sigue una
distribución con función de densidad <span
class="math inline">\(p_X(x)\)</span> con media <span
class="math inline">\(\mu = E\left[ X \right]\)</span>, y <span
class="math inline">\(f: S \rightarrow \mathbb{R}\)</span>.</p>
<blockquote>
<p><strong>Nota</strong>(ción): nos encontramos ante un caso de
<em>“choque de notaciones”</em>. Tradicionalmente las funciones de
densidad utilizan <span class="math inline">\(f\)</span>, pero en
transporte de luz se suele reservar esta letra para las BRDF y/u otras
funciones genéricas, dejando la letra <span
class="math inline">\(p\)</span> para las funciones de densidad.</p>
<p>En este capítulo usaré la notación <span
class="math inline">\(p_X\)</span> para dejar claro de que estamos
hablando de una función de densidad, manteniendo <span
class="math inline">\(f\)</span> para la transformación de la variable
aleatoria.</p>
</blockquote>
<p>Consideremos el promedio de <span class="math inline">\(N\)</span>
muestras de <span class="math inline">\(f(X)\)</span>:</p>
<p><span class="math display">\[
\frac{1}{N} \sum_{i = 1}^{N}{f(X_i)}
\]</span></p>
<p>siendo <span class="math inline">\(X_1 \dots X_N\)</span>
idénticamente distribuidas. En ese caso, la esperanza es</p>
<p><span class="math display">\[
\begin{aligned}
E\left[ \frac{1}{N} \sum_{i = 1}^{N}{f(X_i)} \right] &amp; =
E\left[\frac{1}{N} \sum_{i = 1}^{N}{f(X)}  \right] = \\
                                                    &amp; = \frac{1}{N}
N E\left[ f(X) \right] = \\
                                                    &amp; =  E\left[
f(X) \right] = \\
                                                    &amp; = \int_S f(x)
p_X(x) dx
\end{aligned}
\]</span></p>
<p>¡Genial! Esto nos da una forma de <strong>calcular la integral de una
función</strong> usando las imágenes de <span
class="math inline">\(N\)</span> muestras <span
class="math inline">\(f(X_1), \dots, f(X_N)\)</span> de una variable
aleatoria <span class="math inline">\(X \sim p_X\)</span>. A este
estimador de Monte Carlo lo llamaremos <span
class="math inline">\(\hat{I}_N\)</span>:</p>
<p><span id="eq:mc_integral" class="eqnos"><span class="math display">\[
\begin{aligned}
\hat{I}_N &amp; = \frac{1}{N} \sum_{i = 1}^{N}{f(X_i)} \\
          \Rightarrow E\left[ \hat{I}_N \right] &amp; = \int_S f(x)
p_X(x) dx
\end{aligned}
\]</span><span class="eqnos-number">(28)</span></span></p>
<blockquote>
<p><strong>Nota</strong>(ción): si te preguntas por qué lo llamamos
<span class="math inline">\(\hat{I}_N\)</span>, piensa que queremos
calcular la intergal <span class="math inline">\(I =
\int_{S}{f(x)p_X(x)dx}\)</span>. Para ello, usamos el estimador <span
class="math inline">\(\hat{I}\)</span>, y marcamos explícitamente que
usamos <span class="math inline">\(N\)</span> muestras.</p>
</blockquote>
<p>La varianza del estimador se puede calcular fácilmente utilizando las
propiedades que vimos en la <a
href="#esperanza-y-varianza-de-una-variable-aleatoria">sección de la
varianza</a>:</p>
<p><span id="eq:mc_varianza" class="eqnos"><span class="math display">\[
\begin{aligned}
  Var\left[ \hat{I}_N \right]
    &amp; = Var\left[ \frac{1}{N} \sum_{i = 1}^{N}{f(X_i)} \right] = \\
    &amp; = \frac{1}{N^2} Var\left[  \sum_{i = 1}^{N}{f(X_i)} \right] =
\\
    &amp; = \frac{1}{N^2} N Var\left[ f(X) \right] = \\
    &amp; = \frac{1}{N} Var\left[ f(X) \right]
\end{aligned}
\]</span><span class="eqnos-number">(29)</span></span></p>
<p>Como es natural, el número de muestras que usemos será clave para la
proximidad de la estimación. ¿Cómo <em>de lejos</em> se queda del valor
real de la integral <span class="math inline">\(E\left[ f(X)
\right]\)</span>? Es decir; ¿cómo modifica <span
class="math inline">\(N\)</span> la varianza del estimador <span
class="math inline">\(Var\left[ \hat{I}_N \right]\)</span>?</p>
<p>Para comprobarlo, debemos introducir dos nuevos teoremas: la
desigualdad de Markov y la desigualdad de Chebyshsev <span
class="citation" data-cites="metodos-monte-carlo">(<a
href="#ref-metodos-monte-carlo" role="doc-biblioref">Illana 2013</a>,
Introducción)</span>.</p>
<p><strong>Desigualdad de Markov</strong>: Sea <span
class="math inline">\(X\)</span> una variable aleatoria que toma valores
no negativos, y sea <span class="math inline">\(p_X\)</span> su función
de densidad. Entonces, <span class="math inline">\(\forall x &gt;
0\)</span>,</p>
<p><span id="eq:desigualdad_markov" class="eqnos"><span
class="math display">\[
\begin{aligned}
E\left[ X \right]  &amp;   =  \int_0^x t p_X(t)\ dt + \int_x^\infty t
p_X(t)\ dt \ge  \int_x^\infty t p_X(t)\ dt \\
      &amp; \ge  \int_x^\infty x p_X(t)\ dt = x P\left[ X \ge x \right]
\\
      &amp; \Rightarrow P\left[ X \ge x \right] \le \frac{E\left[ X
\right]}{x}
\end{aligned}
\]</span><span class="eqnos-number">(30)</span></span></p>
<p><strong>Desigualdad de Chebyshev</strong>: Sea <span
class="math inline">\(X\)</span> una variable aleatoria con esperanza
<span class="math inline">\(\mu = E\left[ X \right]\)</span> y varianza
<span class="math inline">\(\sigma^2 = E\left[ (X - \mu)^2
\right]\)</span>. Entonces, aplicando la desigualdad de Markov [<a
href="#eq:desigualdad_markov">30</a>] a <span class="math inline">\(D^2
= (X - \mu)^2\)</span> se tiene que</p>
<p><span id="eq:desigualdad_chebyshev" class="eqnos"><span
class="math display">\[
\begin{aligned}
       P\left[ D^2 \ge x^2 \right]         &amp; \le
\frac{\sigma^2}{x^2} \\
  \iff P\left[ \left\lvert X - \mu \right\rvert \ge x \right] &amp; \le
\frac{\sigma^2}{x^2}
\end{aligned}
\]</span><span class="eqnos-number">(31)</span></span></p>
<p>Ahora que tenemos estas dos desigualdades, apliquemos la de Chebyshev
a [<a href="#eq:mc_integral">28</a>] con <span
class="math inline">\(\sigma^2 = Var\left[ \hat{I}_N \right]\)</span>,
<span class="math inline">\(x^2 = \sigma^2/\varepsilon, \varepsilon &gt;
0\)</span>:</p>
<p><span class="math display">\[
P\left[ \left\lvert \hat{I}_N - E\left[ \hat{I}_N \right] \right\rvert
\ge \left(\frac{Var[\hat{I}_N]}{\varepsilon}\right)^{1/2} \right] \le
\varepsilon
\]</span></p>
<p>Esto nos dice que, usando un número de muestras relativamente grande
(<span class="math inline">\(N &gt;&gt; \frac{1}{\varepsilon}\)</span>),
es prácticamente imposible que el estimador se aleje de <span
class="math inline">\(E\left[ f(X) \right]\)</span>.</p>
<p>La desviación estándar puede calcularse fácilmente a partir de la
varianza:</p>
<p><span id="eq:desviacion_estandar" class="eqnos"><span
class="math display">\[
\sqrt{Var[\hat{I}_N]} = \frac{\sqrt{Var\left[ f(X) \right]}}{\sqrt{N}}
\]</span><span class="eqnos-number">(32)</span></span></p>
<p>así que, como adelantamos al inicio del capítulo, la estimación tiene
un error del orden <span
class="math inline">\(\mathcal{O}(N^{-1/2})\)</span>. Esto nos dice que,
para reducir el error a la mitad, debemos tomar 4 veces más
muestras.</p>
<p>Es importante destacar la <strong>ausencia del parámetro de la
dimensión</strong>. Sabemos que <span class="math inline">\(X \in S
\subset \mathbb{R}^d\)</span>, pero en ningún momento aparece <span
class="math inline">\(d\)</span> en la expresión de la desviación
estándar [<a href="#eq:desviacion_estandar">32</a>]. Este hecho es una
de las ventajas de la integración de Monte Carlo.</p>
<h4 data-number="3.2.2.1" id="un-ejemplo-práctico-en-r"><span
class="header-section-number">3.2.2.1</span> Un ejemplo práctico en
R</h4>
<p>Hagamos un ejemplo práctico para visualizar lo que hemos aprendido en
el software estadístico <strong>R</strong>.</p>
<p>Supongamos que queremos integrar la función <span
class="math inline">\(f: [0, 1] \rightarrow \mathbb{R}\)</span>, <span
class="math inline">\(f(x) = 2x^4\)</span>. Es decir, queremos
calcular</p>
<p><span class="math display">\[
\int_0^1{2x^4\ dx}
\]</span></p>
<p>El valor de esta integral es <span class="math inline">\(2
\left[\frac{x^5}{5}\right]_0^1 = 2/5 = 0.4\)</span>.</p>
<p>Primero, definimos la función <span
class="math inline">\(f\)</span>:</p>
<pre><code class="language-r">f &lt;- function(x) {
  2 * x^4 * (x &gt; 0 &amp; x &lt; 1)
}</code></pre>
<p>Tomamos N muestras en el intervalo <span class="math inline">\([0,
1]\)</span> de forma uniforme:</p>
<pre><code class="language-r">N &lt;- 1000
x &lt;- runif(N)       # x1, ...., xn
f_x &lt;- sapply(x, f) # f(x1), ..., f(xn)
mean(f_x)           # -&gt; 0.3891845</code></pre>
<p>Observamos que el valor se queda muy cerca de <span
class="math inline">\(0.4\)</span>. El error en este caso es <span
class="math inline">\(0.4 - 0.3891845 = 0.01081546\)</span>.</p>
<p>Es interesante estudiar cómo de rápido converge el estimador al valor
de la integral. Con el siguiente código, podemos caclular el error en
función del número de muestras <span
class="math inline">\(N\)</span>:</p>
<pre><code class="language-r"># Calcular la media y su error
estimacion &lt;- cumsum(f_x) / (1:N)
error &lt;- sqrt(cumsum((f_x - estimacion)^2)) / (1:N)

# Gráfico
plot(1:N, estimacion,
    type = &quot;l&quot;,
    ylab = &quot;Aproximación y límites del error (1 - alpha = 0.975)&quot;,
    xlab = &quot;Número de simulaciones&quot;,
)
z &lt;- qnorm(0.025, lower.tail = FALSE)
lines(estimacion - z * error, col = &quot;blue&quot;, lwd = 2, lty = 3)
lines(estimacion + z * error, col = &quot;blue&quot;, lwd = 2, lty = 3)
abline(h = 0.4, col = 2)</code></pre>
<p>Este código produce la siguiente gráfica:</p>
<p><img loading="lazy" src="./img/03/Error%20simulación.png" id="fig:error_simulacion"
alt="Error de la simulación para el estimador de la integral \int_0^1{2x^4\ dx}" />
{width=80%}</p>
<p>Se puede ver cómo debemos usar un número considerable de muestras,
alrededor de 200, para que el error se mantenga bajo control. Aún así,
aumentar el tamaño de <span class="math inline">\(N\)</span> no
disminuye necesariamente el error; nos encontramos en una situación de
rendimientos decrecientes.</p>
<h2 data-number="3.3" id="técnicas-de-reducción-de-varianza"><span
class="header-section-number">3.3</span> Técnicas de reducción de
varianza</h2>
<h3 data-number="3.3.1" id="muestreo-por-importancia"><span
class="header-section-number">3.3.1</span> Muestreo por importancia</h3>
<p>Como hemos visto, <span class="math inline">\(Var\left[ \hat{I}_N
\right]\)</span> depende del número de muestras <span
class="math inline">\(N\)</span> y de <span
class="math inline">\(Var\left[ f(X) \right]\)</span>. Aumentar el
tamaño de <span class="math inline">\(N\)</span> es una forma fácil de
reducir la varianza, pero rápidamente llegaríamos a una situación de
retornos reducidos <span class="citation" data-cites="PBRT3e">(<a
href="#ref-PBRT3e" role="doc-biblioref">Pharr, Jakob, and Humphreys
2016</a>, The Monte Carlo Estimator)</span>. ¿Podemos hacer algo con el
término <span class="math inline">\(Var\left[ f(X) \right]\)</span>?</p>
<p>Vamos a jugar con él.</p>
<p>La integral que estamos evaluando ahora mismo es <span
class="math inline">\(\int_S{f(x)p_X(x)}dx\)</span>, con <span
class="math inline">\(p_X\)</span> una función de densidad sobre <span
class="math inline">\(S \subset \mathbb{R}^d\)</span> <span
class="math inline">\(\Rightarrow p_X(x) = 0\ \forall x \notin
S\)</span>. Ahora bien, si <span class="math inline">\(q_X\)</span> es
otra función de densidad en <span
class="math inline">\(\mathbb{R}^d\)</span>, entonces <span
class="citation" data-cites="mcbook">(<a href="#ref-mcbook"
role="doc-biblioref">Owen 2013</a>, Importance Sampling)</span>:</p>
<p><span class="math display">\[
I = \int_S{f(x)p_X(x)dx} = \int_S{\frac{f(x)p_X(x)}{q_X(x)}q_X(x)dx} =
E\left[ \frac{f(X)p_X(X)}{q_X(X)} \right]
\]</span></p>
<p>Esta última esperanza depende de <span
class="math inline">\(q_X\)</span>. Nuestro objetivo era encontrar <span
class="math inline">\(E\left[ f(X) \right]\)</span>, pero podemos
hacerlo tomando un término nuevo para muestrear desde <span
class="math inline">\(q_X\)</span> en vez de <span
class="math inline">\(p_X\)</span>. Al factor <span
class="math inline">\(\frac{p_X}{q_X}\)</span> lo llamamos
<strong>cociente de probabilidad</strong>, con <span
class="math inline">\(q_X\)</span> la <strong>distribución de
importancia</strong> y <span class="math inline">\(p_X\)</span> la
<strong>distribución nominal</strong>.</p>
<p>No es necesario que <span class="math inline">\(q_X\)</span> sea
positiva en todo punto. Con que se cumpla que <span
class="math inline">\(q_X(x) &gt; 0\)</span> cuando <span
class="math inline">\(f(x)p_X(x) \not = 0\)</span> es suficiente. Es
decir, para <span class="math inline">\(Q = \left\{x \,\middle|\, q_X(x)
&gt; 0  \right\}\)</span>, tenemos que <span class="math inline">\(x \in
Q\)</span> cuando <span class="math inline">\(f(x)p_X(x) \not =
0\)</span>. Así, si <span class="math inline">\(x \in S \cap Q^c
\Rightarrow f(X) = 0\)</span>, mientras que si <span
class="math inline">\(x \in S^c \cap Q \Rightarrow p_X(X) \neq
0\)</span>. Entonces,</p>
<p><span class="math display">\[
\begin{aligned}
E\left[ \frac{f(X)p_X(X)}{q_X(X)} \right] &amp; =
\int_Q{\frac{f(x)p_X(x)}{q_X(x)}q_X(x)dx} = \int_Q{f(x)p_X(x)dx} = \\
                              &amp; = \int_Q{f(x)p_X(x)dx} + \int_{S^c
\cap Q}{f(x)p_X(x)dx} - \int_{S \cap Q^c}{f(x)p_X(x)dx} =  \\
                              &amp; = \int_S{f(x)p_X(x)dx} = \\
                              &amp; = E\left[ f(X) \right]
\end{aligned}
\]</span></p>
<p>De esta forma, hemos llegado al <strong>estimador de Monte Carlo por
importancia</strong>:</p>
<p><span id="eq:mc_integral_importancia" class="eqnos"><span
class="math display">\[
\tilde{I}_N = \frac{1}{N} \sum_{i=1}^N{\frac{f(X_i)p_X(X_i)}{q_X(X_i)}},
\quad X_i \sim q_X
\]</span><span class="eqnos-number">(33)</span></span></p>
<blockquote>
<p><strong>Nota</strong>(ción): ¡fíjate en el gusanito! <span
class="math inline">\(\hat{I}_N\)</span> [<a
href="#eq:mc_integral">28</a>] y <span
class="math inline">\(\tilde{I}_N\)</span> tienen la misma esperanza,
pero son estimadores diferentes.</p>
</blockquote>
<p>Vamos a calcular ahora la varianza de este estimador. Sea <span
class="math inline">\(\mu = E\left[ f(X) \right]\)</span></p>
<p><span class="math display">\[
\begin{aligned}
Var\left[ \tilde{I}_N \right] &amp; = \frac{1}{N} \left(
\int_Q{\left(\frac{f(x)p_X(x)}{q_X(x)}\right)^2q_X(x) dx} -
\mu^2  \right) = \\
                  &amp; = \frac{1}{N}
\textcolor{verde-oscurisimo}{\left(
\int_Q{\frac{\left(f(x)p_X(x)\right)^2}{q_X(x)} dx} - \mu^2  \right)} =
\\
                  &amp; =
\frac{\textcolor{verde-oscurisimo}{\sigma^2_q}}{N}
\end{aligned}
\]</span></p>
<p>La clave de este método reside en escoger una buena distribución de
importancia. Puede probarse que la función de densidad que minimiza
<span class="math inline">\(\sigma^2_q\)</span> es proporcional a <span
class="math inline">\(\left\lvert f(x) \right\rvert p_X(x)\)</span>
<span class="citation" data-cites="mcbook">(<a href="#ref-mcbook"
role="doc-biblioref">Owen 2013, 6</a>)</span>.</p>
<h4 data-number="3.3.1.1"
id="muestreo-por-importancia-en-transporte-de-luz"><span
class="header-section-number">3.3.1.1</span> Muestreo por importancia en
transporte de luz</h4>
<p>Esta técnica es especialmente importante en nuestra área de estudio.
En transporte de luz, buscamos calcular el valor de la rendering
equation [<a href="#eq:rendering_equation">23</a>]. Específicamente, de
la integral</p>
<p><span class="math display">\[
\int_{H^2(\mathbf{n})}{BSDF(p, \omega_o \leftarrow \omega_i) L_i(p,
\omega_i) \cos\theta_i\ d\omega_i}
\]</span></p>
<p>que se suele representar como una simple integral sobre un cierto
conjunto <span class="math inline">\(\int_S{f(x)dx}\)</span>. En la
literatura se usa una versión modificada de muestreo por
importancia:</p>
<p><span id="eq:mc_integral_tl" class="eqnos"><span
class="math display">\[
\tilde{I}_N = \frac{1}{N} \sum_{i=1}^N{\frac{f(X_i)}{p_X(X_i)}}
\]</span><span class="eqnos-number">(34)</span></span></p>
<p>para que, utilizando muestras <span class="math inline">\(X_i \sim
p_X\)</span>, <span class="math inline">\(E\left[ \frac{f}{p_X} \right]
= \int_S{\frac{f}{p_X}p_X}\)</span> y así se evalúe directamente la
integral de <span class="math inline">\(f\)</span>. En cualquiera de los
casos, el fundamento teórico es el mismo <span class="citation"
data-cites="berkeley-mc-lecture">(<a href="#ref-berkeley-mc-lecture"
role="doc-biblioref">Anderson 1999</a>)</span>.</p>
<p>Esta forma de escribir el estimador nos permite amenizar algunos
casos particulares. Por ejemplo, si usamos muestras <span
class="math inline">\(X_i\)</span> que sigan una distribución uniforme
en <span class="math inline">\([a, b]\)</span>, entonces, su función de
densidad es <span class="math inline">\(p_X(x) = \frac{1}{b -
a}\)</span>. Esto da lugar a</p>
<p><span class="math display">\[
\tilde{I}_N = \frac{b - a}{N} \sum_{i = 1}^{N}{f(X_i)}
\]</span></p>
<p><strong>En lo que resta de capítulo, se utilizará indistintamente
<span class="math inline">\(\frac{1}{N}
\sum_{i=1}^N{\frac{f(X_i)p_X(X_i)}{q_X(X_i)}}\)</span> o <span
class="math inline">\(\frac{1}{N}
\sum_{i=1}^N{\frac{f(X_i)}{p_X(X_i)}}\)</span> según convenga</strong>.
¡Tenlo en cuenta!</p>
<p>Usando esta expresión, la distribución de importancia <span
class="math inline">\(p_X\)</span> que hace decrecer la varianza es
aquella proporcional a <span class="math inline">\(f\)</span>. Es decir,
supongamos que <span class="math inline">\(f \propto p_X\)</span>. Esto
es, existe un <span class="math inline">\(s\)</span> tal que <span
class="math inline">\(f(x) = s p_X(x)\)</span>. Como <span
class="math inline">\(p_X\)</span> debe integrar uno, podemos calcular
el valor de <span class="math inline">\(s\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
  \int_{S}{p_X(x)dx} &amp; = \int_{S}{sf(x)dx} = 1 \quad \iff \\
  s &amp; = \frac{1}{\int_{S}{f(x)dx}}
\end{aligned}
\]</span></p>
<p>Y entonces, se tendría que</p>
<p><span class="math display">\[
\begin{aligned}
  Var\left[ \frac{f(X)}{p_X(X)}\right] &amp; =
Var\left[\frac{f(X)}{sf(X)} \right] = \\
  &amp; = Var\left[ \frac{1}{s} \right] = \\
  &amp; = 0
\end{aligned}
\]</span></p>
<p>En la práctica, esto es inviable. El problema que queremos resolver
es calcular la integral de <span class="math inline">\(f\)</span>. Y
para sacar <span class="math inline">\(s\)</span>, necesitaríamos el
valor de la integral de <span class="math inline">\(f\)</span>. ¡Estamos
dando vueltas!</p>
<p>Por fortuna, hay algoritmos que son capaces de proporcionar la
constante <span class="math inline">\(s\)</span> sin necesidad de
calcular la integral. Uno de los más conocidos es
<strong>Metropolis-Hastings</strong>, el cual se basa en cadenas de
Markov de Monte Carlo. Sin embargo, su complejidad hace que se escape
del ámbito de este trabajo. Se puede encontrar más información en <span
class="citation" data-cites="PBRT3e">(<a href="#ref-PBRT3e"
role="doc-biblioref">Pharr, Jakob, and Humphreys 2016</a>, Metropolis
Light Transport)</span>.</p>
<p>En este trabajo nos centraremos en buscar funciones de densidad <span
class="math inline">\(p_X\)</span> que se aproximen a <span
class="math inline">\(f\)</span> lo más fielmente posible, dentro del
contexto del transporte de luz.</p>
<p>Pongamos un ejemplo de estimador de Monte Carlo para una caja de
dimensiones <span class="math inline">\(\small{[x_0, x_1] \times [y_0,
y_1] \times [z_0, z_1]}\)</span>. Si queremos estimar la integral de la
función <span class="math inline">\(f: \mathbb{R}^3 \rightarrow
\mathbb{R}\)</span></p>
<p><span class="math display">\[
\int_{x_0}^{x_1} \int_{y_0}^{y_1} \int_{z_0}^{z_1}{f(x, y, z)dx dy dz}
\]</span></p>
<p>mediante una variable aleatoria <span class="math inline">\(X \sim
\mathcal{U}(\small{[x_0, x_1] \times [y_0, y_1] \times [z_0,
z_1]})\)</span> con función de densidad <span class="math inline">\(p(x,
y, z) = \frac{1}{x_1 - x_0} \frac{1}{y_1 - y_0} \frac{1}{z_1 -
z_0}\)</span>, tomamos el estimador</p>
<p><span class="math display">\[
\tilde{I}_N = \frac{1}{(x_1 - x_0) \cdot (y_1 - y_0) \cdot (z_1 - z_0)}
\sum_{i = 1}^{N}{f(X_i)}
\]</span></p>
<h3 data-number="3.3.2" id="muestreo-por-importancia-múltiple"><span
class="header-section-number">3.3.2</span> Muestreo por importancia
múltiple</h3>
<p>Las técnicas de <a href="#muestreo-por-importancia">muestreo por
importancia</a> nos proporcionan estimadores para una integral de la
forma <span class="math inline">\(\int{f(x)dx}\)</span>. Sin embargo, es
frecuente encontrarse un producto de dos funciones, <span
class="math inline">\(\int{f(x)g(x)dx}\)</span>. Si tuviéramos una forma
de coger muestras para <span class="math inline">\(f\)</span>, y otra
para <span class="math inline">\(g\)</span>, ¿cuál deberíamos usar?</p>
<p>Se puede utilizar un nuevo estimador de Monte Carlo, que viene dado
por <span class="citation" data-cites="PBRT3e">(<a href="#ref-PBRT3e"
role="doc-biblioref">Pharr, Jakob, and Humphreys 2016</a>)</span></p>
<p><span class="math display">\[
\frac{1}{N_f} \sum_{i = 1}^{N_f}{\frac{f(X_i)g(X_i)w_f(X_i)}{p_f(X_i)}}
+ \frac{1}{N_g} \sum_{j =
1}^{N_g}{\frac{f(Y_j)g(Y_j)w_g(Y_j)}{p_g(Y_j)}}
\]</span></p>
<p>donde <span class="math inline">\(N_f\)</span> y <span
class="math inline">\(N_g\)</span> son el número de muestras tomadas
para <span class="math inline">\(f\)</span> y <span
class="math inline">\(g\)</span> respectivamente, <span
class="math inline">\(p_f, p_g\)</span> las funciones de densidad
respectivas y <span class="math inline">\(w_f, w_g\)</span> funciones de
peso escogidas tales que la esperanza del estimador sea <span
class="math inline">\(\int{f(x)g(x)dx}\)</span>.</p>
<p>Estas funciones peso suelen tener en cuenta todas las formas
diferentes que hay de generar muestras para <span
class="math inline">\(X_i\)</span> e <span
class="math inline">\(Y_j\)</span>. Por ejemplo, una de las que podemos
usar es la heurística de balanceo:</p>
<p><span class="math display">\[
w_s(x) = \frac{N_s p_s(x)}{\sum_{i}{N_i p_i(x)}}
\]</span></p>
<p>Una modificación de esta es la heurística potencial (<em>power
heuristic</em>):</p>
<p><span class="math display">\[
w_s(x, \beta) = \frac{\left(N_s p_s(x)\right)^\beta}{\sum_{i}{\left(N_i
p_i(x)\right)^\beta}}
\]</span></p>
<p>la cual reduce la varianza con respecto a la heurística de balanceo.
Un valor para <span class="math inline">\(\beta\)</span> habitual es
<span class="math inline">\(\beta = 2\)</span>.</p>
<h4 data-number="3.3.2.1"
id="muestreo-por-importancia-múltiple-en-transporte-de-luz"><span
class="header-section-number">3.3.2.1</span> Muestreo por importancia
múltiple en transporte de luz</h4>
<p>Si queremos evaluar la contribución de luz en un punto teniendo en
cuenta la luz directa, la expresión utilizada es</p>
<p><span class="math display">\[
L_o(p, \omega_o) = \int_{S^2}{f(p, \omega_o \leftarrow \omega_i)
L_{directa}(p, \omega_i) \cos\theta_i\ d\omega_i}
\]</span></p>
<p>Si utilizáramos muestreo por importancia basándonos en las
distribuciones de <span class="math inline">\(L_{directa}\)</span> o
<span class="math inline">\(f\)</span> por separado, algunas de las dos
no rendiría especialmente bien. Combinando ambas mediante muestreo por
importancia múltiple se conseguiría un mejor resultado.</p>
<div id="fig:multiple_importance_sampling" class="fignos">
<figure>
<img loading="lazy" src="./img/03/Multiple%20importance%20sampling.png"
style="width:60.0%"
alt="Figura 12: Muestreo por importancia múltiple en transporte de luz ilustrado. Fuente: (Veach December 1997, Multiple Importance Sampling)" />
<figcaption aria-hidden="true"><span>Figura 12:</span> Muestreo por
importancia múltiple en transporte de luz ilustrado. Fuente: <span
class="citation" data-cites="robust-monte-carlo">(<a
href="#ref-robust-monte-carlo" role="doc-biblioref">Veach December
1997</a>, Multiple Importance Sampling)</span></figcaption>
</figure>
</div>
<h3 data-number="3.3.3"
id="otras-técnicas-de-reducción-de-varianza-en-transporte-de-luz"><span
class="header-section-number">3.3.3</span> Otras técnicas de reducción
de varianza en transporte de luz</h3>
<p>Hasta ahora, la principal técnica estudiada ha sido muestreo por
importancia (sea o no múltiple). Esto no quiere decir que sea la única.
Al contrario; esas dos son de las más sencillas que se pueden usar.</p>
<p>En esta sección vamos a ver de forma breve otras formas de reducir la
varianza de un estimador, centrádonos específicamente en el contexto de
transporte de luz.</p>
<h4 data-number="3.3.3.1" id="ruleta-rusa"><span
class="header-section-number">3.3.3.1</span> Ruleta rusa</h4>
<p>Un problema habitual en la práctica es saber cuándo terminar la
propagación de un rayo. Una solución simple es utilizar un parámetro de
profundidad –lo cual hemos implementado en el motor–. Otra opción más
eficiente es utilizar el método de <strong>ruleta rusa</strong>.</p>
<p>En esencia, la idea es que se genere un número aleatorio <span
class="math inline">\(\xi \in [0, 1)\)</span>. Si <span
class="math inline">\(\xi &lt; p_i\)</span>, el camino del rayo se
continúa, pero multiplicando la radiancia acumulada por <span
class="math inline">\(L_i(p, \omega_o \leftarrow \omega_i)/p_i\)</span>.
En otro caso (i.e., si <span class="math inline">\(\xi \ge
p_i\)</span>), el rayo se descarta. Esto hace que se acepten caminos más
fuertes, rechazando aquellas rutas con excesivo ruido.</p>
<p>Más información puede encontrarse en <span class="citation"
data-cites="PBRT3e">(<a href="#ref-PBRT3e" role="doc-biblioref">Pharr,
Jakob, and Humphreys 2016</a>, Russian Roulette and
Splitting)</span>.</p>
<h4 data-number="3.3.3.2"
id="next-event-estimation-o-muestreo-directo-de-fuentes-de-luz"><span
class="header-section-number">3.3.3.2</span> Next event estimation, o
muestreo directo de fuentes de luz</h4>
<p>Esta técnica recibe dos nombres. Tradicionalmente, se la conocía como
muestreo directo de fuentes de luz, pero en los últimos años ha adoptado
el nombre de next event estimation. Esencialmente, se trata de utilizar
las luces de la escena para calcular la radiancia de un punto.</p>
<p>Podemos dividir la rendering equation [<a
href="#eq:rendering_equation">23</a>] en dos sumandos <span
class="citation" data-cites="carlos-path-tracing">(<a
href="#ref-carlos-path-tracing" role="doc-biblioref">Ureña
2021</a>)</span>:</p>
<p><span class="math display">\[
L(p, \omega_o) = L_e(p, \omega_o) + \underbrace{L_{directa}(p, \omega_o
\leftarrow \omega_i) + L_{indirecta}(p, \omega_o \leftarrow
\omega_i)}_{\text{La parte integral de la rendering equation}}
\]</span></p>
<p>siendo <span class="math inline">\(L_e\)</span> la radiancia emitida
por la superficie, <span class="math inline">\(L_{directa}\)</span> la
radiancia proporcionada por las fuentes de luz y <span
class="math inline">\(L_{indirecta}\)</span> la radiancia indirecta.</p>
<p><span class="math display">\[
\begin{aligned}
  L_{directa}   &amp; = \int_{S^2}{f(p, \omega_o \leftarrow \omega_i)
L_{e}(y, -\omega_i) \cos\theta_i\ d\omega_i} \\
  L_{indirecta} &amp; = \int_{S^2}{f(p, \omega_o \leftarrow \omega_i)
L_{i}(y \omega_o \leftarrow \omega_i) \cos\theta_i\ d\omega_i}
\end{aligned}
\]</span></p>
<p>siendo <span class="math inline">\(y\)</span> el primer punto visible
desde <span class="math inline">\(p\)</span> en la dirección <span
class="math inline">\(\omega_i\)</span> situado en una fuente de
luz.</p>
<p>En cada punto de intersección <span class="math inline">\(p\)</span>,
escogeremos aleatoriamente un punto <span
class="math inline">\(y\)</span> en la fuente de luz, y calcularemos
<span class="math inline">\(L_{directa}\)</span>. Esta integral es fácil
de conseguir con las técnicas que ya conocemos. Sin embargo, <span
class="math inline">\(L_{indirecta}\)</span> cuesta más trabajo. Al
aparecer la radiancia incidente en el punto <span
class="math inline">\(p, L_i(p, \omega_o \leftarrow \omega_i)\)</span>,
necesitaremos evaluarla de forma recursiva trazando rayos en la
escena.</p>
<p>Aunque estamos haciendo más cálculos en cada punto de la cadena de
ray trace, al evaluar por separado <span
class="math inline">\(L_{directa}\)</span> y <span
class="math inline">\(L_{indirecta}\)</span> conseguimos reducir
considerablemente la varianza. Por tanto, suponiendo fija la varianza,
el coste computacional de un camino es mayor, pero el coste total es más
bajo.</p>
<p>Esta técnica requiere conocer si desde el punto <span
class="math inline">\(p\)</span> se puede ver <span
class="math inline">\(y\)</span> en la fuente de luz. Es decir, ¿hay
algún objeto en medio de <span class="math inline">\(p\)</span> e <span
class="math inline">\(y\)</span>? Para ello, se suele utilizar lo que se
conocen como <strong><em>shadow rays</em></strong>. Dispara uno de estos
rayos para conocer si está ocluido.</p>
<div id="fig:next_event_estimation" class="fignos">
<figure>
<img loading="lazy" src="./img/03/Next%20event%20estimation.png" style="width:67.0%"
alt="Figura 13: El muestreo directo de fuentes de luz cambia la forma de calcular la radiancia en un punto, pero mejora considerablemente el ruido de una imagen. Fuente: (Ureña 2021)" />
<figcaption aria-hidden="true"><span>Figura 13:</span> El muestreo
directo de fuentes de luz cambia la forma de calcular la radiancia en un
punto, pero mejora considerablemente el ruido de una imagen. Fuente:
<span class="citation" data-cites="carlos-path-tracing">(<a
href="#ref-carlos-path-tracing" role="doc-biblioref">Ureña
2021</a>)</span></figcaption>
</figure>
</div>
<p>Si quieres informarte más sobre esta técnica, puedes leer <span
class="citation" data-cites="GemsI-IS">(<a href="#ref-GemsI-IS"
role="doc-biblioref">Moreau and Clarberg 2019</a>)</span>.</p>
<h4 data-number="3.3.3.3" id="quasi-monte-carlo"><span
class="header-section-number">3.3.3.3</span> Quasi-Monte Carlo</h4>
<p>Generalmente, en los estimadores de Monte Carlo se utilizan variables
aleatorias distribuidas uniformemente a las que se le aplican
transformaciones, pues resulta más sencillo generar un número aleatorio
de la primera manera que de la segunda. La idea de los quasi-Monte Carlo
es muestrear puntos que, de la manera posible, se extiendan
uniformemente en <span class="math inline">\([0, 1]^d\)</span>; evitando
así clústeres y zonas vacías <span class="citation"
data-cites="mcbook">(<a href="#ref-mcbook" role="doc-biblioref">Owen
2013</a>, Quasi-Monte Carlo)</span>.</p>
<p>Existen varias formas de conseguir esto. Algunas de las más famosas
son las secuencias de Sobol, que son computacionalmente caras pero
presentan menores discrepancias; o las series de Halton, que son más
fáciles de conseguir.</p>
<p>Se puede estudiar el tema en profundidad en <span class="citation"
data-cites="quasi-monte-carlo">(<a href="#ref-quasi-monte-carlo"
role="doc-biblioref">Roberts 2018</a>)</span></p>
<div id="fig:quasimontecarlo" class="fignos">
<figure>
<img loading="lazy" src="./img/03/Quasi-Monte%20Carlo.png"
alt="Figura 14: Comparativa entre diferentes métodos de quasi-aleatoriedad. Fuente: (Roberts 2018)" />
<figcaption aria-hidden="true"><span>Figura 14:</span> Comparativa entre
diferentes métodos de quasi-aleatoriedad. Fuente: <span class="citation"
data-cites="quasi-monte-carlo">(<a href="#ref-quasi-monte-carlo"
role="doc-biblioref">Roberts 2018</a>)</span></figcaption>
</figure>
</div>
<h2 data-number="3.4" id="escogiendo-puntos-aleatorios"><span
class="header-section-number">3.4</span> Escogiendo puntos
aleatorios</h2>
<p>Una de las partes clave del estimador de Monte Carlo [<a
href="#eq:mc_integral">28</a>] es saber escoger la función de densidad
<span class="math inline">\(p_X\)</span> correctamente. En esta sección,
veremos algunos métodos para conseguir distribuciones específicas
partiendo de funciones de densidad sencillas, así como formas de elegir
funciones de densidad próximas a <span class="math inline">\(f\)</span>.
Los dos métodos principales que estudiaremos se han extraído del libro
<span class="citation" data-cites="PBRT3e">(<a href="#ref-PBRT3e"
role="doc-biblioref">Pharr, Jakob, and Humphreys 2016</a>, Sampling
Random Variables)</span></p>
<h3 data-number="3.4.1" id="método-de-la-transformada-inversa"><span
class="header-section-number">3.4.1</span> Método de la transformada
inversa</h3>
<p>Este método nos permite conseguir muestras de cualquier distribución
continua a partir de variables aleatorias uniformes, siempre que se
conozca la inversa de la función de distribución.</p>
<p>Sea <span class="math inline">\(X\)</span> una variable aleatoria con
función de distribución <span class="math inline">\(F_X\)</span><a
href="#fn3" class="footnote-ref" id="fnref3"
role="doc-noteref"><sup>3</sup></a>. Queremos buscar una transformación
<span class="math inline">\(T: [0, 1] \rightarrow \mathbb{R}\)</span>
tal que <span class="math inline">\(T(\xi) \stackrel{\text{\small d}}{=}
X\)</span>, siendo <span class="math inline">\(\xi\)</span> una v.a.
uniformemente distribuida. Para que esto se cumpla, se debe dar</p>
<p><span class="math display">\[
\begin{aligned}
F_X(x) &amp; = P[X &lt; x] = \\
       &amp; = P[T(\xi) &lt; x] = \\
       &amp; = P(\xi &lt; T^{-1}(x)) = \\
       &amp; = T^{-1}(x)
\end{aligned}
\]</span></p>
<p>Este último paso se debe a que, como <span
class="math inline">\(\xi\)</span> es uniforme en <span
class="math inline">\((0, 1)\)</span>, <span class="math inline">\(P[\xi
&lt; x] = x\)</span>. Es decir, hemos obtenido que <span
class="math inline">\(F_X\)</span> es la inversa de <span
class="math inline">\(T\)</span>.</p>
<p><strong>En resumen</strong>: Para conseguir una muestra de una
distribución específica <span class="math inline">\(F_X\)</span>,
podemos seguir el siguiente algoritmo:</p>
<ol type="1">
<li>Generar un número aleatorio <span class="math inline">\(\xi \sim
\mathcal{U}(0, 1)\)</span>.</li>
<li>Hallar la inversa de la función de distribución deseada <span
class="math inline">\(F_X\)</span>, denotada <span
class="math inline">\(F_X^{-1}(x)\)</span>.</li>
<li>Calcular <span class="math inline">\(F_X^{-1}(\xi) =
X\)</span>.</li>
</ol>
<blockquote>
<p>TODO: dibujo similar a <a
href="https://cs184.eecs.berkeley.edu/public/sp22/lectures/lec-12-monte-carlo-integration/lec-12-monte-carlo-integration.pdf">este:
p.52</a></p>
</blockquote>
<h4 data-number="3.4.1.1"
id="ejemplo-práctico-de-la-transformada-inversa-para-x2"><span
class="header-section-number">3.4.1.1</span> Ejemplo práctico de la
transformada inversa para <span class="math inline">\(x^2\)</span></h4>
<p>Como ejemplo, vamos a muestrear la función <span
class="math inline">\(f(x) = x^2,\ x \in [0, 2]\)</span> <span
class="citation" data-cites="berkeley-cs184">(<a
href="#ref-berkeley-cs184" role="doc-biblioref">Berkeley cs184 2022</a>,
Monte Carlo Integration)</span>.</p>
<p>Primero, normalizamos esta función para obtener una función de
densidad <span class="math inline">\(p_X(x)\)</span>. Es decir, buscamos
<span class="math inline">\(p_X(x) = c f(x)\)</span> tal que</p>
<p><span class="math display">\[
\begin{aligned}
1 &amp; = \int_{0}^{2}{p_X(x)dx} = \int_{0}^{2}{c f(x)dx} = c
\int_{0}^{2}{f(x)dx} = \\
  &amp; = \left.\frac{cx^3}{3}\right\rvert_{2}^{3} = \frac{8c}{3} \\
  &amp; \Rightarrow c = \frac{3}{8} \\
  &amp; \Rightarrow p_X(x) = \frac{3x^2}{8}
\end{aligned}
\]</span></p>
<p>A continuación, integramos la función de densidad para obtener la de
distribución <span class="math inline">\(F_X\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
F_X(x) = \int_{0}^{x}{p_X(x)dx} = \int_{0}^{x}{\frac{3x^2}{8}} =
\frac{x^3}{8}
\end{aligned}
\]</span></p>
<p>Solo nos queda conseguir la muestra. Para ello,</p>
<p><span class="math display">\[
\begin{aligned}
\xi &amp; = F_X(x)  = \frac{x^3}{8} \quad \iff \\
x &amp; = \sqrt[3]{8 \xi}
\end{aligned}
\]</span></p>
<p>Sacando un número aleatorio <span class="math inline">\(\xi\)</span>,
y pasándolo por la función obtenida, conseguimos un elemento con
distribución <span class="math inline">\(F(x)\)</span>.</p>
<h4 data-number="3.4.1.2"
id="ejemplo-práctico-del-método-de-la-transformada-inversa-en-r"><span
class="header-section-number">3.4.1.2</span> Ejemplo práctico del método
de la transformada inversa en R</h4>
<p>Aunque el ejemplo anterior nos enseña cómo calcular a mano una
función sencilla, resulta algo difícil de visualizar qué es lo que está
ocurriendo. Por ello, vamos a utilizar el programa <strong>R</strong>
para dar otro ejemplo.</p>
<p>Consideremos la distribución exponencial de parámetro <span
class="math inline">\(\lambda\)</span>, la cual tiene función de
densidad y de distribución</p>
<p><span class="math display">\[
\begin{aligned}
  f(x) &amp; = \lambda e^{-\lambda x}, \quad &amp; x \ge 0 \\
  F(x) &amp; =  1 - e^{-\lambda x},    \quad &amp; x \ge 0
\end{aligned}
\]</span></p>
<p>La función inversa se calcula tal que</p>
<p><span class="math display">\[
\begin{aligned}
  \xi &amp; = F(x)        = 1 - e^{-\lambda x}  &amp; \iff \\
    x &amp; = F^{-1}(\xi) = - \frac{log(1 - \xi)}{\lambda} &amp;
\end{aligned}
\]</span></p>
<p>Generemos ahora <span class="math inline">\(\xi_1, \dots,
\xi_n\)</span> valores en <span class="math inline">\(\mathcal{U}(0,
1)\)</span> y obtengamos las imágenes inversas <span
class="math inline">\(X_i = -\frac{log(\xi_i)}{\lambda}\)</span>.</p>
<p>Para el ejemplo, fijemos <span class="math inline">\(\lambda =
1.5\)</span> y calculemos los valores para <span
class="math inline">\(X_i\)</span>:</p>
<pre><code class="language-r">F &lt;- function(xi, lambda) {
  -log(xi) / lambda
}

N &lt;- 1000
xi &lt;- runif(N)
lambda &lt;- 1.5
x &lt;- F(xi, lambda)</code></pre>
<p>Es fácil comprobar que los valores generados se asemejan fielmente a
la función de densidad exponencial:</p>
<pre><code class="language-r">hist(x, freq = FALSE, breaks = &#39;FD&#39;, main = &#39;Método de la inversa para la exponencial&#39;, ylim = c(0, 1.5))
lines(density(x), col = &#39;blue&#39;)
curve(dexp(x, rate = lambda), add = TRUE, col = 2)</code></pre>
<div id="fig:metodo_inversa_R" class="fignos">
<figure>
<img loading="lazy" src="./img/03/metodo_inversa.png" style="width:70.0%"
alt="Figura 15: Histograma del método de la función inversa." />
<figcaption aria-hidden="true"><span>Figura 15:</span> Histograma del
método de la función inversa.</figcaption>
</figure>
</div>
<h3 data-number="3.4.2" id="método-del-rechazo"><span
class="header-section-number">3.4.2</span> Método del rechazo</h3>
<p>El método anterior presenta principalmente dos problemas:</p>
<ol type="1">
<li>No siempre es posible integrar una función para hallar su función de
densidad.</li>
<li>La inversa de la función de distribución, <span
class="math inline">\(F_X^{-1}\)</span> no tiene por qué existir.</li>
</ol>
<p>Como alternativa, existe el <strong>método del rechazo</strong> o
<strong>aceptación-rechazo</strong> (en inglés, <em>rejection
method</em>) <span class="citation" data-cites="mcbook">(<a
href="#ref-mcbook" role="doc-biblioref">Owen 2013</a>, Non-uniform
random variables)</span>. Necesitamos una variable aleatoria <span
class="math inline">\(Y\)</span> con función de densidad <span
class="math inline">\(p_Y(y)\)</span> de la cual resulta sencillo
generar muestras. El objetivo es conseguir muestras de <span
class="math inline">\(X \sim p_X\)</span>.</p>
<p>Como premisa, debemos buscar un <span class="math inline">\(M \in [1,
\infty)\)</span> tal que</p>
<p><span class="math display">\[
p_X(x) \le M p_Y(x) \quad \forall x \in \mathbb{R}
\]</span></p>
<p>La idea principal es sacar una muestra de <span
class="math inline">\(Y\)</span> y aceptarla con probabilidad <span
class="math inline">\(p_X/Mp_Y\)</span>. En otro caso, desecharla y
volver a sacar otra. Para evitar casos absurdos, se puede especificar
que <span class="math inline">\(p_Y(y)\)</span> sea 0 cuando <span
class="math inline">\(p_X(y)\)</span> lo sea.</p>
<p>El valor más pequeño que podemos tomar para <span
class="math inline">\(M\)</span> es <span
class="math inline">\(\sup_x{\frac{p_X(x)}{p_Y(x)}}\)</span>. De hecho,
debemos buscar cotas próximas a este supremo, pues hará que el método
sea más eficiente.</p>
<p>En esencia, estamos jugando a los dardos: si la muestra de <span
class="math inline">\(y\)</span> que hemos obtenido se queda por debajo
de la gráfica de la función <span class="math inline">\(Mp_Y &lt;
p_X\)</span>, estaremos obteniendo una de <span
class="math inline">\(p_X\)</span>.</p>
<p>El algoritmo consiste en:</p>
<ol type="1">
<li>Obtener una muestra de <span class="math inline">\(Y \sim
p_Y\)</span>, denotada <span class="math inline">\(y\)</span>, y otra de
<span class="math inline">\(\mathcal{U}(0, 1)\)</span>, denominada <span
class="math inline">\(\xi\)</span>.</li>
<li>Comprobar si <span class="math inline">\(\xi &lt;
\frac{p_X(y)}{Mp_Y(y)}\)</span>.
<ol type="1">
<li>Si se cumple, se acepta <span class="math inline">\(y\)</span> como
muestra de <span class="math inline">\(p_X\)</span></li>
<li>En caso contrario, se rechaza <span class="math inline">\(y\)</span>
y se vuelve al paso 1.</li>
</ol></li>
</ol>
<p>Probemos por qué este algoritmo produce una muestra de la función de
densidad <span class="math inline">\(p_X\)</span>:</p>
<p>Sea <span class="math inline">\(Y \sim p_Y\)</span>, <span
class="math inline">\(\xi \sim \mathcal{U}(0, 1)\)</span>
independientes. Una muestra <span class="math inline">\(Y = y\)</span>
es aceptada si se cumple que <span class="math inline">\(\xi \le
\frac{p_X(y)}{Mp_Y(y)}\)</span>. La probabilidad de que esto ocurra
es</p>
<p><span class="math display">\[
\int_{-\infty}^{\infty}{p_Y(y) \frac{p_X(y)}{Mp_Y(y)} dy} =
\int_{-\infty}^{\infty}{\frac{p_X(y)}{M} dy} = \frac{1}{M}
\]</span></p>
<p>Ahora bien, para cualquier <span class="math inline">\(x \in
\mathbb{R}\)</span></p>
<p><span class="math display">\[
\begin{aligned}
P\left[ X \le x \right] &amp; = \int_{-\infty}^{x}{
  \underbrace{p_Y(y) \frac{p_X(y)}{Mp_Y(y)} dy}_{\text{Si } y \text{ se
acepta}}
  + \underbrace{\left(1 - \frac{1}{M}\right) P\left[ X \le x
\right]}_{\text{Si } y \text{ se rechaza}}
} =  \\
  &amp; = \frac{1}{M} \int_{-\infty}^{x}{p_X(y) dy} + \left(1 -
\frac{1}{M}\right) P\left[ X \le x \right] \\
  &amp; = \int_{\infty}^{\infty}{p_X(y) dy}
\end{aligned}
\]</span></p>
<p>Lo que nos dice que <span class="math inline">\(X \sim p_X\)</span>
como queríamos comprobar.</p>
<h4 data-number="3.4.2.1"
id="ejemplo-práctico-del-método-del-rechazo-en-r"><span
class="header-section-number">3.4.2.1</span> Ejemplo práctico del método
del rechazo en R</h4>
<p>De la misma forma que hicimos con el <a
href="#ejemplo-práctico-del-método-de-la-transformada-inversa-en-r">método
de la inversa</a>, implementaremos un ejemplo gráfico de esta técnica en
el software R. En este caso, generaremos valores de una distribución
Beta a partir de la uniforme. Es decir, podemos tomar</p>
<p><span class="math display">\[
\begin{aligned}
p_X (x) &amp; = \frac{\Gamma(a + b)}{\Gamma(a) \Gamma(b)} x^{a - 1} (1 -
x)^{b - 1}, \quad 0 \le x \le 1; a = 2, b = 6 \\
p_Y (x) &amp; = 1, \quad 0 \le x \le 1
\end{aligned}
\]</span></p>
<p>Como <span class="math inline">\(M\)</span> podemos tomar</p>
<p><span class="math display">\[
M = \sup_{x}\frac{p_Y(x)}{p_X(x)} = \sup_{x}{p_X(x)}
\]</span></p>
<p>En la sección anterior dijimos que este algoritmo es “como jugar a
los dardos”. Pues bien, la figura [<a
href="#fig:metodo_rechazo_grafica">16</a>] muestra la diana. El
siguiente fragmento de código de R calcula este valor:</p>
<pre><code class="language-r">a &lt;- 2
b &lt;- 6
resultado &lt;- optimize(
    f = function(x) { dbeta(x, shape1 = a, shape2 = b) },
    maximum = TRUE,
    interval = c(0, 1)
)

# $maximum
# [1] 0.1666692
#
# $objective
# [1] 2.813143

M &lt;- resultado$objective</code></pre>
<div id="fig:metodo_rechazo_grafica" class="fignos">
<figure>
<img loading="lazy" src="./img/03/metodo_rechazo_grafica.png" style="width:60.0%"
alt="Figura 16: Podemos ver la función de densidad objetivo y la de densidad reescalada de la que tomamos muestras." />
<figcaption aria-hidden="true"><span>Figura 16:</span> Podemos ver la
función de densidad objetivo y la de densidad reescalada de la que
tomamos muestras.</figcaption>
</figure>
</div>
<p>Para resolver el problema planteado, podemos usar el siguiente
código:</p>
<pre><code class="language-r">N &lt;- 1000
x &lt;- double(N)

p_X &lt;- function(x) dbeta(x, shape1 = a, shape2 = b)
p_Y &lt;- function(x) 1

valores_generados &lt;- 0

for (i in 1:N) {
    xi &lt;- runif(1)
    y &lt;- runif(1)
    valores_generados &lt;- valores_generados + 1

    while (xi &gt; p_X(y) / (M * p_Y(y))) {
        # Seguir generando hasta que aceptemos uno
        xi &lt;- runif(1)
        y &lt;- runif(1)
        valores_generados &lt;- valores_generados + 1
    }

    # Aceptar el valor
    x[i] &lt;- y
}</code></pre>
<p>El hecho de que exista una posibilidad de que falle evidencia que el
algoritmo no es muy eficiente. De hecho, podemos ver una medida de las
veces que ha fallado:</p>
<pre><code class="language-r">valores_generados
# [1] 2906</code></pre>
<p>Es decir, para sacar 1000 muestras válidas ha hecho falta generar
2906; casi 3 veces más de las que queríamos.</p>
<p>Finalmente, veamos cómo de buena es la aproximación:</p>
<pre><code class="language-r">hist(x, freq = FALSE, breaks = &#39;FD&#39;, main = &#39;Método del rechazo para la distribución Beta(a = 2, b = 6)&#39;)
lines(density(x), col = &#39;blue&#39;)
curve(dbeta(x, shape1 = a, shape2 = b), add = TRUE, col = 2)</code></pre>
<div id="fig:metodo_rechazo_R" class="fignos">
<figure>
<img loading="lazy" src="./img/03/metodo_rechazo.png" style="width:70.0%"
alt="Figura 17: Histograma del método de rechazo." />
<figcaption aria-hidden="true"><span>Figura 17:</span> Histograma del
método de rechazo.</figcaption>
</figure>
</div>
<h1 data-number="4" id="construyamos-un-path-tracer"><span
class="header-section-number">4</span> ¡Construyamos un path
tracer!</h1>
<p>Ahora que hemos introducido toda la teoría necesaria, es hora de
ponernos manos a la obra. En este capítulo escogeremos una serie de
herramientas y con ellas implementaremos un pequeño motor de path
tracing en tiempo real.</p>
<p>La implementación estará basada en Vulkan, junto al pequeño framework
de nvpro-samples. El motor mantendrá el mismo espíritu que la serie de
<span class="citation" data-cites="Shirley2020RTW1">(<a
href="#ref-Shirley2020RTW1" role="doc-biblioref">Shirley
2020a</a>)</span>, Ray Tracing In One Weekend.</p>
<p>El resultado final puede verse en el siguiente vídeo <span
class="citation" data-cites="video">(<a href="#ref-video"
role="doc-biblioref">Millán 2022c</a>)</span></p>
<iframe width="784" height="441" src="https://www.youtube.com/embed/pXrD3K69MqE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
<h2 data-number="4.1" id="el-algoritmo-de-path-tracing"><span
class="header-section-number">4.1</span> El algoritmo de path
tracing</h2>
<p>Hemos llegado a una de las partes más importantes de este trabajo. Es
el momento de poner en concordancia todo lo que hemos visto a lo largo
de los capítulos anteriores. Vamos a aplicar las <a
href="#métodos-de-monte-carlo">técnicas de Monte Carlo</a> a las
ecuaciones vistas en <a href="#transporte-de-luz">radiometría</a>,
teniendo en cuenta las propiedades de los diferentes materiales.</p>
<p>El código ilustrado en las siguientes secciones está basado en el de
<span class="citation" data-cites="nvpro-samples-tutorial">(<a
href="#ref-nvpro-samples-tutorial" role="doc-biblioref">Nvidia
2022a</a>)</span>, aunque se pueden encontrar numerosísimas
modificaciones en la literatura del sector.</p>
<h3 data-number="4.1.1"
id="estimando-la-rendering-equation-con-monte-carlo"><span
class="header-section-number">4.1.1</span> Estimando la rendering
equation con Monte Carlo</h3>
<p>Lo que buscamos en esta sección es aproximar el valor de la radiancia
en un cierto punto, que dependerá de cada píxel dela pantalla.
¿Recuerdas la ecuación de dispersión [<a
href="#eq:scattering_equation">17</a>]?</p>
<p><span class="math display">\[
L_o(p, \omega_o \leftarrow \omega_i) = \int_{\mathbb{S}^2}{f(p, \omega_o
\leftarrow \omega_i)L_i(p, \omega_i)\cos\theta_i} d\omega_i
\]</span></p>
<p>Recordemos que <span class="math inline">\(L_o(p, \omega_o \leftarrow
\omega_i)\)</span> es la radiancia emitida en un punto <span
class="math inline">\(p\)</span> hacia la dirección <span
class="math inline">\(\omega_o\)</span> desde <span
class="math inline">\(\omega_i\)</span>, <span
class="math inline">\(f(p, \omega_o \leftarrow \omega_i)\)</span> es la
función de distribución de dispersión bidireccional (i.e., cómo refleja
la luz el punto) y <span class="math inline">\(\cos\theta_i\)</span> el
ángulo que forman el ángulo sólido de entrada <span
class="math inline">\(\omega_i\)</span> y la normal en el punto <span
class="math inline">\(p\)</span>, <span
class="math inline">\(\mathbf{n}\)</span>: <span
class="math inline">\(\cos\theta_i = \omega_i \cdot
\mathbf{n}\)</span>.</p>
<p>Añadamos el término de radiancia emitida <span
class="math inline">\(L_e(p, \omega_o)\)</span>, la cantidad de
randiancia emitida por el material del punto <span
class="math inline">\(p\)</span>:</p>
<p><span class="math display">\[
L_o(p, \omega_o \leftarrow \omega_i) = L_e(p, \omega_o) +
\int_{\mathbb{S}^2}{f(p, \omega_o \leftarrow \omega_i)L_i(p,
\omega_i)\cos\theta_i} d\omega_i
\]</span></p>
<p>Podemos aproximar el valor de la integral utilizando el estimador de
Monte Carlo comúnmente considerado en la <a
href="#muestreo-por-importancia-múltiple-en-transporte-de-luz">industria</a>,
[<a href="#eq:mc_integral_importancia">33</a>]:</p>
<p><span id="eq:rendering_equation_mc" class="eqnos"><span
class="math display">\[
\begin{aligned}
L_o(p, \omega_o \leftarrow \omega_i) &amp; = \int_{\mathbb{S}^2}{f(p,
\omega_o \leftarrow \omega_i)L_i(p, \omega_i)\cos\theta_i} d\omega_i \\
                 &amp; \approx \frac{1}{N} \sum_{j = 1}^{N}{\frac{f(p,
\omega_o \leftarrow \omega_j) L_i(p, \omega_j)
\cos\theta_j}{p(\omega_j)}}
\end{aligned}
\]</span><span class="eqnos-number">(35)</span></span></p>
<p>Con <span class="math inline">\(N \in \mathbb{Z}^+\)</span>. Con
<span class="math inline">\(N\)</span> suficientemente grande, se
conseguiría un valor de radiancia relativamente acertado. Sin embargo,
en algunos casos, podemos simplificar más el sumando.</p>
<p>Fijémonos en el denominador. Lo que estamos haciendo es tomar una
muestra de un vector en la esfera. Si trabajamos con una BRDF en vez de
una BSDF, usaríamos un hemisferio en vez de la esfera.</p>
<p>En el caso de la componente difusa, sabemos que la BRDF es <span
class="math inline">\(f_r(p, \omega_o \leftarrow \omega_i) =
\frac{\rho}{\pi}\)</span> aplicando reflectancia lambertiana, así
que</p>
<p><span class="math display">\[
\frac{1}{N} \sum_{j = 1}^{N}{\frac{(\rho / \pi) L_i(p, \omega_j)
\cos\theta_j}{p(\omega_j)}}
\]</span></p>
<p>En la sección <a href="#muestreo-por-importancia">muestreo por
importancia</a>, introducimos la idea de buscar una función proporcional
a <span class="math inline">\(f\)</span> para con el fin de reducir el
error. Podemos usar <span class="math inline">\(p(\omega) =
\frac{\cos\theta}{\pi}\)</span>, de forma que</p>
<p><span id="eq:rendering_eq_lambertian" class="eqnos"><span
class="math display">\[
\frac{1}{N} \sum_{j = 1}^{N}{\frac{(\rho / \pi) L_i(p, \omega_j)
\cos\theta_j}{(\cos\theta_j / \pi)}} = \frac{1}{N} \sum_{j =
1}^{N}{L_i(p, \omega_j) \rho}
\]</span><span class="eqnos-number">(36)</span></span></p>
<p>Lo cual nos proporciona una expresión muy agradable para los
materiales difusos.</p>
<p>Por lo general, no será necesario simplificar hasta tal punto la
expresión.</p>
<h3 data-number="4.1.2" id="pseudocódigo-de-un-path-tracer"><span
class="header-section-number">4.1.2</span> Pseudocódigo de un path
tracer</h3>
<p>Con lo que conocemos hasta ahora, podemos empezar a programar los
shaders. Una primera implementación inspirada en la rendering equation
[<a href="#eq:rendering_equation_mc">35</a>] sería similar a lo
siguiente:</p>
<pre><code class="language-cpp">pathtrace(Rayo r, profundidad) {
    if (profundidad == profundidad_maxima) {
        return contribucion_ambiental;
    }

    r.closest_hit()     // -&gt; Guardar información del impacto

    if (!r.ha_impactado()) {
        // Si no se golpea nada, añadir una pequeña contribución del entorno.
        return contribucion_ambiental
    }

    // Sacar información del punto de impacto
    hit_info = r.hit_info
    material = hit_info.material
    emision  = material.emision

    // Calcular los parámetros de la ecuación
    cos_theta = dot(r.direccion, hit_info.normal)
    BRDF, pdf = extraer_info(material)

    nuevo_rayo = Rayo(
        origen    = hit_info.punto_impacto,
        direccion = siguiente_direccion(hit_info.normal)
    )

    // Devolver la radiancia del punto de impacto.
    // L_i se calcula a partir del pathtrace del nuevo rayo.
    return emision
        + (BRDF * pathtrace(nuevo_rayo, profundidad + 1) * cos_theta) / pdf;
}</code></pre>
<p>El término <code>emision</code> corresponde a <span
class="math inline">\(L_e(p, \omega_o)\)</span>. Siempre lo añadimos,
pues en caso de que el objeto no emita luz, la contribución de este
término sería 0.</p>
<p>La principal desventaja de esta implementación es que utiliza
recursividad. Como bien es conocido, abusar de recursividad provoca que
el tiempo de ejecución aumente significativamente. Además, con la
implementación anterior, se generan rayos desde el closest hit shader,
lo cual no es ideal.</p>
<h3 data-number="4.1.3" id="evitando-la-recursividad"><span
class="header-section-number">4.1.3</span> Evitando la recursividad</h3>
<p>Podemos evitar los problemas de la implementación anterior con una
pequeña modificación. En vez de calcular la radiancia desde el closest
hit, nos traemos la información necesaria al raygen shader, y calculamos
la radiancia total desde allí.</p>
<p>Para conseguirlo, debemos hacer que el <code>HitPayload</code>
almacene dos nuevos parámetros: <code>weight</code> y
<code>hit_value</code>, así como el nuevo origen y la dirección del
rayo.</p>
<p>El pseudocódigo sería el siguiente: por una parte, una función se
encarga de generar los rayos:</p>
<pre><code class="language-cpp">pathtrace() {
    // Inicializar parámetros del primer rayo
    HitPayload prd {
        hit_value,
        weight,
        ray_origin,
        ray_direction
    }

    current_weight = vec3(1);
    hit_value      = vec3(0);

    for (profundidad in [0, profunidad_maxima]) {
        closest_hit(prd.ray_origin, prd.ray_dir);
        // prd actualiza sus parámetros

        hit_value = hit_value + prd.hit_value * current_weight;
        current_weight = current_weight * prd.weight;
    }

    return hit_value;
}</code></pre>
<p>Y por otro lado, otra función debe almacenar correctamente la
información del punto de impacto, así como la radiancia de ese punto.
Corresponde al closest hit:</p>
<pre><code class="language-cpp">closest_hit() {
    // Sacar información sobre el punto de impacto: material, normal...

    // Preparar información para el raygen
    prd.ray_origin = punto_impacto
    prd.ray_dir = siguiente_direccion(material)

    // Calcular la radiancia
    float cos_theta = dot(prd.ray_dir, normal);
    BRDF, pdf = extraer_info(material)

    prd.hit_value = material.emision

    prd.weight = (BRDF * cos_theta) / pdf

    return prd
}</code></pre>
<p>Esta versión no es tan intuitiva. ¿Por qué este último genera el
mismo resultado que el de <a href="#pseudocódigo-de-un-path-tracer">la
versión recursiva</a>?</p>
<p>Analicemos lo está ocurriendo.</p>
<p>Sea <span class="math inline">\(h\)</span> el <em>hit value</em> (que
simboliza la radiancia), <span class="math inline">\(w\)</span> el peso,
<span class="math inline">\(f_i\)</span> la BRDF (o en su defecto,
BTDF/BSDF), <span class="math inline">\(i\)</span>, <span
class="math inline">\(e_i\)</span> la emisión, <span
class="math inline">\(\cos\theta_i\)</span> el coseno del ángulo que
forman la nueva dirección del rayo y la normal, y <span
class="math inline">\(p_i\)</span> la función de densidad que, dada una
dirección, proporciona la probabilidad de que se escoja. El subíndice
denota el <span class="math inline">\(i\)</span>-ésimo punto de
impacto.</p>
<p>En esencia, este algoritmo está descomponiendo lo que recogemos en
<code>weight</code>, que es <span class="math inline">\(f_i \cos\theta_i
/ p_i\)</span>. Inicialmente, para el primer envío del rayo, <span
class="math inline">\(h = (0, 0, 0)\)</span>, <span
class="math inline">\(w = (1, 1, 1)\)</span>. Tras trazar el primer
rayo, se tiene que</p>
<p><span class="math display">\[
\begin{aligned}
    h &amp; = 0 + e_1 w = e_1 \\
    w &amp; = \frac{f_1 \cos\theta_1}{p_1}
\end{aligned}
\]</span></p>
<p>Tras el segundo rayo, obtenemos</p>
<p><span class="math display">\[
\begin{aligned}
    h &amp; = e_1 + e_2 w = \\
      &amp; = e_1 + e_2 \frac{f_1 \cos\theta_1}{p_1} \\
    w &amp; = \frac{f_1 \cos\theta_1}{p_1} \frac{f_2 \cos\theta_2}{p_2}
\end{aligned}
\]</span></p>
<p>Y para el tercero</p>
<p><span class="math display">\[
\begin{aligned}
    h &amp; = e_1 + e_2 \frac{f_1 \cos\theta_1}{p_1} + e_3 w = \\
      &amp; = e_1 + e_2 \frac{f_1 \cos\theta_1}{p_1} + e_3 \frac{f_1
\cos\theta_1}{p_1} \frac{f_2 \cos\theta_2}{p_2} = \\
      &amp; = e_1 + \frac{f_1
\cos\theta_1}{p_1}\textcolor{verde-oscurisimo}{\left(e_2 + e_3 \frac{f_2
\cos\theta_2}{p_2}\right)} \\
    w &amp; = \frac{f_1 \cos\theta_1}{p_1} \frac{f_2 \cos\theta_2}{p_2}
\frac{f_3 \cos\theta_3}{p_3}
\end{aligned}
\]</span></p>
<p>El <span
class="math inline">\(\textcolor{verde-oscurisimo}{\text{término que
acompaña}}\)</span> a <span class="math inline">\(\frac{f_1
\cos\theta_1}{p_1}\)</span> es la radiancia del tercer punto de impacto.
Por tanto, a la larga, se tendrá que <span
class="math inline">\(h\)</span> estima correctamente la radiancia de un
punto. Con esto, podemos afirmar que</p>
<p><span class="math display">\[
h \approx \frac{1}{N} \sum_{j = 1}^{N}{\frac{f(p, \omega_o \leftarrow
\omega_j) L_i(p, \omega_j) \cos\theta_j}{p(\omega_j)}}
\]</span></p>
<p>Este algoritmo supone una mejora de hasta 3 veces mayor rendimiento
que el recursivo <span class="citation"
data-cites="nvpro-samples-tutorial">(<a
href="#ref-nvpro-samples-tutorial" role="doc-biblioref">Nvidia
2022a</a>, glTF Scene)</span>.</p>
<h2 data-number="4.2"
id="requisitos-de-ray-tracing-en-tiempo-real"><span
class="header-section-number">4.2</span> Requisitos de ray tracing en
tiempo real</h2>
<p>Como es natural, el tiempo es una limitación enorme para cualquier
programa en tiempo real. Mientras que en un <em>offline renderer</em>
disponemos de un tiempo muy considerable por frame (desde varios
segundos hasta horas), en un programa en tiempo real necesitamos que un
frame salga en 16 milisegundos o menos. Este concepto se suele denominar
<em>frame budget</em>: la cantidad de tiempo que disponemos para un
frame.</p>
<blockquote>
<p><strong>Nota</strong>: cuando hablamos del tiempo disponible para un
frame, solemos utilizarmilisegundos (ms) o frames por segundo (FPS).
Para que un programa en tiempo real vaya suficientemente fluido,
necesitaremos que el motor corra a un mínimo de 30 FPS (que equivalen a
33 ms por frame). Hoy en día, debido al avance del área en campos como
los videosjuegos, el estándar se está convirtiendo en 60 FPS (16
ms/frame).</p>
</blockquote>
<p>Las nociones de los capítulos anteriores no distinguen entre un motor
en tiempo real y <em>offline</em>. Como es natural, necesitaremos
introducir unos pocos conceptos más para llevarlo a tiempo real. Además,
existen una serie de requisitos hardware que debemos cumplir para que un
motor en tiempo real con ray tracing funcione.</p>
<h3 data-number="4.2.1" id="arquitecturas-de-gráficas"><span
class="header-section-number">4.2.1</span> Arquitecturas de
gráficas</h3>
<p>El requisito más importante de todos es la gráfica. Para ser capaces
de realizar cálculos de ray tracing en tiempo real, necesitaremos una
arquitectura moderna con núcleos dedicados a este tipo de cáclulos <a
href="#fn4" class="footnote-ref" id="fnref4"
role="doc-noteref"><sup>4</sup></a>.</p>
<p>A día 17 de abril de 2022, para correr ray tracing en tiempo real, se
necesita alguna de las siguientes tarjetas gráficas:</p>
<table>
<colgroup>
<col style="width: 20%" />
<col style="width: 10%" />
<col style="width: 68%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Arquitectura</strong></th>
<th style="text-align: left;"><strong>Fabricante</strong></th>
<th style="text-align: center;"><strong>Modelos de
gráficas</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Turing</strong></td>
<td style="text-align: left;">Nvidia</td>
<td style="text-align: center;">RTX 2060, RTX 2060 Super, RTX 2070, RTX
2070 Super, RTX 2080, RTX 2080 Super, RTX 2080 Ti, RTX Titan</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Ampere</strong></td>
<td style="text-align: left;">Nvidia</td>
<td style="text-align: center;">RTX 3050, RTX 3060, RTX 3060 Ti, RTX
3070, RTX 3070 Ti, RTX 3080, RTX 3080 Ti, RTX 3090, RTX 3090 Ti</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>RDNA2</strong> (Navi 2X, Big
Navi)</td>
<td style="text-align: left;">AMD</td>
<td style="text-align: center;">RX 6400, RX 6500 XT, RX 6600, RX 6600
XT, RX 6700 XT, RX 6800, RX 6800 XT, RX 6900 XT</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Arc Alchemist</strong></td>
<td style="text-align: left;">Intel</td>
<td style="text-align: center;"><em>No reveleado aún</em></td>
</tr>
</tbody>
</table>
<p>Se puede encontrar más información sobre las diferentes arquitecturas
y gráficas en el siguiente artículo de AMD Radeon <span class="citation"
data-cites="wikipedia-radeon">(<a href="#ref-wikipedia-radeon"
role="doc-biblioref">Wikipedia 2022f</a>)</span>, Nvidia <span
class="citation" data-cites="wikipedia-nvidia">(<a
href="#ref-wikipedia-nvidia" role="doc-biblioref">Wikipedia
2022b</a>)</span>, e <span class="citation" data-cites="intel-arc">(<a
href="#ref-intel-arc" role="doc-biblioref">Intel 2022a</a>)</span>. Solo
se han incluido las gráficas de escritorio de consumidor.</p>
<p>Para este trabajo se ha utilizado una <strong>RTX 2070
Super</strong>. En el capítulo de análisis del rendimiento se hablará
con mayor profundidad de este apartado.</p>
<h3 data-number="4.2.2"
id="frameworks-y-api-de-ray-tracing-en-tiempo-real"><span
class="header-section-number">4.2.2</span> Frameworks y API de ray
tracing en tiempo real</h3>
<p>Una vez hemos cumplido los requisitos de hardware, es hora de escoger
los frameworks de trabajo.</p>
<p>Las API de gráficos están empezando a adaptarse a los requisitos del
tiempo real, por lo que cambian frecuentemente. La mayoría adquirieron
las directivas necesarias muy recientemente. Aun así, son lo
suficientemente sólidas para que se pueda usar en aplicaciones
empresariales de gran embergadura.</p>
<p>Esta es una lista de las API disponibles con capacidades de Ray
Tracing disponibles para, al menos, la arquitectura Turing:</p>
<ul>
<li>Vulkan, junto a los <em>bindings</em> de ray tracing, denominados
KHR.</li>
<li>Microsoft DirectX Ray Tracing (DXR), una extensión de DirectX 12
<span class="citation" data-cites="directx-12">(<a
href="#ref-directx-12" role="doc-biblioref">Wikipedia
2022a</a>)</span>.</li>
<li>Nvidia OptiX <span class="citation" data-cites="optix">(<a
href="#ref-optix" role="doc-biblioref">Wikipedia 2022c</a>)</span>.</li>
</ul>
<p>De momento, no hay mucho donde elegir.</p>
<p>OptiX es la API más vieja de todas. Su primera versión salió en 2009,
mientras que la última estable es de 2021. Tradicionalmente se ha usado
para offline renderers, y no tiene un especial interés para este trabajo
estando las otras dos disponibles.</p>
<p>Tanto DXR como Vulkan son los candidatos más sólidos. DXR salió en
2018, con la llegada de Turing. Es un par de años más reciente que
Vulkan KHR. Cualquiera de las dos cumpliría su cometido de forma
exitosa. Sin embargo, para este trabajo, <strong>hemos escogido
Vulkan</strong> por los siguientes motivos:</p>
<ul>
<li>DirectX 12 está destinado principalmente a plataformas de Microsoft.
Es decir, está pensado para sistemas operativos Windows 10 o mayor <a
href="#fn5" class="footnote-ref" id="fnref5"
role="doc-noteref"><sup>5</sup></a>.</li>
<li>Vulkan, al estar apoyado principalmente por AMD y desarrollado por
Khonos, es un proyecto de código. Su principal aliciente es la capacidad
de correr en múltiples sistemas operativos, como Windows, distribuciones
de Linux o Android.</li>
</ul>
<p>Ambas API se comportan de manera muy similar, y no existe una gran
diferencia entre ellas; tanto en rendimiento como en complejidad de
desarrollo. Actualmente el proyecto solo compila en Windows 10 o mayor,
por lo que estos dos puntos no resultan especialmente relevantes para el
trabajo.</p>
<p>Si se desea, se puede encontrar una comparación más a fondo de las
API en el blog de <span class="citation" data-cites="alain-API">(<a
href="#ref-alain-API" role="doc-biblioref">Galvan 2022a</a>)</span>.
Además, el manual de Vulkan con las extensiones de KHR se puede
encontrar en <span class="citation" data-cites="vulkan">(<a
href="#ref-vulkan" role="doc-biblioref">The Khronos Vulkan Working Group
2022</a>)</span>.</p>
<h2 data-number="4.3" id="setup-del-proyecto"><span
class="header-section-number">4.3</span> Setup del proyecto</h2>
<p>Un proyecto de Vulkan necesita una cantidad de código inicial
considerable. Para acelerar este trámite y partir de una base más
sólida, se ha decidido usar un pequeño framework de trabajo de Nvidia
llamado [nvpro-samples] <span class="citation"
data-cites="nvpro-samples">(<a href="#ref-nvpro-samples"
role="doc-biblioref">Nvidia 2022b</a>)</span>.</p>
<p>Esta serie de repositorios de Nvidia DesignWorks contienen proyectos
de ray tracing de Nvidia con fines didácticos. Nosotros usaremos
<strong>vk_raytracing_tutorial_KHR</strong> <span class="citation"
data-cites="nvpro-samples-tutorial">(<a
href="#ref-nvpro-samples-tutorial" role="doc-biblioref">Nvidia
2022a</a>)</span>, pues ejemplifica cómo añadir ray tracing en tiempo
real a un proyecto de Vulkan. Estos frameworks contienen asimismo otras
utilidades menores. Destacan <strong>GLFW</strong> (gestión de ventanas
en C++), <strong>imgui</strong> (interfaz de usuario) y
<strong>tinyobjloader</strong> (carga de <code>.obj</code> y
<code>.mtl</code>).</p>
<p>Nuestro repositorio utiliza las herramientas citadas anteriormente
para compilar su proyecto. El Makefile es una modificación del que se
usa para ejecutar los ejemplos de Nvidia. Por defecto, ejecuta una
aplicación muy simple que muestra un cubo mediante rasterización, la
cual modificaremos hasta añadir ray tracing en tiempo real. Por tanto,
la parte inicial del desarrollo consiste en adaptar Vulkan para usar la
extensión de ray tracing, extrayendo la información de la gráfica y
cargando correspondientemente el dispositivo.</p>
<div id="fig:raster" class="fignos">
<figure>
<img loading="lazy" src="./img/04/Raster.jpg" style="width:70.0%"
alt="Figura 18: Por defecto, el programa muestra un cubo rasterizado muy simple. Es, prácticamente, un hello world gráfico" />
<figcaption aria-hidden="true"><span>Figura 18:</span> Por defecto, el
programa muestra un cubo rasterizado muy simple. Es, prácticamente, un
<em>hello world</em> gráfico</figcaption>
</figure>
</div>
<h3 data-number="4.3.1" id="un-vistazo-general-a-la-estructura"><span
class="header-section-number">4.3.1</span> Un vistazo general a la
estructura</h3>
<p>La estructura final del proyecto (es decir, la carpeta
<code>application</code>) es la siguiente:</p>
<ul>
<li>La carpeta <code>application/build</code> contiene todo lo
relacionado con CMake y el ejecutable final.</li>
<li>Las dependencias del proyecto se encuentran en el repositorio
<code>application/nvpro_core</code>. Se descargan automáticamente seguir
las instrucciones de compilación.</li>
<li>En <code>application/vulkan_ray_tracing/media/</code> se encuentran
todos los archivos <code>.obj</code>, <code>.mtl</code> y las
texturas.</li>
<li>La subcarpeta <code>application/vulkan_ray_tracing/src</code>
contiene el código fuente de la propia aplicación.
<ul>
<li>Toda la implementación relacionada con el motor (y por tanto,
Vulkan), se halla en <code>engine.h/cpp</code>. Una de las desventajas
de seguir un framework “de juguete” es que el acoplamiento es
considerablemente alto. Más adelante comentaremos los motivos.</li>
<li>Los parámetros de la aplicación (como tamaño de pantalla y otras
estructuras comunes) se encuetran en <code>globals.hpp</code>.</li>
<li>La carga de escenas y los objetos se gestionan en
<code>scene.hpp</code>.</li>
<li>En <code>main.cpp</code> se gestiona tanto el punto de entrada de la
aplicación como la actualización de la interfaz gráfica.</li>
<li>La carpeta <code>application/vulkan_ray_tracing/src/shaders</code>
contiene todos los shaders; tanto de rasterización, como de ray tracing.
<ul>
<li>Para ray tracing, se utilizan los <code>raytrace.*</code>,
<code>pathtrace.glsl</code> (que contiene el grueso del path
tracer).</li>
<li>En rasterización se usan principalmente
<code>frag_shader.frag</code>, <code>passthrough.vert</code>,
<code>post.frag</code>, <code>vert_shader.vert</code>.</li>
<li>El resto de shaders son archivos comunes a ambos o utilidades
varias, como pueden ser <code>sampling.glsl</code> (donde se implementan
distribuciones aleatorias) o <code>random.glsl</code> (que contiene
generadores de números aleatorios).</li>
</ul></li>
<li>Finalmente, la carpeta
<code>application/vulkan_ray_tracing/src/spv</code> contiene los shaders
compilados a SPIR-V.</li>
</ul></li>
</ul>
<p>El diagrama <a href="#fig:estructura_repo">19</a> permite visualizar
los puntos anteriores, así como la estructura general del
repositorio.</p>
<div id="fig:estructura_repo" class="fignos">
<figure>
<img loading="lazy" src="./img/04/repo_diagram.png" style="width:90.0%"
alt="Figura 19: Estructura del repositorio" />
<figcaption aria-hidden="true"><span>Figura 19:</span> Estructura del
repositorio</figcaption>
</figure>
</div>
<h3 data-number="4.3.2" id="diagramas"><span
class="header-section-number">4.3.2</span> Diagramas</h3>
<p>Teniendo en cuenta que utilizamos un framework que no está pensado
para producción y la naturaleza de Vulkan, realizar un diagrama de clase
es muy complicado. Sin embargo, podemos ilustrar las clases más
importantes de la aplicación: la el motor [<a
href="#fig:diagrama-clases-engine">20</a>] y la de escenas [<a
href="#fig:diagrama-clases-scenes">21</a>] . En las secciones
posteriores detallaremos algunos de los miembros de estas.</p>
<p>Una figura que se asemeja a un diagrama de secuencia específico para
el loop de ray tracing puede encontrarse en [<a
href="#fig:pipeline">23</a>].</p>
<div id="fig:diagrama-clases-engine" class="fignos">
<figure>
<img loading="lazy" src="./img/08/Mermaid%20engine.png"
alt="Figura 20: Diagrama de clases para Engine" />
<figcaption aria-hidden="true"><span>Figura 20:</span> Diagrama de
clases para Engine</figcaption>
</figure>
</div>
<div id="fig:diagrama-clases-scenes" class="fignos">
<figure>
<img loading="lazy" src="./img/08/Mermaid%20scenes.png" style="width:60.0%"
alt="Figura 21: Diagrama de clases para Scenes" />
<figcaption aria-hidden="true"><span>Figura 21:</span> Diagrama de
clases para Scenes</figcaption>
</figure>
</div>
<h2 data-number="4.4" id="compilación-y-ejecución"><span
class="header-section-number">4.4</span> Compilación y ejecución</h2>
<p>Las dependencias necesarias son:</p>
<ol type="1">
<li><strong>CMake</strong>.</li>
<li>Un <strong>driver de Nvidia</strong> compatible con la extensión
<code>VK_KHR_ray_tracing_pipeline</code>.</li>
<li>El SDK de <strong>Vulkan</strong>, versión 1.2.161 o mayor.</li>
</ol>
<p>Ejecuta los siguientes comandos desde la terminal para compilar el
proyecto:</p>
<pre class="sh"><code>$ git clone --recursive --shallow-submodules https://github.com/Asmilex/Raytracing.git
$ cd .\Raytracing\application\vulkan_ray_tracing\
$ mkdir build
$ cd build
$ cmake ..
$ cmake --build .</code></pre>
<p>Si todo funciona correctamente, debería generarse un binario en
<code>./application/bin_x64/Debug</code> llamado
<code>asmiray.exe</code>. Desde la carpeta en la que deberías
encontrarte tras seguir las instrucciones, puedes conseguir ejecutarlo
con</p>
<pre class="sh"><code>$ ..\..\bin_x64\Debug\asmiray.exe</code></pre>
<h2 data-number="4.5" id="estructuras-de-aceleración"><span
class="header-section-number">4.5</span> Estructuras de aceleración</h2>
<p>El principal coste de ray tracing es el cálculo de las intersecciones
con objetos; hasta un 95% del tiempo de ejecución total <span
class="citation" data-cites="scratchapixel-2019">(<a
href="#ref-scratchapixel-2019" role="doc-biblioref">Scratchapixel
2019</a>)</span>. Reducir el número de test de intersección es
clave.</p>
<p>Las <strong>estructuras de aceleración</strong> son una forma de
representar la geometría de la escena. Aunque existen diferentes tipos,
en esencia, todos engloban a uno o varios objetos en una estructura con
la que resulta más eficiente hacer test de intersección. Son similares a
los grafos de escena de un rasterizador.</p>
<p>Uno de los tipos más comunes (y el que se usa en <span
class="citation" data-cites="Shirley2020RTW2">(<a
href="#ref-Shirley2020RTW2" role="doc-biblioref">Shirley
2020b</a>)</span>) es la <strong>Bounding Volume Hierarchy
(BVH)</strong>. Fue una técnica desarrollada por Kay y Kajilla en 1986.
Este método encierra un objeto en una caja (denomina una
<strong>bounding box</strong>), de forma que el test de intersección
principal se hace con la caja y no con la geometría. Si un rayo impacta
en la <em>bounding box</em>, entonces se pasa a testear la
geometría.</p>
<p>Se puede repetir esta idea repetidamente, de forma que agrupemos
varias <em>bounding boxes</em>. Así, creamos una jerarquía de objetos
–como si nodos de un árbol se trataran–. A esta jerarquía es a la que
llamamos BVH.</p>
<p>Es importante crear buenas divisiones de los objetos en la BVH.
Cuanto más compacta sea una BVH, más eficiente será el test de
intersección.</p>
<p>Una forma habitual de crear la BVH es mediante la división del
espacio en una rejilla. Esta técnica se llama <strong>Axis-Aligned
Bounding Box (AABB)</strong>. Usualmente se usa el método del
<em>slab</em> (también introducido por Kay y Kajilla). Se divide el
espacio en una caja n-dimensional alineada con los ejes, de forma que
podemos verla como <span class="math inline">\([x_0, x_1]
\times\)</span> <span class="math inline">\([y_0, y_1] \times\)</span>
<span class="math inline">\([z_0, z_1] \times \dots\)</span> De esta
forma, comprobar si un rayo impacta en una bounding box es tan sencillo
como comprobar que está dentro del intervalo. Este es el método que se
ha usado en Ray Tracing in One Weekend.</p>
<p>Vulkan gestiona las estructuras de aceleración diviéndolas en dos
partes: <strong>Top-Level Acceleration Structure</strong> (TLAS) y
<strong>Bottom-Level Acceleration Structure</strong> (BLAS).</p>
<div id="fig:TLAS" class="fignos">
<figure>
<img loading="lazy" src="./img/04/Acceleration%20structure.png" style="width:70.0%"
alt="Figura 22: La TLAS guarda información de las instancias de un objeto, así como una referencia a BLAS que contiene la geometría correspondiente. Fuente: (Nvidia 2022a)" />
<figcaption aria-hidden="true"><span>Figura 22:</span> La TLAS guarda
información de las instancias de un objeto, así como una referencia a
BLAS que contiene la geometría correspondiente. Fuente: <span
class="citation" data-cites="nvpro-samples-tutorial">(<a
href="#ref-nvpro-samples-tutorial" role="doc-biblioref">Nvidia
2022a</a>)</span></figcaption>
</figure>
</div>
<blockquote>
<p>TODO: Deberíamos cambiar esa foto por otra propia.</p>
</blockquote>
<h3 data-number="4.5.1"
id="botom-level-acceleration-structure-blas"><span
class="header-section-number">4.5.1</span> Botom-Level Acceleration
Structure (BLAS)</h3>
<p>Las <strong>estructuras de aceleración de bajo nivel</strong>
(<em>Bottom-Level Acceleration Structure</em>, BLAS) almacenan la
geometría de un objeto individual; esto es, los vértices y los índices
de los triángulos, además de una AABB que la encapsula.</p>
<p>Pueden almacenar varios modelos, puesto que alojan uno o más buffers
de vértices junto a sus matrices de transformación. Si un modelo es
instanciado varias veces <em>dentro de la misma BLAS</em>, la geometría
se duplica. Esto se hace para mejorar el rendimiento.</p>
<p>Como regla general, cuantas menos BLAS, mejor <span class="citation"
data-cites="nvidia-best-practices">(<a href="#ref-nvidia-best-practices"
role="doc-biblioref">Nvidia 2020b</a>)</span>.</p>
<p>El código correspondiente a la creación de la BLAS en el programa es
el siguiente:</p>
<pre><code class="language-cpp">void Engine::createBottomLevelAS() {
    // BLAS - guardar cada primitiva en una geometría

    std::vector&lt;nvvk::RaytracingBuilderKHR::BlasInput&gt; allBlas;
    allBlas.reserve(m_objModel.size());

    for (const auto&amp; obj: m_objModel) {
        auto blas = objectToVkGeometryKHR(obj);

        // Podríamos añadir más geometrías en cada BLAS.
        // De momento, solo una.
        allBlas.emplace_back(blas);
    }

    m_rtBuilder.buildBlas(
        allBlas,
        VK_BUILD_ACCELERATION_STRUCTURE_PREFER_FAST_TRACE_BIT_KHR
    );
}</code></pre>
<h3 data-number="4.5.2" id="top-level-acceleration-structure-tlas"><span
class="header-section-number">4.5.2</span> Top-Level Acceleration
Structure (TLAS)</h3>
<p>Las Top-Level Acceleration Structures almacenan las instancias de los
objetos, cada una con su matriz de transformación y referencia a la BLAS
correspondiente.</p>
<p>Además, guardan información sobre el <em>shading</em>. Así, los
shaders pueden relacionar la geometría intersecada y el material de
dicho objeto. En esta última parte jugará un papel fundamental la <a
href="#shader-binding-table">Shader Binding Table</a>.</p>
<p>En el programa hacemos lo siguiente para construir la TLAS:</p>
<pre><code class="language-cpp">void Engine::createTopLevelAS() {
    std::vector&lt;VkAccelerationStructureInstanceKHR&gt; tlas;
    tlas.reserve(m_instances.size());

    for (const HelloVulkan::ObjInstance&amp; inst: m_instances) {
        VkAccelerationStructureInstanceKHR rayInst{};

        // Posición de la instancia
        rayInst.transform = nvvk::toTransformMatrixKHR(inst.transform);

        rayInst.instanceCustomIndex = inst.objIndex;

        // returns the acceleration structure device address of the blasId. The id correspond to the created BLAS in buildBlas.
        rayInst.accelerationStructureReference = m_rtBuilder.getBlasDeviceAddress(inst.objIndex);

        rayInst.flags = VK_GEOMETRY_INSTANCE_TRIANGLE_FACING_CULL_DISABLE_BIT_KHR;
        rayInst.mask  = 0xFF; // Solo registramos hit si rayMask &amp; instance.mask != 0
        rayInst.instanceShaderBindingTableRecordOffset = 0; // Usaremos el mismo hit group para todos los objetos

        tlas.emplace_back(rayInst);
    }

    m_rtBuilder.buildTlas(
        tlas,
        VK_BUILD_ACCELERATION_STRUCTURE_PREFER_FAST_TRACE_BIT_KHR
    );
}</code></pre>
<h2 data-number="4.6" id="la-ray-tracing-pipeline"><span
class="header-section-number">4.6</span> La ray tracing pipeline</h2>
<h3 data-number="4.6.1" id="descriptores-y-conceptos-básicos"><span
class="header-section-number">4.6.1</span> Descriptores y conceptos
básicos</h3>
<p>Primero, debemos introducir unas nociones básicas de Vulkan sobre
cómo gestiona la información que se pasa a los shaders.</p>
<p>Un <strong><em>resource descriptor</em></strong> (usualmente lo
abreviaremos como descriptor) es una forma de cargar recursos como
buffers o imágenes para que la tarjeta gráfica los pueda utilizar;
concretamente, los shaders. El <strong><em>descriptor
layout</em></strong> especifica el tipo de recurso que va a ser
accedido, mientras que el <strong><em>descriptor set</em></strong>
determina el buffer o imagen que se va a asociar al descriptor. Este set
es el que se utiliza en los <strong>drawing commands</strong>. Un
<strong>pipeline</strong> es una secuencia de operaciones que reciben
una geometría y sus texturas, y la transforma en unos pixels.</p>
<p>Si necesitas más información, todos estos conceptos aparecen
desarrollados extensamente en <span class="citation"
data-cites="overvoorde-2022">(<a href="#ref-overvoorde-2022"
role="doc-biblioref">Overvoorde 2022</a>, Descriptor layout and
buffer)</span>.</p>
<p>Tradicionalmente, en rasterización se utiliza un descriptor set por
tipo de material, y consecuentemente, un pipeline por cada tipo. En ray
tracing esto no es posible, puesto que <strong>no se sabe qué
material</strong> se va a usar: un rayo puede impactar en
<em>cualquier</em> material presente en la escena, lo cual invocaría un
shader específico. Debido a esto, empaquetaremos todos los recursos en
un único set de descriptores.</p>
<h3 data-number="4.6.2" id="la-shader-binding-table"><span
class="header-section-number">4.6.2</span> La Shader Binding Table</h3>
<p>Para solucionar esto, vamos a crear la <strong>Shader Binding
Table</strong> (SBT). Esta estructura permitirá cargar el shader
correspondiente dependiendo de dónde impacte un rayo.</p>
<p>Para cargar esta estructura, se debe hacer lo siguiente:</p>
<ol type="1">
<li>Cargar y compilar cada shader en un
<code>VkShaderModule</code>.</li>
<li>Juntar los cada <code>VkShaderModule</code> en un array
<code>VkPipelineShaderStageCreateInfo</code>.</li>
<li>Crear un array de <code>VkRayTracingShaderGroupCreateInfoKHR</code>.
Cada elemento se convertirá al final en una entrada de la Shader Binding
Table.</li>
<li>Compilar los dos arrays anteriores más un pipeline layout para
generar un <code>vkCreateRayTracingPipelineKHR</code>.</li>
<li>Conseguir los <em>handlers</em> de los shaders usando
<code>vkGetRayTracingShaderGroupHandlesKHR</code>.</li>
<li>Alojar un buffer con el bit
<code>VK_BUFFER_USAGE_SHADER_BINDING_TABLE_BIT_KHR</code> y copiar los
<em>handlers</em>.</li>
</ol>
<div id="fig:pipeline" class="fignos">
<figure>
<img loading="lazy" src="./img/04/Pipeline.png"
alt="Figura 23: La Shader Binding Table permite selccionar un tipo de shader dependiendo del objeto en el que se impacte. Para ello, se genera un rayo desde el shader raygen, el cual viaja a través de la Acceleration Structure. Dependiendo de dónde impacte, se utiliza un closest hit, any hit, o miss shaders. Fuente: (Usher 2021, 194)" />
<figcaption aria-hidden="true"><span>Figura 23:</span> La Shader Binding
Table permite selccionar un tipo de shader dependiendo del objeto en el
que se impacte. Para ello, se genera un rayo desde el shader
<code>raygen</code>, el cual viaja a través de la Acceleration
Structure. Dependiendo de dónde impacte, se utiliza un
<code>closest hit</code>, <code>any hit</code>, o <code>miss</code>
shaders. Fuente: <span class="citation" data-cites="GemsII-SBT">(<a
href="#ref-GemsII-SBT" role="doc-biblioref">Usher 2021,
194</a>)</span></figcaption>
</figure>
</div>
<p>Cada entrada de la SBT contiene un handler y una serie de parámetros
embebidos. A esto se le conoce como <strong>Shader Record</strong>.
Estos records se clasifican en:</p>
<ul>
<li><strong>Ray generation record</strong>: contiene el handler del ray
generation shader.</li>
<li><strong>Hit group record</strong>: se encargan de los handlers del
closest hit, anyhit (opcional), e intersection (opcional).</li>
<li><strong>Miss group record</strong>: se encarga del miss shader.</li>
<li><strong>Callable group record</strong>.</li>
</ul>
<p>Una de las partes más difíciles de la SBT es saber cómo se relacionan
record y geometría. Es decir, cuando un rayo impacta en una geometría,
¿a qué record de la SBT llamamos? Esto se determina mediante los
parámetros de la instancia, la llamada a <em>trace rays</em>, y el orden
de la geometría en la BLAS. En particular, resulta problemático de los
índices en los <em>hit groups</em>.</p>
<p>Para conocer a fondo cómo funciona la Shader Binding Table, puedes
visitar <span class="citation" data-cites="GemsII-SBT">(<a
href="#ref-GemsII-SBT" role="doc-biblioref">Usher 2021, 193</a>)</span>
o <span class="citation" data-cites="shader-binding-table">(<a
href="#ref-shader-binding-table" role="doc-biblioref">Usher
2019</a>)</span>.</p>
<div id="fig:SBT" class="fignos">
<figure>
<img loading="lazy" src="./img/04/SBT.png" alt="Figura 24: Fuente: (Usher 2019)" />
<figcaption aria-hidden="true"><span>Figura 24:</span> Fuente: <span
class="citation" data-cites="shader-binding-table">(<a
href="#ref-shader-binding-table" role="doc-biblioref">Usher
2019</a>)</span></figcaption>
</figure>
</div>
<h3 data-number="4.6.3" id="tipos-de-shaders"><span
class="header-section-number">4.6.3</span> Tipos de shaders</h3>
<p>El pipeline soporta varios tipos de shaders diferentes que cubren la
funcionalidad esencial de un ray tracer:</p>
<ul>
<li><strong>Ray generation shader</strong>: es el punto de inicio del
viaje de un rayo. Calcula punto de inicio y procesa el resultado final.
Idealmente, solo se invocan rayos desde aquí. La implementación se
encuentra en
<code>application/vulkan_ray_tracing/src/shaders/raytrace.rgen</code>.</li>
<li><strong>Closest hit shader</strong>: este shader se ejecuta cuando
un rayo impacta en una geometría por primera vez. Se pueden trazar rayos
recursivamente desde aquí (por ejemplo, para calcular oclusión
ambiental). El archivo correspondiente es
<code>application/vulkan_ray_tracing/src/shaders/raytrace.rchit</code>.</li>
<li><strong>Any-hit shader</strong>: similar al closest hit, pero
invocado en cada intersección del camino del rayo que cumpla <span
class="math inline">\(t \in [t_{min}, t_{max})\)</span>. Es comúnmente
utilizado en los cálculos de transparencias (<em>alpha-testing</em>).
Puedes comprobarlo en
<code>application/vulkan_ray_tracing/src/shaders/raytrace_rahit.glsl</code>.</li>
<li><strong>Miss shader</strong>: si el rayo no choca con ninguna
geometría –pega con el infinito–, se ejecuta este shader. Normalmente,
añade una pequeña contribución ambiental al rayo. Se halla
<code>application/vulkan_ray_tracing/src/shaders/raytrace.rmiss</code>.</li>
<li><strong>Intersection shader</strong>: este shader es algo diferente
al resto. Su función es calcular el punto de impacto de un rayo con una
geometría. Por defecto se utiliza un test triángulo - rayo. En nuestro
path tracer lo dejaremos por defecto, pero podríamos definir algún
método como los que vimos en la sección <a
href="#intersecciones-rayo---objeto">intersecciones rayo -
objeto</a>.</li>
</ul>
<p>Existe otro tipo de shader adicional denominado <strong>callable
shader</strong>. Este es un shader que se invoca desde otro shader. Por
ejemplo, un shader de intersección puede invocar a un shader de
oclusión. Otro ejemplo sería un closest hit que reemplaza un bloque
if-else por un shader para hacer cálculos de iluminación. Este tipo de
shaders no se han implementado en el path tracer, pero se podrían añadir
con un poco de trabajo.</p>
<h3 data-number="4.6.4" id="traspaso-de-información-entre-shaders"><span
class="header-section-number">4.6.4</span> Traspaso de información entre
shaders</h3>
<p>En ray tracing, los shaders por sí solos no pueden realizar todos los
cálculos necesarios para conseguir la imagen final. Necesitaremos enviar
información de uno a otro. Para conseguirlo tenemos diferentes
mecanismos:</p>
<p>El primero de ellos son las <strong>push constants</strong>. Estas
son variables que se pueden traspasar a los shaders (es decir, de CPU a
GPU), pero que no se pueden modificar entre fases. Únicamente podemos
mandar un pequeño número de variables, el cual se puede consultar
mediante <code>VkPhysicalDeviceLimits.maxPushConstantSize</code>.
Además, es importante tener en cuenta el alineamiento de las estructuras
almacenadas.</p>
<p>Nuestro path tracer tiene implementado actualmente (19 de abril de
2022) las siguientes constantes:</p>
<pre><code class="language-cpp">struct PushConstantRay {
    vec4  clearColor;     // Color ambiental
    vec3  lightPosition;
    float lightIntensity;
    int   lightType;
    int   maxDepth;       // Cuántos rebotes máximos permitimos
    int   nb_samples;     // Para antialiasing
    int   frame;          // Para acumulación temporal
};</code></pre>
<p>¿Y si queremos pasar información mutable entre shaders?</p>
<p>Para eso están los <strong>payloads</strong>. Cada rayo puede llevar
información adicional, que se conoce como carga. En esencia, es como una
pequeña mochila: el rayo puede recoger información de un shader y
pasarlo a otro. Esto resulta <em>muy</em> útil, por ejemplo, a la hora
de calcular la radiancia de un camino, o saber desde qué punto venía el
rayo. Se crean mediante la estructura <code>rayPayloadEXT</code>, y se
reciben en otro shader mediante <code>rayPayloadInEXT</code>. Es
importante controlar que el tamaño de la carga no sea excesivamente
grande.</p>
<h3 data-number="4.6.5" id="creación-de-la-ray-tracing-pipeline"><span
class="header-section-number">4.6.5</span> Creación de la ray tracing
pipeline</h3>
<p>El código de la creación de la pipeline está encapsulado en la
función <code>Engine::createRtPipeline()</code>, que se puede consultar
en el archivo
<code>application/vulkan_ray_tracing/src/engine.cpp</code>.</p>
<p>En esencia, este método realiza las siguientes tareas:</p>
<ol type="1">
<li>Define las fases o <em>stages</em> que tendrán los shaders.</li>
<li>Prepara las estructuras <code>VkPipelineShaderStageCreateInfo</code>
para almacenar la información de cada fase.</li>
<li>Carga cada archivo de shader compilado <code>.spv</code> en la
estructura junto con sus parámetros correctos.</li>
<li>Configura correctamente cada <em>shader group</em>.</li>
<li>Prepara las <em>push constants</em>.</li>
<li>Hace el setup del <em>pipeline layout</em> junto a sus descriptor
sets.</li>
<li>Limpia la información innecesaria creada por la función.</li>
</ol>
<h2 data-number="4.7" id="materiales-y-objetos"><span
class="header-section-number">4.7</span> Materiales y objetos</h2>
<p>El formato de materiales y objetos usados es el
<strong>Wavefront</strong> (<code>.obj</code>). Aunque es un sistema
relativamente antiguo y sencillo, se han usado definiciones específicas
en los materiales para adaptarlo a Physically Based Rendering. Entre los
parámetros del archivo de materiales <code>.mtl</code>, destacan:</p>
<ul>
<li><span class="math inline">\(K_a \in [0, 1]^3\)</span>: representa el
color ambiental. Dado que esto es un path tracer físicamente realista,
no se usará.</li>
<li><span class="math inline">\(K_d \in [0, 1]^3\)</span>: componente
difusa.</li>
<li><span class="math inline">\(K_s \in [0, 1]^3\)</span>: componente
especular. Viene acompañada del exponente especular <span
class="math inline">\(N_s \in [0, 1000]\)</span>. Usualmente, <span
class="math inline">\(N_s = 10\)</span>. Controla los brillos en los
modelos de Blinn-Phong.</li>
<li><span class="math inline">\(d \in [0, 1]\)</span>
(<em>dissolve</em>): representa la transparencia. Alternativamente, se
usa <span class="math inline">\(T_r = 1 - d\)</span>.</li>
<li><span class="math inline">\(T_f \in [0, 1]^3\)</span>: filtro de
transmisión.</li>
<li><span class="math inline">\(N_i \in [0.001, 10]\)</span>: índice de
refracción. Usualmente <span class="math inline">\(N_i =
1\)</span>.</li>
<li><span class="math inline">\(K_e \in [0, 1]^3\)</span>: componente
emisiva (PBR).</li>
<li>Todos los valores con tres componentes pueden presentar un
<em>texture map</em>.</li>
</ul>
<p>Existe un parámetro adicional llamado <code>illum</code>. Controla el
modelo de iluminación usado. Nosotros lo usaremos para distinguir tipos
diferentes de materiales. Los códigos representan lo siguiente:</p>
<table>
<colgroup>
<col style="width: 14%" />
<col style="width: 35%" />
<col style="width: 26%" />
<col style="width: 23%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Modelo</strong></th>
<th><strong>Color</strong></th>
<th><strong>Reflejos</strong></th>
<th><strong>Transparencias</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><code>0</code></td>
<td>Difusa</td>
<td>No</td>
<td>No</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>1</code></td>
<td>Difusa, ambiental</td>
<td>No</td>
<td>No</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>2</code></td>
<td>Difusa, especular, ambiental</td>
<td>No</td>
<td>No</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>3</code></td>
<td>Difusa, especular, ambiental</td>
<td>Ray traced</td>
<td>No</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>4</code></td>
<td>Difusa, especular, ambiental</td>
<td>Ray traced</td>
<td>Cristal</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>5</code></td>
<td>Difusa, especular, ambiental</td>
<td>Ray traced (Fresnel)</td>
<td>No</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>6</code></td>
<td>Difusa, especular, ambiental</td>
<td>Ray traced</td>
<td>Refracción</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>7</code></td>
<td>Difusa, especular, ambiental</td>
<td>Ray traced (Fresnel)</td>
<td>Refracción</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>8</code></td>
<td>Difusa, especular, ambiental</td>
<td>Sí</td>
<td>No</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>9</code></td>
<td>Difusa, especular, ambiental</td>
<td>Sí</td>
<td>Cristal</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>10</code></td>
<td>Sombras arrojadizas</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<pre><code class="language-cpp">// host_device.h
struct WaveFrontMaterial
{
  vec3  ambient;
  vec3  diffuse;
  vec3  specular;
  vec3  transmittance;
  vec3  emission;
  float shininess;
  float ior;       // index of refraction
  float dissolve;  // 1 == opaque; 0 == fully transparent
  int   illum;     // illumination model (see http://www.fileformat.info/format/material/)
  int   textureId;
};</code></pre>
<h2 data-number="4.8" id="fuentes-de-luz"><span
class="header-section-number">4.8</span> Fuentes de luz</h2>
<p>La última estructura de datos importante que debemos estudiar es la
utilizada para las fuentes de luces. Desafortunadamente, en este trabajo
no se ha implementado una abstracción sólida.</p>
<p>Se ha reaprovechado la definición del <a
href="#setup-del-proyecto">rasterizador por defecto</a> para que tanto
el path tracer como el anterior utilicen fácilmente iluminación
estática.</p>
<p>La idea básica es que, en vez de depender de los elementos de la
escena para proporcionar luz, se conozca una fuente de iluminación en
todo momento. Dicha fuente puede ser puntual o direccional, y puede ser
controlada mediante la interfaz. El estado de la fuente se traspasa a
los shaders mediante una push constant:</p>
<pre><code>struct PushConstantRay
{
    ...
    vec3  light_position;
    float light_intensity;
    int   light_type;
};</code></pre>
<p>El parámetro <code>light_intensity</code> corresponde a la potencia
<span class="math inline">\(\Phi\)</span>, y el tipo
<code>light_type</code> puede ser <code>0</code> para puntual o
<code>1</code> para direccional.</p>
<p>Claramente esta decisión técnica favocere facilidad de implementación
en detrimento de flexibilidad, solidez y correctitud. Esta interfaz es
una de las áreas de futura mejora, y haría falta una revisión
considerable. Sin embargo, por el momento, funciona.</p>
<p>La implementación en los shaders es muy sencilla. Podemos usar lo
aprendido en <a
href="#next-event-estimation-o-muestreo-directo-de-fuentes-de-luz">muestreo
directo de fuentes de luz</a>. En el closest hit, primero calculamos la
información relativa a la posición y la intensidad de la luz:</p>
<pre><code class="language-glsl">vec3 L;
float light_intensity = pcRay.light_intensity;
float light_distance = 100000.0;

float pdf_light       = 1;  // prob. de escoger ese punto de la fuente de luz
float cos_theta_light = 1;  // Ángulo entre la la dir. del rayo y luz.

if (pcRay.light_type == 0) {         // Point light
    vec3 L_dir = pcRay.light_position - world_position;  // vector hacia la luz

    light_distance   = length(L_dir);
    light_intensity = pcRay.light_intensity / (light_distance * light_distance);
    L               = normalize(L_dir);
    // Solo tenemos un punto =&gt; pdf light = 1, cos_theta light = 1.
    cos_theta_light = dot(L, world_normal);
}
else if (pcRay.light_type == 1) {    // Directional light
    L = normalize(pcRay.light_position);
    cos_theta_light = dot(L, world_normal);
}</code></pre>
<p>Sin embargo, esto no es suficiente. Se nos olvida comprobar un
detalle sumamente importante:</p>
<p>¿Se ve la fuente de luz desde el punto de intersección?</p>
<p>Si no es así, ¡no tiene sentido que calculemos la influencia
luminaria de la fuente! La carne de burro no se transparenta, después de
todo. A no ser que sea un toro hecho de algún material que presente
transmitancia, en cuyo caso se debería refractar acordemente el rayo de
luz.</p>
<p>Volviendo al tema: este tipo de problemas de oclusión se suelen
resolver mediante algún tipo de test de visibilidad. El más habitual es
usar <strong>shadow rays</strong>. Al preparar la <a
href="#la-ray-tracing-pipeline">pipeline</a> fijamos el stage de los
shadow rays precisamente por este motivo.</p>
<p>La continuación del código quería de la siguiente forma:</p>
<pre><code class="language-glsl">if (dot(normal, L) &gt; 0) {
    // Preparar la invocación del shadow ray
    float tMin = 0.001;
    float tMax = light_distance;

    vec3 origin = gl_WorldRayOriginEXT + gl_WorldRayDirectionEXT * gl_HitTEXT;
    vec3 ray_dir = L;

    uint flags = gl_RayFlagsSkipClosestHitShaderEXT;
    prdShadow.is_hit = true;
    prdShadow.seed = prd.seed;

    traceRayEXT(topLevelAS,
        flags,       // rayFlags
        0xFF,        // cullMask
        1,           // sbtRecordOffset =&gt; invocar el shader de sombras
        0,           // sbtRecordStride
        1,           // missIndex
        origin,      // ray origin
        tMin,        // ray min range
        ray_dir,     // ray direction
        tMax,        // ray max range
        1            // payload (location = 1)
    );

    float attenuation = 1;

    if (!prdShadow.is_hit) {
        hit_value = hit_value + light_intensity*BSDF*cos_theta_light / pdf_light;
    }
    else {
        attenuation = 1.0 / (1.0 + light_distance);
    }
}</code></pre>
<p>Y con esto, hemos conseguido añadir dos tipos de fuentes de
iluminación.</p>
<h2 data-number="4.9"
id="antialiasing-mediante-jittering-y-acumulación-temporal"><span
class="header-section-number">4.9</span> Antialiasing mediante jittering
y acumulación temporal</h2>
<p>Normalmente, mandamos los rayos desde el centro de un pixel. Podemos
conseguir una mejora sustancial de la calidad con un pequeño truco: en
vez de generarlos siempre desde el mismo sitio, le aplicamos una pequeña
perturbación (<em>jittering</em>). Así, tendremos una variación de
colores para un mismo pixel, por lo que podemos hacer una ponderación de
todos ellos. A este proceso lo que llamamos <strong>acumulación
temporal</strong>.</p>
<p>Es importante destacar que el efecto de esta técnica solo es válido
cuando la <strong>cámara se queda estática</strong>. Al cambiar de
posición, la información del píxel se ve alterada significativamente,
por lo que debemos reconstruir las muestras desde el principio.</p>
<p>La implementación es muy sencilla. Está basada en el tutorial de
<span class="citation" data-cites="nvpro-samples-tutorial">(<a
href="#ref-nvpro-samples-tutorial" role="doc-biblioref">Nvidia
2022a</a>, jitter camera)</span>. Debemos modificar tanto el motor como
los shaders para llevar el recuento del número de frames en las push
constants.</p>
<p>Definimos el número máximo de frames que se pueden acumular:</p>
<pre><code class="language-cpp">// engine.h
class Engine {
    //...
    int m_maxAcumFrames {100};
}</code></pre>
<p>Las push constant deberán llevar un registro del frame en el que se
encuentran, así como un número máximo de muestras a acumular para un
pixel:</p>
<pre><code class="language-cpp">// host_device.h
struct PushConstantRay {
    //...
    int   frame;
    int   nb_samples
}</code></pre>
<p>El número de frame se reseteará cuando la cámara se mueva, la ventana
se reescale, o se produzca algún efecto similar en la aplicación.</p>
<p>Finalmente, en los shaders podemos implementar lo siguiente:</p>
<pre><code class="language-glsl">// raytrace.rgen
vec3 pixel_color = vec3(0);

for (int smpl = 0; smpl &lt; pcRay.nb_samples; smpl++) {
    pixel_color += sample_pixel(image_coords, image_res);
}

pixel_color = pixel_color / pcRay.nb_samples;

if (pcRay.frame &gt; 0) {
    vec3 old_color = imageLoad(image, image_coords).xyz;
    vec3 new_result = mix(
        old_color,
        pixel_color,
        1.f / float(pcRay.frame + 1)
    );

    imageStore(image, image_coords, vec4(new_result, 1.f));
}
else {
    imageStore(image, image_coords, vec4(pixel_color, 1.0));
}</code></pre>
<pre><code class="language-glsl">// pathtrace.glsl
vec3 sample_pixel() {
    float r1 = rnd(prd.seed);
    float r2 = rnd(prd.seed);

    // Subpixel jitter: mandar el rayo desde una pequeña perturbación del pixel para aplicar antialiasing
    vec2 subpixel_jitter = pcRay.frame == 0
        ? vec2(0.5f, 0.5f)
        : vec2(r1, r2);

    const vec2 pixelCenter = vec2(image_coords.xy) + subpixel_jitter;

    // ...

    vec3 radiance = pathtrace(rayo);
}</code></pre>
<p>En la sección de la <a
href="#comparativa-con-in-one-weekend">comparativa</a> estudiaremos a
fondo los efectos de esta técnica.</p>
<h2 data-number="4.10" id="corrección-de-gamma"><span
class="header-section-number">4.10</span> Corrección de gamma</h2>
<p>Con el código de la sección <a
href="#antialiasing-mediante-jittering-y-acumulación-temporal">anterior</a>,
existe un problema con los colores finales. El algoritmo de pathtracing
no limita el máximo valor que puede tomar un camino. Sin embargo, Vulkan
espera que la terna RGB provista esté en <span class="math inline">\([0,
1]^3\)</span>. Esto implica que los colores acabarán quemados.</p>
<div id="fig:quemado" class="fignos">
<figure>
<img loading="lazy" src="./img/04/Quemado.png" style="width:80.0%"
alt="Figura 25: Fíjate en la parte de la izquierda. La pared roja aparece demasiado brillante; especialmente, aquella impactada por la fuente de luz." />
<figcaption aria-hidden="true"><span>Figura 25:</span> Fíjate en la
parte de la izquierda. La pared roja aparece demasiado brillante;
especialmente, aquella impactada por la fuente de luz.</figcaption>
</figure>
</div>
<p>Podemos corregir este problema mediante <strong>corrección de
gamma</strong>. Esta es una operación no lineal utilizada en fotografía
para corregir la luminacia, con el fin de compensar la percepción no
lineal del brillo por parte de los humanos. En este caso, lo haremos al
estilo <span class="citation" data-cites="Shirley2020RTW1">(<a
href="#ref-Shirley2020RTW1" role="doc-biblioref">Shirley
2020a</a>)</span>: tras tomar las muestras, aplicaremos una corrección
para <span class="math inline">\(\gamma = 2.2\)</span>, lo cual implica
elevar cada componente del píxel a la potencia <span
class="math inline">\(\frac{1}{2.2}\)</span>; es decir, <span
class="math inline">\((r_f, g_f, b_f) = (r^{\frac{1}{2.2}},
g^{\frac{1}{2.2}}, b^{\frac{1}{2.2}})\)</span>.</p>
<p>Tras esto, limitaremos el valor máximo de cada componente a 1 con la
operación <span class="math inline">\(clamp()\)</span>.</p>
<pre><code class="language-glsl">vec3 pixel_color = vec3(0);

for (int smpl = 0; smpl &lt; pcRay.nb_samples; smpl++) {
    pixel_color += sample_pixel(image_coords, image_res);
}

pixel_color = pixel_color / pcRay.nb_samples;

if (USE_GAMMA_CORRECTION == 1) {
    pixel_color = pow(pixel_color, vec3(1.0 / 2.2));  // Gamma correction for 2.2
    pixel_color = clamp(pixel_color, 0.0, 1.0);
}</code></pre>
<div id="fig:correccion_gamma" class="fignos">
<figure>
<img loading="lazy" src="./img/04/Corrección%20de%20gamma.png" style="width:80.0%"
alt="Figura 26: Con la corección de gamma aplicada, vemos que los colores de la foto no son tan intensos." />
<figcaption aria-hidden="true"><span>Figura 26:</span> Con la corección
de gamma aplicada, vemos que los colores de la foto no son tan
intensos.</figcaption>
</figure>
</div>
<blockquote>
<p>Espera. Esa no parece la misma escena. ¿No han cambiado los colores
demasiado?</p>
</blockquote>
<p>¡Bien visto! Es cierto que los colores se ven significativamente
alterados. Esto es debido a la conversión de un espacio lineal de
respuesta de radiancia a uno logarítmico. Algunos autores como Íñigo
Quílez (coautor de la página Shader Toy) prefieren asumir esta
deficiencia, y modificar los materiales acordemente a esto <span
class="citation" data-cites="gamma-correction">(<a
href="#ref-gamma-correction" role="doc-biblioref">Quílez 2013</a>, The
Color Space)</span>.</p>
<p>Nosotros no nos preocuparemos especialmente por esto. Este no es un
trabajo sobre teoría del color, aunque nos metamos en varias partes en
ella. El área de tone mapping es extensa y merecería su propio
estudio.</p>
<p>Es importante mencionar que sin acumulación temporal, el código
anterior produciría variaciones significativas para pequeños
movimientos. Hay otras formas de compensarlo, como dividir por el valor
promedio de las muestras más brillantes. Nosotros hemos optado por
mezclar los píxeles generados a lo largo del tiempo.</p>
<h1 data-number="5" id="análisis-de-rendimiento"><span
class="header-section-number">5</span> Análisis de rendimiento</h1>
<p>En este capítulo vamos a analizar el resultado final del proyecto.
Estudiaremos cómo se ve el motor, cómo rinde en términos de <em>frame
time</em>, y compararemos las imágenes producidas con otras similares;
tanto producidas por otros motores, como situaciones en la vida
real.</p>
<h2 data-number="5.1" id="usando-el-motor"><span
class="header-section-number">5.1</span> Usando el motor</h2>
<p>Una vez se ha <a href="#compilación-y-ejecución">compilado</a> el
proyecto, puedes encontrar el ejecutable en
<code>./application/bin_x64/Debug</code>. Abre el binario para entrar en
el programa.</p>
<div id="fig:asmiray" class="fignos">
<figure>
<img loading="lazy" src="./img/05/Asmiray.png" style="width:70.0%"
alt="Figura 27: Al abrir el motor, te encontrarás con una pantalla similar a esta: una escena cargada junto a un panel lateral con numerosas opciones." />
<figcaption aria-hidden="true"><span>Figura 27:</span> Al abrir el
motor, te encontrarás con una pantalla similar a esta: una escena
cargada junto a un panel lateral con numerosas opciones.</figcaption>
</figure>
</div>
<p>Si alguna vez has usado un motor de renderización en 3D (como
Blender, Unity, Unreal Engine o AutoCAD), el comportamiento debería
resultarte familiar. El uso de nuestro progama es muy similar al de los
anteriores:</p>
<ul>
<li>El <strong>botón izquierdo del ratón rota</strong> la cámara
alderedor del punto de mira.</li>
<li>Para acercar o alejar la cámara, utiliza la <strong>rueda de
scroll</strong> o el <strong>botón derecho del ratón + hacia arriba o
abajo</strong>.</li>
<li>Si quieres moverte lateralmente, mantén pulsado la tecla
<strong>control</strong> y utiliza el <strong>botón izquierdo +
arrastrar</strong>. Alternativamente, <strong>aprieta el click de la
rueda del ratón</strong> y múevete.</li>
<li>Para girar la cámara alderedor como si de un <em>first person
shooter</em> se tratara, pulsa <strong>alt + click
izquierdo</strong>.</li>
</ul>
<p>Puedes cambiar el modo de cámara en la pestaña “Extra” de la interfaz
gráfica. Los diferentes modos alternan entre las acciones listadas
anteriormente.</p>
<p>Para ocultar la interfaz gráfica, pulsa <strong>F10</strong>.</p>
<h3 data-number="5.1.1" id="cambio-de-escena"><span
class="header-section-number">5.1.1</span> Cambio de escena</h3>
<p>El programa viene acompañado de varios mapas. Desafortunadamente,
para cambiar de escena es necesario recompilar el programa. Las
instrucciones necesarias para conseguirlo son las siguientes:</p>
<ul>
<li>Ubica la sentencia <code>load_scene(Scene::escena, engine);</code>
que se encuentra en el archivo <code>main.cpp</code>.</li>
<li>Cambia el valor del primer parámetro: reemplaza
<code>Scene::escena</code> por alguna entrada del enumerado
<code>Scene</code>. Puedes encontrar sus posiblidades en el archivo
<code>Scenes.hpp</code>.</li>
<li>Recompila el programa.</li>
</ul>
<p>Las escenas son las siguientes:</p>
<table>
<colgroup>
<col style="width: 12%" />
<col style="width: 65%" />
<col style="width: 21%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>Nombre de escena</strong></th>
<th><strong>Descripción</strong></th>
<th><strong>Imagen</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>cube_default</code></td>
<td>La escena por defecto del programa. Muestra un simple cubo.</td>
<td><img loading="lazy" src="./img/05/cube_default.png" title="fig:" /></td>
</tr>
<tr class="even">
<td><code>any_hit</code></td>
<td>Desmostración de las capacidades del shader <em>anyhit</em>.</td>
<td><img loading="lazy" src="./img/05/any_hit.png" title="fig:" /></td>
</tr>
<tr class="odd">
<td><code>cube_reflective</code></td>
<td>Ejemplifica <em>ray traced reflections</em>.</td>
<td><img loading="lazy" src="./img/05/cube_reflective.png" title="fig:" /></td>
</tr>
<tr class="even">
<td><code>medieval</code> <code>_building</code></td>
<td>Una sencilla escena que contiene una casa medieval con
texturas.</td>
<td><img loading="lazy" src="./img/05/medieval_building.png" title="fig:" /></td>
</tr>
<tr class="odd">
<td><code>cubes</code></td>
<td>Dos cubos de diferente material sobre un plano reflectante.</td>
<td><img loading="lazy" src="./img/05/cubes.png" title="fig:" /></td>
</tr>
<tr class="even">
<td><code>cornell_box</code> <code>_original</code></td>
<td>Una reconstrucción de la caja de Cornell original <span
class="citation" data-cites="cornell-box-original">(<a
href="#ref-cornell-box-original" role="doc-biblioref">Cornell University
2005</a>)</span>.</td>
<td><img loading="lazy" src="./img/05/cornell_box_original.png" title="fig:" /></td>
</tr>
<tr class="odd">
<td><code>cornell_box</code> <code>_mirror</code></td>
<td>Similar a la caja original, esta escena es una recreación de <span
class="citation" data-cites="cornell-box-compare">(<a
href="#ref-cornell-box-compare" role="doc-biblioref">Cornell University
1998</a>)</span>.</td>
<td><img loading="lazy" src="./img/05/cornell_box_mirror.png" title="fig:" /></td>
</tr>
<tr class="even">
<td><code>cornell_box</code> <code>_esferas</code></td>
<td>Una caja de Cornell con esferas. Se puede comparar con <span
class="citation" data-cites="Jensen2001">(<a href="#ref-Jensen2001"
role="doc-biblioref">Jensen 2001, 107</a> fig. 9.10)</span>.</td>
<td><img loading="lazy" src="./img/05/cornell_box_esferas.png" title="fig:" /></td>
</tr>
<tr class="odd">
<td><code>cornell_box</code> <code>_saturada</code></td>
<td>Otra caja de Cornell similar a la original, pero con las paredes
saturadas.</td>
<td><img loading="lazy" src="./img/05/cornell_box_saturada.png" title="fig:" /></td>
</tr>
<tr class="even">
<td><code>cornell_box</code> <code>_glossy</code></td>
<td>En esta caja se encuentran dos esferas de diferente material. Se
puede comparar con <span class="citation"
data-cites="cornell-box-glossy">(<a href="#ref-cornell-box-glossy"
role="doc-biblioref">"Jensen "1996", 17</a>, fig. 6)</span></td>
<td><img loading="lazy" src="./img/05/cornell_box_glossy.png" title="fig:" /></td>
</tr>
<tr class="odd">
<td><code>cornell_box</code> <code>_iow</code></td>
<td>La última caja de Cornell implementada en <span class="citation"
data-cites="Shirley2020RTW3">(<a href="#ref-Shirley2020RTW3"
role="doc-biblioref">Shirley 2020c</a>)</span>.</td>
<td><img loading="lazy" src="./img/05/cornell_box_iow.png" title="fig:" /></td>
</tr>
<tr class="even">
<td><code>cornell_box</code> <code>_vacia</code></td>
<td>La caja original sin las cajitas pequeñas dentro.</td>
<td><img loading="lazy" src="./img/05/cornell_box_vacia.png" title="fig:" /></td>
</tr>
<tr class="odd">
<td><code>cornell_box</code> <code>_vacia_an</code></td>
<td>Similar a la anterior, pero con las paredes naranjas y azules.</td>
<td><img loading="lazy" src="./img/05/cornell_box_vacia_an.png" title="fig:" /></td>
</tr>
<tr class="even">
<td><code>cornell_box</code> <code>_blanca</code></td>
<td>Una caja vacía. Es un benchmark infernal para el ruido generado por
la iluminación global.</td>
<td><img loading="lazy" src="./img/05/cornell_box_blanca.png" title="fig:" /></td>
</tr>
</tbody>
</table>
<p>Ten en cuenta que las imágenes de las escenas no son definitivas.
Están sujetas a cambios, pues se podría cambiar el comportamiento de los
shaders.</p>
<h2 data-number="5.2" id="path-tracing-showcase"><span
class="header-section-number">5.2</span> Path tracing showcase</h2>
<p>A lo largo de este trabajo hemos visto una gran variedad de conceptos
desde el punto de vista teórico. Ahora es el momento de ponerlo en
práctica.</p>
<h3 data-number="5.2.1" id="materiales"><span
class="header-section-number">5.2.1</span> Materiales</h3>
<p>Empecemos por materiales. Se han implementado unos cuantos tipos
diferentes, los cuales veremos ilustrados a continuación.</p>
<p>Los más simples son los <a
href="#reflexión-difusa-o-lamberiana">difusos</a>. La caja de Cornell
original contiene dos objetos de este tipo:</p>
<div id="fig:materiales_difusos" class="fignos">
<figure>
<img loading="lazy" src="./img/05/Materiales%20difusos.png" style="width:70.0%"
alt="Figura 28: Materiales difusos de la escena Cornell Box original. Vemos que la luz se esparce uniformemente al rebotar en el objeto." />
<figcaption aria-hidden="true"><span>Figura 28:</span> Materiales
difusos de la escena Cornell Box original. Vemos que la luz se esparce
uniformemente al rebotar en el objeto.</figcaption>
</figure>
</div>
<p>Los materiales <a href="#reflexión-especular-no-perfecta">especulares
<em>glossy</em></a> han sido modificados ligeramente para simular el
parámetro de <em>roughness</em> de los metales, para compararlos con los
de <span class="citation" data-cites="Shirley2020RTW1">(<a
href="#ref-Shirley2020RTW1" role="doc-biblioref">Shirley
2020a</a>)</span>:</p>
<div id="fig:materiales_glossy" class="fignos">
<figure>
<img loading="lazy" src="./img/05/Materiales%20glossy.png" style="width:70.0%"
alt="Figura 29: Materiales especulares metálicos de la escena Cornell Box glossy" />
<figcaption aria-hidden="true"><span>Figura 29:</span> Materiales
especulares metálicos de la escena Cornell Box glossy</figcaption>
</figure>
</div>
<p>Si hay algo en lo que destaca ray tracing, es en la simulación de <a
href="#reflexión-especular-perfecta">espejos</a>. En rasterización
debemos recurrir a técnicas específicas como reflejos planares o
<em>cubemaps</em>. Ray tracing solventa el problema con elegancia:</p>
<div id="fig:materiales_espejos" class="fignos">
<figure>
<img loading="lazy" src="./img/05/Materiales%20espejos.png" style="width:70.0%"
alt="Figura 30: Una caja que actúa como un espejo prácticamente perfecto en la escena cornell_box_mirror" />
<figcaption aria-hidden="true"><span>Figura 30:</span> Una caja que
actúa como un espejo prácticamente perfecto en la escena
<code>cornell_box_mirror</code></figcaption>
</figure>
</div>
<p>En la siguiente escena observamos dos esferas: una que presenta
refracción y otra que no. Ambas utilizan las ecuaciones de Fresnel para
modelar el comportamiento de la luz.</p>
<div id="fig:reflectantes" class="fignos">
<figure>
<img loading="lazy" src="./img/05/Materiales%20reflectantes.png" style="width:70.0%"
alt="Figura 31: La esfera de la derecha refracta la luz al pasar por ella, adquiriendo en el proceso un color más oscuro. También podemos ver la esfera de la izquierda recursivamente, dentro del propio reflejo de la esfera." />
<figcaption aria-hidden="true"><span>Figura 31:</span> La esfera de la
derecha refracta la luz al pasar por ella, adquiriendo en el proceso un
color más oscuro. También podemos ver la esfera de la izquierda
recursivamente, dentro del propio reflejo de la esfera.</figcaption>
</figure>
</div>
<p>Los materiales transparentes los gestiona el shader <a
href="#tipos-de-shaders">anyhit</a>. Permite descartar las
intersecciones con aquellos objetos transparentes para permitir pasar
algunos rayos:</p>
<div id="fig:materiales_transparentes" class="fignos">
<figure>
<img loading="lazy" src="./img/05/Materiales%20transparentes.png" style="width:70.0%"
alt="Figura 32: El modelo del Wuson, pero transparente." />
<figcaption aria-hidden="true"><span>Figura 32:</span> El modelo del
Wuson, pero transparente.</figcaption>
</figure>
</div>
<h3 data-number="5.2.2" id="fuentes-de-luz-1"><span
class="header-section-number">5.2.2</span> Fuentes de luz</h3>
<p>En la primera versión del motor, se han implementado dos tipos de
fuentes de luces: puntuales y direccionales.</p>
<p>Las <strong>fuentes de luz puntuales</strong> (<em>spotlights</em> en
inglés) emiten luz alrededor suya, como si de pequeños soles se
trataran. La figura [<a href="#fig:spotlights">33</a>] muestra cómo se
comportan en la caja de Cornell original.</p>
<div id="fig:spotlights" class="fignos">
<figure>
<img loading="lazy" src="./img/05/Spotlight.png" style="width:60.0%"
alt="Figura 33: Una fuente de luz puntual iluminando la caja de Cornell original. Vemos cómo se proyectan sombras hacian la pared." />
<figcaption aria-hidden="true"><span>Figura 33:</span> Una fuente de luz
puntual iluminando la caja de Cornell original. Vemos cómo se proyectan
sombras hacian la pared.</figcaption>
</figure>
</div>
<p>Por otro lado, las <strong>luces direccionales</strong>: imitan la
luz proporcionada por algún objeto infinitamente lejano. Puedes ver un
ejemplo en la figura [<a href="#fig:directional_lights">34</a>].</p>
<div id="fig:directional_lights" class="fignos">
<figure>
<img loading="lazy" src="./img/05/Directional.png" style="width:60.0%"
alt="Figura 34: La caja de Cornell original iluminada por una luz direccional" />
<figcaption aria-hidden="true"><span>Figura 34:</span> La caja de
Cornell original iluminada por una <strong>luz
direccional</strong></figcaption>
</figure>
</div>
<p>Como las puntuales se comportan de manera muy similar en la caja de
Cornell, podemos referirnos a la escena del edificio medieval para ver
una diferencia más sustancial [<a
href="#fig:spotlights_medieval">35</a>, <a
href="#fig:directional_medieval">36</a>]. En este caso, se aprecia el
radio de influencia de la luz puntual.</p>
<div id="fig:spotlights_medieval" class="fignos">
<figure>
<img loading="lazy" src="./img/05/Spotlight%202.png" style="width:60.0%"
alt="Figura 35: Luz puntual en la escena medieval_building." />
<figcaption aria-hidden="true"><span>Figura 35:</span> Luz puntual en la
escena <code>medieval_building</code>.</figcaption>
</figure>
</div>
<div id="fig:directional_medieval" class="fignos">
<figure>
<img loading="lazy" src="./img/05/Directional%202.png" style="width:60.0%"
alt="Figura 36: Luz direccional en la escena medieval_building." />
<figcaption aria-hidden="true"><span>Figura 36:</span> Luz direccional
en la escena <code>medieval_building</code>.</figcaption>
</figure>
</div>
<h3 data-number="5.2.3" id="iluminación-global"><span
class="header-section-number">5.2.3</span> Iluminación global</h3>
<p>La <strong>iluminación global</strong> es un fenómeno físico que se
refiere a luz que proviene de <em>todas</em> direcciones. Es el efecto
que propicia el rebote constante de los fotones emitidos por fuentes de
luz hacia una escena, adquiriendo las propiedades de los materiales en
los que rebotan.</p>
<p>Dicho de esta forma, es difícil imaginarse cómo se comporta en la
vida real. Para ilustrarlo, tomemos dos fotografías que se asemejan a la
caja de Cornell.</p>
<p>En la escena [<a href="#fig:cornell_bath_1">37</a>] observamos cómo
la luz del sol entra desde la parte de la derecha, rebotando en todo el
espacio. Notamos cómo la escena tiene una tonalidad natural y cálida.
Sin embargo, esta impresión es fácilmente modificable si alteramos la
forma de arrojar la luz. Cerrando la puerta (la cual no puede ser vista
en la fotografía, pero se encuentra a la derecha y es idéntica al
cristal), la iluminación cambia completamente [<a
href="#fig:cornell_bath_2">38</a>].</p>
<div id="fig:cornell_bath_1" class="fignos">
<figure>
<img loading="lazy" src="./img/05/Cornell%20bath%201.jpg" style="width:60.0%"
alt="Figura 37: Escena similar a la caja de Cornell, en la vida real. También es mi baño." />
<figcaption aria-hidden="true"><span>Figura 37:</span> Escena similar a
la caja de Cornell, en la vida real. También es mi baño.</figcaption>
</figure>
</div>
<p>Podemos observar cómo todos los materiales adquieren un tinte rojizo,
debido a la influencia tanto difusa como especular de la pared de la
izquierda. Objetos que antes eran blancos inmaculados se vuelven rojos,
como el inodoro. Incluso aquellas zonas en sombra consiguen un color
rojizo. Esto es debido a que los fotones rebotan en el cristal rojo
cuando más energía tienen. De esta forma, en la siguiente dirección
tomada, los rayos transportan esta propiedad al resto de materiales, los
cuales se visualizan como una tonalidad roja.</p>
<div id="fig:cornell_bath_2" class="fignos">
<figure>
<img loading="lazy" src="./img/05/Cornell%20bath%202.jpg" style="width:60.0%"
alt="Figura 38: Cuando cambiamos la forma de iluminar la escena, los colores se ven drásticamente modificados" />
<figcaption aria-hidden="true"><span>Figura 38:</span> Cuando cambiamos
la forma de iluminar la escena, los colores se ven drásticamente
modificados</figcaption>
</figure>
</div>
<p><strong>Path tracing consigue este efecto de manera natural</strong>
por diseño. Este es uno de sus mayores puntos fuertes, pero a la vez lo
hace computacionalmente caro. Dado que la escena de [<a
href="#fig:cornell_bath_1">37</a>] y [<a
href="#fig:cornell_bath_2">38</a>] es, esencialmente, una caja de
Cornell, deberíamos apreciar un efecto similar en nuestra escena,
¿verdad?</p>
<p>¡Así es! La figura [<a href="#fig:global_illumination_1">39</a>] es
muy similar a la [<a href="#fig:cornell_bath_2">38</a>]. Se pueden
apreciar los mismos efectos en la caja izquierda, los cuales no ocurrían
con tanta intensidad en la escena original [fig. <a
href="#fig:materiales_difusos">28</a>]</p>
<div id="fig:global_illumination_1" class="fignos">
<figure>
<img loading="lazy" src="./img/05/Ambient%20occlusion%201.png" style="width:50.0%"
alt="Figura 39: La caja de Cornell original con luz direccional apuntando a la pared de la izquierda" />
<figcaption aria-hidden="true"><span>Figura 39:</span> La caja de
Cornell original con luz direccional apuntando a la pared de la
izquierda</figcaption>
</figure>
</div>
<p>Lo mismo ocurre cuando cambiamos el foco a la pared de la derecha. Al
ser verde, tintará el resto de los materiales de dicho color [Figura <a
href="#fig:global_illumination_2">40</a>].</p>
<div id="fig:global_illumination_2" class="fignos">
<figure>
<img loading="lazy" src="./img/05/Ambient%20occlusion%202.png" style="width:50.0%"
alt="Figura 40: Enfocando a la pared de la derecha, conseguimos un tinte verde para la escena" />
<figcaption aria-hidden="true"><span>Figura 40:</span> Enfocando a la
pared de la derecha, conseguimos un tinte verde para la
escena</figcaption>
</figure>
</div>
<p>Este efecto es esencial para proporcionar realismo a una imagen
digital. Sin iluminación global, los motores presentan un aspecto que
podríamos considerar <em>videojueguil</em>: imágenes planas, con sombras
abruptas y un aire de falsedad al que nos hemos llegado a acostumbrar.
Por ello se han implementado varias técnicas en rasterización para
suplir este efecto. Destacan los <em>lightmaps</em>, la <a
href="#materiales-y-objetos">componente ambiental</a> de los materiales,
<em>cubemaps</em>, oclusión ambiental e iluminación indirecta basada en
<em>probes</em>.</p>
<div id="fig:lightmpas" class="fignos">
<figure>
<img loading="lazy" src="./img/05/Shadowmap.png"
alt="Figura 41: Los lightmaps requieren que la geometría no pueda ser movida debido a que la sombra no es calculada en tiempo real. Fuente: Epic Games (2022b)" />
<figcaption aria-hidden="true"><span>Figura 41:</span> Los
<em>lightmaps</em> requieren que la geometría no pueda ser movida debido
a que la sombra no es calculada en tiempo real. Fuente: <span
class="citation" data-cites="lumen">Epic Games (<a href="#ref-lumen"
role="doc-biblioref">2022b</a>)</span></figcaption>
</figure>
</div>
<p>Para un vistazo más a fondo de la iluminación global, puedes
referirte al vídeo de Alex Battaglia en <span class="citation"
data-cites="df-global-illumination">(<a
href="#ref-df-global-illumination" role="doc-biblioref">Digital Foundry
2021b</a>)</span>, en el cual cubre diferentes formas de resolver este
problema, tanto en el caso de ray tracing como en el de
rasterización.</p>
<h2 data-number="5.3" id="rendimiento"><span
class="header-section-number">5.3</span> Rendimiento</h2>
<p>Path tracing es un algoritmo costoso. Teniendo en cuenta que tratamos
de desarrollar una aplicación en tiempo real, debemos prestar especial
atención al coste de renderizar un <em>frame</em>. En esta sección vamos
a hacer una comparativa de las diferentes opciones que se han
implementado en el motor, estudiando la relación calidad de imagen y
rendimiento.</p>
<p>Utilizaremos principalmente dos escenas:
<code>cornell_box_original</code> y <code>cornell_box_esferas</code>.
Esto es debido a que ofrecen cierta complejidad y los materiales de los
objetos permiten estudiar los parámetros del motor.</p>
<p>Para los análisis del rendimiento, se ha utilizado un procesador
<strong>Intel i5 12600K</strong>, una tarjeta gráfica Nvidia
<strong>2070 Super</strong> con un ligero overclock a 1900MHz y
<strong>2x8GB DDR4 3200MHz</strong> de RAM. A no ser que se diga lo
contrario, todas las imágenes tienen una resolución de 1280 x 720. Con
el fin de realizar una comparación justa, se ha implementado un modo de
benchmarking que se puede activar en el archivo
<code>globals.hpp</code>.</p>
<p>La medición del framerate ha sido realizada mediante la combinación
de los programas Afterburner y RTSS, los cuales han tomado muestras a
una tasa de 10 veces/s. EL procesamiento del <em>log</em> se ha llevado
a cabo con los notebooks de Jupyter disponibles en la carpeta
<code>./utilities</code> del repositorio; en la cual también se
encuentran los ya mencionados logs.</p>
<h3 data-number="5.3.1" id="número-de-muestras"><span
class="header-section-number">5.3.1</span> Número de muestras</h3>
<p>El principal parámetro que podemos variar es el número de muestras
por píxel. En un estimador de Monte Carlo [<a
href="#eq:mc_integral">28</a>], <span class="math inline">\(\hat{I}_N =
\frac{1}{N} \sum_{i = 1}^{N}{f(X_i)}\)</span>, corresponde a <span
class="math inline">\(N \in \mathbb{N}\)</span>.</p>
<div id="fig:grafica_samples" class="fignos">
<figure>
<img loading="lazy" src="./img/graficas/CB_original_comparativa_samples.png"
alt="Figura 42: Para conseguir esta gráfica, iniciamos la escena cornell_box_original, y sin mover la cámara, vamos cambiando el número de muestras." />
<figcaption aria-hidden="true"><span>Figura 42:</span> Para conseguir
esta gráfica, iniciamos la escena <code>cornell_box_original</code>, y
sin mover la cámara, vamos cambiando el número de muestras.</figcaption>
</figure>
</div>
<p>La figura [<a href="#fig:grafica_samples">42</a>] muestra cómo afecta
al rendimiento el valor de <span class="math inline">\(N\)</span>. Vemos
cómo un número bajo de muestras (alrededor de 5) produce un frametime de
aproximadamente 12 milisegundos, lo cual corresponde a 83 frames por
segundo. Duplicando <span class="math inline">\(N\)</span> hasta las 10
muestras, produce un aumento del frametime hasta los 20 ms de media (50
FPS). Algo similar pasa con el resto de valores: 15 muestras suponen una
media de 28 ms (35 FPS) y 20 muestras unos 35 ms (28 FPS). Sacamos en
claro que, en esta escena, <strong>no debemos aumentar las muestras a un
valor superior a 20</strong>, pues entraríamos en terreno de renderizado
en diferido. No debemos superar la barrera de los 33 milisegundos, pues
supondría una tasa de refresco de imagen inferior a los 30 FPS.</p>
<p>Podemos concluir que, en esta escena, el <strong>coste de una muestra
por píxel</strong> es de aproximadamente <strong>2
milisegundos</strong>. Este valor puede ser hallado promediando el coste
medio de cada frame en cada valor del parámetro.</p>
<p>El número de muestras tiene un grandísimo efecto en la calidad de
imagen. Volviendo a la escena anterior, podemos ver cómo cambia el ruido
al variar el parámetro <code>samples</code>. Para las siguientes
imágenes, se ha deshabilitado la <a
href="#acumulación-temporal">acumulación temporal</a>, pues en esencia,
proporcionaría un mayor número de muestras en el tiempo.</p>
<p>Con una única muestra por píxel, la imagen final aparece muy ruidosa.
Aumentarlo a 5 mejora bastante la situación, pero sigue habiendo
demasiado ruido [<a href="#fig:samples_1">43</a>]</p>
<div id="fig:samples_1" class="fignos">
<figure>
<img loading="lazy" src="./img/05/1,%205%20samples.png"
alt="Figura 43: Izquierda: 1 muestra. Derecha: 5 muestras" />
<figcaption aria-hidden="true"><span>Figura 43:</span>
<strong>Izquierda</strong>: 1 muestra. <strong>Derecha</strong>: 5
muestras</figcaption>
</figure>
</div>
<p>De forma similar, aumentarlo a 10 y a 20 implica un aumento de la
calidad visual significativa.</p>
<div id="fig:samples_2" class="fignos">
<figure>
<img loading="lazy" src="./img/05/10,%2020%20samples.png"
alt="Figura 44: Izquierda: 10 muestras. Derecha: 20 muestras" />
<figcaption aria-hidden="true"><span>Figura 44:</span>
<strong>Izquierda</strong>: 10 muestras. <strong>Derecha</strong>: 20
muestras</figcaption>
</figure>
</div>
<p>No obstante, el cambio de 10 a 20 no es tan significativo como de 1 a
5. Esto sugiere que debemos usar otras técnicas para reducir la varianza
del estimador. ¡La fuerza bruta no suele ser la solución!</p>
<h3 data-number="5.3.2" id="profundidad-de-un-rayo"><span
class="header-section-number">5.3.2</span> Profundidad de un rayo</h3>
<p>Una de las decisiones que tenemos que tomar en el diseño del
algoritmo es saber cuándo cortar un camino. Hay varias formas de
hacerlo, aunque destacan principalmente dos: fijar un valor máximo de
profundidad o la <a href="#ruleta-rusa">ruleta rusa</a>.</p>
<p>Analicemos la primera opción, que es la que hemos implementado
nosotros. Para ello, usaremos a la escena
<code>cornell_box_esferas</code>, pues los materiales reflectivos y
refractantes de las esferas nos servirán de ayuda para estudiar el coste
de un camino.</p>
<div id="fig:grafica_depth" class="fignos">
<figure>
<img loading="lazy" src="./img/graficas/CB_original_comparativa_depth.png"
alt="Figura 45: Coste de un frame en función de la profundidad de" />
<figcaption aria-hidden="true"><span>Figura 45:</span> Coste de un frame
en función de la profundidad de</figcaption>
</figure>
</div>
<p>En esta figura [<a href="#fig:grafica_depth">45</a>] ocurre algo
similar a [<a href="#fig:grafica_samples">42</a>]: como es evidente,
aumentar la profundidad de un rayo aumenta el coste de renderizar un
frame. Sin embargo, hay algunos matices que debemos estudiar con más
detalle.</p>
<p>El primero es que cambiar la profundidad no es tan costoso como
aumentar el número de muestras. Aún quintuplicando el valor por defecto
de 10 rebotes a 50, vemos que el motor se mantiene por debajo de los 33
milisegundos. Para una profundidad de 10, el coste de un frame es de 19
milisegundos (52 FPS), mientras que para 50 es de 28 milisegundos (35
FPS). Tomando un valor intermedio de 20, el coste se vuelve de 24
milisegundos (41 FPS).</p>
<p>Llaman la atención las variaciones en el frametime conforme aumenta
la profundidad. Para un valor de <code>depth = 10</code>, observamos que
oscila entre los 18 y los 20 milisegundos. Sin embargo, para los otros
dos valores de 20 y 50 son habituales picos de varios frames, llegando
hasta los 5 milisegundos. Además, se aprecia cierta inconsistencia. Sin
embargo, esto no resulta un problema, pues la oscilación media es de
unos 3 milisegundos aproximadamente, lo cual supone un decremento de
unos 5 frames por segundo como máximo.</p>
<p>La naturaleza de la escena afecta en gran medida al resultado. Por
mera probabilidad, cuando un rayo rebota <em>dentro</em> de la caja,
puede salir disparado hacia muchas direcciones. Destacarían en este caso
dos situaciones:</p>
<ul>
<li>El rayo continúa rebotando en la caja, impactando múltiples veces en
las esferas. Esto hace que aumente el coste del camino.</li>
<li>Se escapa de la caja, llegando hasta el infinito y cortando el
camino. En este caso, no se alcanza la profundidad máxima, y el camino
se vuelve más barato.</li>
</ul>
<p>La diferencia de rendimiento es sustancial. Pero, <strong>¿merece la
pena el coste adicional?</strong>.</p>
<p>Para responder a esta pregunta, primero debemos conocer cómo actúa
este parámetro. Empezando con un número extremadamente bajo para los
rebotes, vemos que parte de la escena ni siquiera se renderiza [<a
href="#fig:depth_1">46</a>].</p>
<div id="fig:depth_1" class="fignos">
<figure>
<img loading="lazy" src="./img/05/1%20bounce.png" style="width:70.0%"
alt="Figura 46: depth = 1" />
<figcaption aria-hidden="true"><span>Figura 46:</span>
<code>depth = 1</code></figcaption>
</figure>
</div>
<p>Aumentar el número de rebotes progresivamente permite que el camino
adquiera mayor información. Con dos rebotes, permitimos que un camino
adquiera información sobre la caja por dentro, así como un reflejo
primitivo en las esferas [<a href="#fig:depth_2">47</a>].</p>
<div id="fig:depth_2" class="fignos">
<figure>
<img loading="lazy" src="./img/05/2%20bounces.png" style="width:70.0%"
alt="Figura 47: depth = 2" />
<figcaption aria-hidden="true"><span>Figura 47:</span>
<code>depth = 2</code></figcaption>
</figure>
</div>
<p>Con 3 rebotes, la esfera izquierda refleja casi en su totalidad la
esfera, pero vemos que el reflejo de la esfera derecha <em>dentro</em>
de la izquierda está oscurecido [<a href="#fig:depth_3">48</a>].</p>
<div id="fig:depth_3" class="fignos">
<figure>
<img loading="lazy" src="./img/05/3%20bounces.png" style="width:70.0%"
alt="Figura 48: depth = 3" />
<figcaption aria-hidden="true"><span>Figura 48:</span>
<code>depth = 3</code></figcaption>
</figure>
</div>
<p>Subiéndolo a 4 rebotes [<a href="#fig:depth_4">49</a>] se arregla
mayoritariamente esto.</p>
<div id="fig:depth_4" class="fignos">
<figure>
<img loading="lazy" src="./img/05/4%20bounces.png" style="width:70.0%"
alt="Figura 49: depth = 4" />
<figcaption aria-hidden="true"><span>Figura 49:</span>
<code>depth = 4</code></figcaption>
</figure>
</div>
<p>En esta escena, aumentar más allá de 5 o 6 rebotes produce una
situación de retornos reducidos. La calidad de imagen no aumenta
prácticamente nada, pero el coste se vuelve muy elevado [<a
href="#fig:depth_20">50</a>, <a href="#fig:depth_50">51</a>].</p>
<div id="fig:depth_20" class="fignos">
<figure>
<img loading="lazy" src="./img/05/20%20bounces.png" style="width:70.0%"
alt="Figura 50: depth = 20" />
<figcaption aria-hidden="true"><span>Figura 50:</span>
<code>depth = 20</code></figcaption>
</figure>
</div>
<div id="fig:depth_50" class="fignos">
<figure>
<img loading="lazy" src="./img/05/50%20bounces.png" style="width:70.0%"
alt="Figura 51: depth = 50" />
<figcaption aria-hidden="true"><span>Figura 51:</span>
<code>depth = 50</code></figcaption>
</figure>
</div>
<h3 data-number="5.3.3" id="acumulación-temporal"><span
class="header-section-number">5.3.3</span> Acumulación temporal</h3>
<p>La acumulación temporal proporcionará una mejora enorme de la calidad
visual sin perder rendimiento. Sin embargo, tiene como contrapartida que
necesita dejar la cámara estática. Dependiendo de la situación esto
podría ser un motivo factor no negociable, pero en nuestro caso, nos
servirá.</p>
<p>Utilizando una única muestra, pero un valor de acumulación temporal
de 100 frames máximos, proporciona una imagen sin apenas ruido [<a
href="#fig:acumulacion_temp">52</a>].</p>
<div id="fig:acumulacion_temp" class="fignos">
<figure>
<img loading="lazy" src="./img/05/Acumulación%20temporal%201.png" style="width:40.0%"
alt="Figura 52: 1 muestra, acumulación temporal de 100 frames. A diferencia de 43, el resultado es impecable." />
<figcaption aria-hidden="true"><span>Figura 52:</span> 1 muestra,
acumulación temporal de 100 frames. A diferencia de <a
href="#fig:samples_1">43</a>, el resultado es impecable.</figcaption>
</figure>
</div>
<p>Subiendo los parámetros a 200 frames de acumulación temporal y 10
muestras, se obtiene una imagen muy buena[<a
href="#fig:acumulacion_temp_2">53</a>].</p>
<div id="fig:acumulacion_temp_2" class="fignos">
<figure>
<img loading="lazy" src="./img/05/Acumulación%20temporal%202.png" style="width:40.0%"
alt="Figura 53: 10 muestras, acumulación temporal de 200 frames." />
<figcaption aria-hidden="true"><span>Figura 53:</span> 10 muestras,
acumulación temporal de 200 frames.</figcaption>
</figure>
</div>
<p>El tremendo efecto de esta técnica es debido a que actúa como
normalización entre imágenes. Interpolando linealmente los resultados de
diferentes frames, con el tiempo se conseguirá una foto de lo que se
debe ver realmente, eliminando así el ruido y las luciérnagas.</p>
<h3 data-number="5.3.4" id="resolución"><span
class="header-section-number">5.3.4</span> Resolución</h3>
<p>Como se ha mencionado en la introducción, todas las escenas
anteriores se han renderizado a 720p. Podemos controlar la resolución
interna del motor desde el archivo <code>globals.hpp</code>. Veamos cómo
escala al variarla.</p>
<div id="fig:resolucion" class="fignos">
<figure>
<img loading="lazy" src="./img/graficas/CB_original_comparativa_resolucion.png"
alt="Figura 54: Tiempo de renderización de un frame dependiendo de la resolución" />
<figcaption aria-hidden="true"><span>Figura 54:</span> Tiempo de
renderización de un frame dependiendo de la resolución</figcaption>
</figure>
</div>
<p>A 720p, la escena <code>cornell_box_original</code> corre a 105 FPS
(9.6 ms/frame), mientras que a 1080p, el motor corre a 47FPS (21
ms/frame) y a 1440p, a 28 FPS (36 ms/frame). Como vemos, la resolución
tiene un gran impacto en el rendimiento. El cambio de 720p a 1080p
implica un aumento del 125% en el número de píxeles a dibujar, por lo
que es natural que el coste sea proporcional a esta cantidad. 1440p
tiene 1.7 veces más píxeles que 1080p.</p>
<p>En la práctica, ray tracing no suele utilizar resoluciones internas
tan grandes. Se aplican otro tipo de técnicas para reducir el ruido,
como veremos en el capítulo de estado del arte.</p>
<h3 data-number="5.3.5" id="importance-sampling"><span
class="header-section-number">5.3.5</span> Importance sampling</h3>
<p>El <a href="#muestreo-por-importancia">muestreo por importancia</a>
consiste en tomar una función de densidad proporcional a la función a
integrar para reducir la varianza. Se ha implementado muestreo por
importancia para los materiales difusos. En particular, se ha utilizado
dos estrategias diferentes para escoger direcciones aleatorias, que
pueden observarse en la figura [<a
href="#fig:importance_sample">55</a>].</p>
<div id="fig:importance_sample" class="fignos">
<figure>
<img loading="lazy" src="./img/05/Importance%20sample.png"
alt="Figura 55: Escena: cornell_box_original. 10 rebotes, acumulación temporal off, 10 muestras   Izquierda: hemisphere sampling. Derecha: Cosine-Weighted Hemisphere Sampling." />
<figcaption aria-hidden="true"><span>Figura 55:</span> Escena:
<code>cornell_box_original</code>. 10 rebotes, acumulación temporal off,
10 muestras <br> <strong>Izquierda</strong>: <em>hemisphere
sampling</em>. <strong>Derecha</strong>: <em>Cosine-Weighted Hemisphere
Sampling</em>.</figcaption>
</figure>
</div>
<p>Para facilitar la comparativa, desde una posición estática se ha
tomado una foto, y hemos ampliado la caja de la derecha digitalmente. De
esta forma, podemos ver cómo la caja de la derecha contiene una menor
cantidad de ruido, aún manteniendo los mismos parámetros de
renderizado.</p>
<h2 data-number="5.4" id="comparativa-con-in-one-weekend"><span
class="header-section-number">5.4</span> Comparativa con In One
Weekend</h2>
<p>Con el fin de preparar este trabajo, se ha implementado la serie de
libros de P. Shirley: <em>In One Weekend</em> <span class="citation"
data-cites="Shirley2020RTW1">(<a href="#ref-Shirley2020RTW1"
role="doc-biblioref">Shirley 2020a</a>)</span>, <em>The Next Week</em>
<span class="citation" data-cites="Shirley2020RTW2">(<a
href="#ref-Shirley2020RTW2" role="doc-biblioref">Shirley
2020b</a>)</span> y <em>The Rest of your Life</em> <span
class="citation" data-cites="Shirley2020RTW3">(<a
href="#ref-Shirley2020RTW3" role="doc-biblioref">Shirley
2020c</a>)</span>. Teniendo en cuenta que el producto final de esos
libros es un <em>offline renderer</em>, sería interesante compararlo con
nuestro motor que corre en tiempo real.</p>
<p>En esta sección enseñaremos escenas similares, mostraremos cuánto
tarda en renderizar un frame en comparación a nuestro motor, y
estudiaremos las diferencias en la calidad visual.</p>
<h3 data-number="5.4.1"
id="sobre-la-implementación-de-in-one-weekend"><span
class="header-section-number">5.4.1</span> Sobre la implementación de In
One Weekend</h3>
<p>La implementación de los tres libros se encuentra en la carpeta
<code>./RT_in_one_weekend</code> del repositorio. Aunque el proyecto
presenta una gran complejidada por sí mismo, no comentaremos nada en
este trabajo. Sin embargo, comentaremos algunos detalles necesarios para
esta comparativa:</p>
<ul>
<li>La configuración se encuentra principalmente en el archivo
<code>./RT_in_one_weekend/src/main.cpp</code>. Los parámetros que se
pueden ajustar son el número de muestras
(<code>samples_per_pixel</code>), resolución de la imagen
(<code>image_width</code>) y profundidad del rayo
(<code>max_depth</code>).</li>
<li>Las escenas se han implementado en el archivo
<code>./RT_in_one_weekend/src/scenes.hpp</code>.</li>
<li>Para mantener la comparación lo más justa posible, se ha fijado la
profundidad del rayo a 10, y la resolución a 720 x 720.</li>
</ul>
<p>Es importante tener en mente que <strong>In One Weekend no está
optimizado</strong>. No está pensado para ser rápido; sino para ser
didáctico. Es por ello que el procesamiento está <strong>limitado a un
único hilo</strong>, y el renderizado se realiza <strong>únicamente por
CPU</strong>. Las imágenes que genera este motor utilizan el formato
<code>.ppm</code> y han sido reconvertidas a <code>.png</code> para este
trabajo.</p>
<h3 data-number="5.4.2" id="tiempos-de-renderizado"><span
class="header-section-number">5.4.2</span> Tiempos de renderizado</h3>
<p>Se ha implementado una escena específica para esta comparativa,
llamada <code>cornell_box_iow</code>. Es una situación análoga a la
última caja de Cornell del tercer libro. Para sacar las imágenes de In
One Weekend se han utilizado todas las técnicas vistas en los tres
libros, por lo que se espera que la calidad gráfica sea óptima. En
nuestra versión disponemos de prácticamente todos los métodos vistos en
este trabajo, variando diferentes parámetros con el fin de ver
resultados diferentes.</p>
<p>La siguiente tabla muestra una comparativa entre el coste de
renderizar un frame en In One Weekend y en nuestro motor, usando una
profundidad de 10 rebotes y una resolución de 720p:</p>
<table>
<colgroup>
<col style="width: 20%" />
<col style="width: 26%" />
<col style="width: 33%" />
<col style="width: 18%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Número de muestras</strong></th>
<th style="text-align: left;"><strong>In One Weekend</strong>
(ms/frame)</th>
<th style="text-align: left;"><strong>Nuestra implementación</strong>
(ms/frame)</th>
<th style="text-align: left;"><strong>Veces más rápido</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">1</td>
<td style="text-align: left;"><code>1032</code></td>
<td style="text-align: left;"><code>2.6</code></td>
<td style="text-align: left;"><span
class="math inline">\(\times\)</span> <code>396.92</code></td>
</tr>
<tr class="even">
<td style="text-align: left;">5</td>
<td style="text-align: left;"><code>3934</code></td>
<td style="text-align: left;"><code>11</code></td>
<td style="text-align: left;"><span
class="math inline">\(\times\)</span> <code>357.636</code></td>
</tr>
<tr class="odd">
<td style="text-align: left;">10</td>
<td style="text-align: left;"><code>7459</code></td>
<td style="text-align: left;"><code>20.4</code></td>
<td style="text-align: left;"><span
class="math inline">\(\times\)</span> <code>365.63</code></td>
</tr>
<tr class="even">
<td style="text-align: left;">20</td>
<td style="text-align: left;"><code>14516</code></td>
<td style="text-align: left;"><code>39</code></td>
<td style="text-align: left;"><span
class="math inline">\(\times\)</span> <code>372.20</code></td>
</tr>
<tr class="odd">
<td style="text-align: left;">100</td>
<td style="text-align: left;"><code>69573</code></td>
<td style="text-align: left;"><code>~200</code></td>
<td style="text-align: left;"><span
class="math inline">\(\times\)</span> <code>347.87</code></td>
</tr>
<tr class="even">
<td style="text-align: left;">1000</td>
<td style="text-align: left;"><code>688388</code></td>
<td style="text-align: left;"><code>~2000</code></td>
<td style="text-align: left;"><span
class="math inline">\(\times\)</span> <code>344.194</code></td>
</tr>
</tbody>
</table>
<p>Como podemos observar, la diferencia es abismal. En el tiempo que
tarda In One Weekend en producir una imagen con una única muestra,
nuestro motor es capaz de generar una imagen de 500 muestras. Sin
embargo, este resultado es esperable, pues a fin de cuentas, In One
Weekend corre en la CPU con un único hilo, mientras que en nuestro motor
se utilizan todos los recursos posibles.</p>
<p>Ahora bien, debemos hacernos una pregunta: ¿cómo es la calidad
gráfica de cada uno?</p>
<p>Enfocaremos la respuesta desde dos puntos de vista diferentes: en el
primero, nos fijaremos puramente en el número de muestras; y en el
segundo, fijaremos un cierto margen de milisegundos por frame y
comprobaremos el resultado en cada motor.</p>
<h4 data-number="5.4.2.1" id="por-número-de-muestras"><span
class="header-section-number">5.4.2.1</span> Por número de muestras</h4>
<p>Comencemos la comparativa utilizando el número de muestras. Para las
primeras imágenes fijaremos la acumulación temporal a un único frame.
Explicaremos el motivo después.</p>
<p>Con una única muestra, se observa una diferencia enorme entre ambas
versiones [<a href="#fig:comparativa_1s">56</a>]. En nuestra
implementación no se observa prácticamente nada. Solo somos capaces de
distinguir la luz, un poco del reflejo de la caja izquierda y los
<em>caustics</em> causados por la luz del techo. Mientras tanto, en In
One Weekend, la imagen es ruidosa pero definida.</p>
<div id="fig:comparativa_1s" class="fignos">
<figure>
<img loading="lazy" src="./img/05/Comparativa_1s.png" style="width:85.0%"
alt="Figura 56: 1 muestra. Izquierda: In One Weekend. Derecha: nuestro motor" />
<figcaption aria-hidden="true"><span>Figura 56:</span> 1 muestra.
<strong>Izquierda</strong>: In One Weekend. <strong>Derecha</strong>:
nuestro motor</figcaption>
</figure>
</div>
<p>El <strong>motivo de esta diferencia</strong> es la <strong>forma de
muestrear la escena</strong>. In One Weekend implementa muestreo directo
de las fuentes de luz. Para conseguirlo, almacena la posición de la
lámpara del techo, y en cada intersección muestrea un punto aleatorio de
la fuente. En cambio, en nuestro motor, este tipo de fuentes no se
muestrean directamente, sino que debemos contar con el azar para que
aporten radiancia.</p>
<p>Una vez pasamos a 5 muestras [Figura <a
href="#fig:comparativa_5s">57</a>], nuestro motor consigue una imagen
más nítida, similar a la que In One Weekend genera con una muestra. En
cambio, In One Weekend consigue un resultado muy bueno, aunque con
muchas luciérnagas. Este fenómeno no ocurre en nuestra implementación
por el tipo de muestreo.</p>
<div id="fig:comparativa_5s" class="fignos">
<figure>
<img loading="lazy" src="./img/05/Comparativa_5s.png" style="width:85.0%"
alt="Figura 57: 5 muestras. Izquierda: In One Weekend. Derecha: nuestro motor" />
<figcaption aria-hidden="true"><span>Figura 57:</span> 5 muestras.
<strong>Izquierda</strong>: In One Weekend. <strong>Derecha</strong>:
nuestro motor</figcaption>
</figure>
</div>
<p>Con 20 muestras nuestra implementación aún muestra un resultado algo
ruidoso. ¿Se podría hacer algo para mejorarlo?</p>
<div id="fig:comparativa_20s" class="fignos">
<figure>
<img loading="lazy" src="./img/05/Comparativa_20s.png" style="width:85.0%"
alt="Figura 58: 20 muestras. Izquierda: In One Weekend. Derecha: nuestro motor" />
<figcaption aria-hidden="true"><span>Figura 58:</span> 20 muestras.
<strong>Izquierda</strong>: In One Weekend. <strong>Derecha</strong>:
nuestro motor</figcaption>
</figure>
</div>
<p>La respuesta es la acumulación temporal. Aunque, en esencia, la
acumulación temporal es una forma de aumentar el número de muestras con
respecto al tiempo, nuestra implementación utiliza interpolación para
mezclar los colores de los frames. De esta forma, se consigue el efecto
de normalización, lo cual elimina el ruido de la imagen con el tiempo.
De esta forma conseguimos equiparar la imagen de ambas versiones [Figura
<a href="#fig:comparativa_100s">59</a>].</p>
<p>Se puede observar cómo el tipo de ruido es diferente. En In One
Weekend, el ruido se presenta en forma de píxeles blancos, debido a las
luciérnagas generadas por el muestreo directo de la fuente de luz. En
contrapartida, en nuestra implementación el ruido es negro debido a los
rayos que no impactan en ninguna superficie tras rebotar.</p>
<div id="fig:comparativa_100s" class="fignos">
<figure>
<img loading="lazy" src="./img/05/Comparativa_100s.png" style="width:85.0%"
alt="Figura 59: 100 muestras. Izquierda: In One Weekend (100 muestras). Derecha: nuestro motor (7 muestras, 15 frames de acumulación temporal)" />
<figcaption aria-hidden="true"><span>Figura 59:</span> 100 muestras.
<strong>Izquierda</strong>: In One Weekend (100 muestras).
<strong>Derecha</strong>: nuestro motor (7 muestras, 15 frames de
acumulación temporal)</figcaption>
</figure>
</div>
<p>Por último, subiendo el número de muestras a 1000 conseguimos una
imagen muy nítida en ambas implementaciones [Figura <a
href="#fig:comparativa_1000s">60</a>]. Conseguimos apreciar una
diferencia en los bordes de la esfera de la derecha, la cual seguramente
se deba a un fallo en la implementación de la BRDF.</p>
<div id="fig:comparativa_1000s" class="fignos">
<figure>
<img loading="lazy" src="./img/05/Comparativa_1000s.png" style="width:100.0%"
alt="Figura 60: La imagen final, con 1000 muestras para cada versión. Izquierda: In One Weekend (1000 muestras). Derecha: nuestro motor (10 muestras, 100 frames de acumulación temporal)" />
<figcaption aria-hidden="true"><span>Figura 60:</span> La imagen final,
con 1000 muestras para cada versión. <strong>Izquierda</strong>: In One
Weekend (1000 muestras). <strong>Derecha</strong>: nuestro motor (10
muestras, 100 frames de acumulación temporal)</figcaption>
</figure>
</div>
<h4 data-number="5.4.2.2" id="por-presupuesto-de-tiempo"><span
class="header-section-number">5.4.2.2</span> Por presupuesto de
tiempo</h4>
<p>El presupuesto de tiempo (o <em>frame budget</em> en inglés) es la
cantidad de milisegundos que disponemos para renderizar un frame. Este
valor es importante cuando tratamos con aplicaciones en tiempo real. Por
ejemplo, si queremos que nuestro motor corra a 60 imágenes por segundo,
cada frame debe tardar un máximo de 16 milisegundos en ser generado. Una
comparativa interesante sería fijar un valor para el tiempo, y ver qué
calidad de imagen podemos conseguir con ambas implementaciones</p>
<p>Utilizando un presupuesto de 4000 milisegundos, In One Weekend es
capaz de utilizar 5 muestras para la imagen, mientras que nuestro motor
podría utilizar 10 muestras y 19 frames de acumulación temporal [Figura
<a href="#fig:comparativa_4000ms">61</a>].</p>
<p><span class="math display">\[
20.4 \frac{\text{ms}}{\text{10 muestras}} \cdot 19 \text{ frames de
temp. acum.} = 3876 \text{ ms}
\]</span></p>
<div id="fig:comparativa_4000ms" class="fignos">
<figure>
<img loading="lazy" src="./img/05/Comparativa_4000ms.png" style="width:85.0%"
alt="Figura 61: 4000 milisegundos de frame budget. Izquierda: In One Weekend. Derecha: nuestro motor" />
<figcaption aria-hidden="true"><span>Figura 61:</span> 4000 milisegundos
de <em>frame budget</em>. <strong>Izquierda</strong>: In One Weekend.
<strong>Derecha</strong>: nuestro motor</figcaption>
</figure>
</div>
<p>Como podemos observar, nuestra implementación consigue un resultado
abismalmente mejor en el mismo periodo de tiempo.</p>
<p>Usando un valor mucho más alto de 70000 milisegundos, nuestra
implementación se puede permitir utilizar 20 muestras y 1750 frames de
acumulación temporal: <span class="math display">\[
40 \frac{\text{ms}}{\text{20 muestras}} \cdot 1750 \text{ frames de
temp. acum.} = 70000 \text{ ms}
\]</span></p>
<p>Esencialmente, la imagen que genera nuestra implementación es
perfecta. No muestra ni un ápice de ruido; mientras que la de In One
Weeekend presenta un resultado poco nítido [Figura <a
href="#fig:comparativa_70000ms">62</a>].</p>
<div id="fig:comparativa_70000ms" class="fignos">
<figure>
<img loading="lazy" src="./img/05/Comparativa_70000ms.png" style="width:85.0%"
alt="Figura 62: 70000 milisegundos de frame budget. Izquierda: In One Weekend. Derecha: nuestro motor" />
<figcaption aria-hidden="true"><span>Figura 62:</span> 70000
milisegundos de <em>frame budget</em>. <strong>Izquierda</strong>: In
One Weekend. <strong>Derecha</strong>: nuestro motor</figcaption>
</figure>
</div>
<h4 data-number="5.4.2.3" id="conclusiones-de-la-comparativa"><span
class="header-section-number">5.4.2.3</span> Conclusiones de la
comparativa</h4>
<p>Analizando las imágenes proporcionadas por ambos motores, podemos
concluir que In One Weekend consigue un mejor resultado si nos basamos
en el ruido de la imagen por número de muestras. Sin embargo, la rapidez
de nuestro motor permite compensar los problemas de muestreo con un
mayor número de muestras por milisegundo, lo cual le permite superar el
resultado visual que In One Weekend consigue en un cierto periodo de
tiempo.</p>
<p>No obstante, es importante recordar que ambas implementaciones tienen
sus inconvenientes debido a la naturaleza de los respectivos trabajos,
por lo que en ambos casos se podría mejorar el rendimiento. En el caso
de In One Weekend, paralelizando el programa; y en el de nuestro motor,
haciendo más robusto el muestreo directo de fuentes de luz.</p>
<h1 data-number="6" id="conclusiones"><span
class="header-section-number">6</span> Conclusiones</h1>
<p>Al inicio de este trabajo nos propusimos crear un motor de path
tracing en tiempo real. Esta no era una tarea fácil, así que tuvimos que
empezar desde las bases de la informática gráfica.</p>
<p>Primero estudiamos qué es exactamente el <strong>algoritmo path
tracing</strong>, el cual es una forma de crear imágenes virtuales de
entornos tridimensionales. Dado que es un método físicamente realista,
necesitábamos comprender <strong>cómo se comporta la luz</strong>, con
el fin de conocer de qué color pintar un píxel de nuestra imagen.
Esencialmente, entendimos que los fotones emitidos por las fuentes de
iluminación rebotan por los diferentes objetos de un entorno,
adquiriendo partes de sus propiedades en el impacto.</p>
<p>No obstante, imitar a la realidad es una tarea titánica. Las
ecuaciones radiométricas resultan computacionalmente complejas, por lo
que no es posible crear una simulación perfecta de la luz. Por ese
motivo recurrimos a las <strong>técnicas de Monte Carlo</strong>, las
cuales utilizan sucesos aleatorios para estimar la cantidad de luz de un
punto. Estos métodos hicieron viables ciertos cálculos necesarios para
el algoritmo. Sin embargo, su naturaleza inexacta implica que existe un
error de estimación. Esto nos hizo explorar algunas formas de reducir el
ruido generado por las técnicas de integración de Monte Carlo.</p>
<p>Una vez adquirimos el fundamento teórico, <strong>diseñamos un
software</strong> que nos permitiera poner en práctica nuestros
conocimientos. Escogimos la API gráfica Vulkan y un framework de Nvidia
como base para el desarrollo. Debido a la complejidad del algoritmo
tuvimos que construir numerosas abstracciones que aceleraran el proceso
de renderizado, basándonos en tarjetas gráficas modernas. Aprender a
programar en tarjetas gráficas no es sencillo, así que tuvimos que
diseñar unos programas específicos de éstas llamados <em>shaders</em>;
específicamente, los shaders de ray tracing.</p>
<p>Al final, obtuvimos el resultado deseado: un motor de path tracing
capaz de producir imágenes de escenas virtuales.</p>
<div id="fig:pathtracing_showcase" class="fignos">
<figure>
<img loading="lazy" src="./img/06/Showcase.png"
alt="Figura 63: El motor es capaz de producir preciosas imágenes de objetos físicamente realistas que se mueven en tiempo real" />
<figcaption aria-hidden="true"><span>Figura 63:</span> El motor es capaz
de producir preciosas imágenes de objetos físicamente realistas que se
mueven en tiempo real</figcaption>
</figure>
</div>
<p>Como en cualquier otro trabajo, durante el proceso de desarrollo
tuvimos que algunas tomar decisiones técnicas que alteran la calidad del
producto. Por ello, realizamos un <strong>análisis del
rendimiento</strong> basándonos en el <em>frame time</em> –tiempo que
tarda en un frame en renderizarse–, variando los parámetros de path
tracing en el proceso. De esta forma, pudimos comprobar cuánto nos
cuesta sacar imágenes de gran nitidez.</p>
<p>Finalmente, para poner en contexto el motor, comparamos nuestra
implementación con el de otros autores. En este caso, con el path tracer
de Peter Shirley creado en Ray Tracing In One Weekend <span
class="citation" data-cites="Shirley2020RTW1">(<a
href="#ref-Shirley2020RTW1" role="doc-biblioref">Shirley
2020a</a>)</span>. Nos dimos cuenta de que, aunque nuestra versión
genera las muestras de forma más naïve, la rapidez con la que rinde
consigue compensar el ruido de la imagen, produciendo así resultados más
nítidos.</p>
<h2 data-number="6.1" id="posibles-mejoras"><span
class="header-section-number">6.1</span> Posibles mejoras</h2>
<p>Crear un software de este calibre es una tarea de una complejidad
enorme. Los motores de renderización requieren un equipo de desarrollo
de un tamaño considerable, una gran inversión y un esfuerzo constante.
Teniendo en cuenta el contexto del proyecto, en el camino ha sido
necesario tomar decisiones imperfectas. En esta sección exploraremos
algunas posibles mejoras para este trabajo.</p>
<h3 data-number="6.1.1" id="interfaces"><span
class="header-section-number">6.1.1</span> Interfaces</h3>
<p>La parte que más margen de mejora presenta es <strong>la interfaz de
las fuentes de luz</strong>. En su estado actual, únicamente es posible
utilizar dos tipos de fuentes externas a la escena: luces puntuales y
direccionales. Aunque se muestrean de forma directa. Además, solo puede
existir una única fuente de este tipo.</p>
<p>Este diseño propicia un error relacionado con la nitidez de la
imagen. Una de las ideas clave de path tracing es generar muestras de
<em>forma inteligente</em>: dado que calcular caminos es caro, tira
rayos hacia zonas que aporten mucha información. En dichas zonas deben
encontrarse, esencialmente, fuentes de luz. ¡Pero la interfaz <strong>no
conoce dónde se encuentran los materiales emisivos de la
escena</strong>! Eso implica que algunos rayos toman direcciones que no
aportan nada. Esto lo pudimos comprobar empíricamente en la <a
href="#comparativa-con-in-one-weekend">comparativa</a>.</p>
<p>Otro tipo de interfaz que necesita una mejora sustancial es la de
materiales y objetos. En el estado actual del programa, los elementos de
la escena son cargados desde un fichero <code>.obj</code>. Las
propiedades del material son determinadas en el momento del impacto de
un rayo basándonos en los parámetros del archivo de materiales asociado
<code>.mtl</code>. Esta estrategia funciona suficientemente bien en este
caso, pues el ámbito de desarrollo es bastante reducido.</p>
<p>Sin embargo, a la larga sería beneficioso <strong>reestructurar la
carga y almacenamiento de los objetos</strong>. De esta forma, atacamos
el problema anterior relacionado con los materiales emisivos,
conseguiríamos mayor granularidad en los tipos de objetos y podríamos
diseñar estrategias específicas para algunos tipos de materiales (como
separar en diferentes capas de impacto los objetos transparentes). Esto
también nos permitiría añadir nuevos tipos de materiales más fácilmente,
como pueden ser objetos dieléctricos, plásticos, mezclas entre varios
tipos, <em>subsurface scattering</em>…</p>
<p>Estas nuevas interfaces deberían ir acompañadas de una
<strong>refactorización de la clase <code>Engine</code></strong>. El
framework que escogimos, Nvidia nvpro-samples <span class="citation"
data-cites="nvpro-samples">(<a href="#ref-nvpro-samples"
role="doc-biblioref">Nvidia 2022b</a>)</span>, está destinado a ser
didáctico y no eficiente. Por tanto, el software presenta un alto
acoplamiento. Separar en varias clases más reducidas, como una clase
para rasterización y otra para ray tracing, sería esencial. Sin embargo,
Vulkan es una API muy compleja, por lo que requeriría de una gran
cantidad de trabajo.</p>
<h3 data-number="6.1.2" id="nuevas-técnicas-de-reducción-de-ruido"><span
class="header-section-number">6.1.2</span> Nuevas técnicas de reducción
de ruido</h3>
<p>En este trabajo hemos implementado algunas técnicas de reducción de
varianza del estimador de Monte Carlo para la ecuación del transporte de
luz [<a href="#eq:rendering_equation">23</a>], lo cual permite reducir
el ruido de la imagen final. Entre estas, se encuentran muestreo por
importancia, <em>next-event estimation</em> o acumulación temporal de
las muestras.</p>
<p>Sin embargo, existen numerosas técnicas que no se han desarrollado.
Entre estas, se encuentran el uso de ruido como <em>blue noise</em>,
muestreo por importancia múltiple, ruleta rusa o secuencias de baja
discrepancia (métodos de quasi-Monte Carlo). Resultaría interesante ver
cómo estas técnicas se comportan en comparación con las que sí hemos
usado.</p>
<h3 data-number="6.1.3" id="otras-mejoras-varias"><span
class="header-section-number">6.1.3</span> Otras mejoras varias</h3>
<p>Como es evidente, nos hemos dejado muchos detalles en el tintero.
Otro aspecto que me hubiera gustado explorar, pero que no ha sido
posible, es la capcacidad de hacer tests unitarios (lo que se conoce
como <em>Test Driven Development</em>). Debido a la gran velocidad de
desarrollo que ha sido requerida para sacar este proyecto adelante,
integrar test unitarios hubiera supuesto una inversión de tiempo
considerable.</p>
<p>Esto, evidentemente, es un gran problema. Hoy en día un software
profesional requiere la comprobación de que el código funciona. En este
caso, se podrían hacer algunas comprobaciones como el <em>white furnace
test</em>. Aun así, integrar test en un ray tracer resulta algo más
complicado que de costumbre teniendo en cuenta la naturaleza del
programa.</p>
<p>Sería también conveniente aprender un sistema de <em>debugging</em>
sólido. En el blog de <span class="citation"
data-cites="alain-debug">(<a href="#ref-alain-debug"
role="doc-biblioref">Galvan 2022c</a>)</span> se pueden encontrar
algunas herramientras muy útiles para este propósito.</p>
<h1 data-number="7" id="el-presente-y-futuro-de-ray-tracing"><span
class="header-section-number">7</span> El presente y futuro de Ray
Tracing</h1>
<p>Después de un esfuerzo monumental como el que supone el desarrollo de
un motor de rendering es satisfactorio ver el resultado; y más sabiendo
que tratamos con tecnología puntera. Sin embargo, en el fondo, este
trabajo no es más que un juguete. Mientras que este proyecto ha durado
escasos meses, los profesionales llevan años trabajando en transporte de
luz. Así que, ¡veamos qué se cuece en la industria!</p>
<p>En esta sección vamos a explorar ligeralmente el estado del arte.
Veremos algunas de las técnicas que están cobrando fuerza en los últimos
años, así como algunos renderers profesionales. Le pondremos especial
atención a cómo funciona el sistema de iluminación global de Unreal
Engine 5, conocido como <em>Lumen</em>.</p>
<h2 data-number="7.1" id="denoising"><span
class="header-section-number">7.1</span> <em>Denoising</em></h2>
<p>Durante el desarrollo del trabajo aprendimos que el número de
muestras no lo es todo. De hecho, es una de las partes menos
importantes. Para reducir el ruido de la imagen final, resulta muchísimo
más eficiente usar una buena estrategia de muestreo.</p>
<p>Con los avances de ray tracing en tiempo real surgió una nueva rama
del tratamiento de la computación gráfica conocida como
<em>denoising</em>. Este proceso consiste en eliminar el ruido de la
imagen final, y es una técnica que se puede aplicar a cualquier
imagen.</p>
<p>A continuación, veremos una introducción a las técnicas modernas que
se utilizan en ray tracing para acelerar la convergencia de rayos. Nos
basaremos en las notas recogidas por Alain Galvan en su blog <span
class="citation" data-cites="alain-filtering">(<a
href="#ref-alain-filtering" role="doc-biblioref">Galvan
2022e</a>)</span> <span class="citation"
data-cites="alain-denoising">(<a href="#ref-alain-denoising"
role="doc-biblioref">Galvan 2022d</a>)</span> para enumerar algunas de
las técnicas. Todas las referencias a los trabajos originales se
encuentran recogidas en dicha entrada, por lo que recomiendo leerla para
comprender mejor cada técnica.</p>
<h3 data-number="7.1.1" id="filtrado"><span
class="header-section-number">7.1.1</span> Filtrado</h3>
<p>Entre las <strong>técnicas basadas en muestreo</strong> que ya hemos
estudiado se encuentran multiple importance sampling, next-event
estimation, ruleta rusa y series de quasi-Monte Carlo.</p>
<p>Otro método muy común es el uso de <strong>ruido azul</strong>, o
<em>Blue Noise</em> <span class="citation" data-cites="blue-noise">(<a
href="#ref-blue-noise" role="doc-biblioref">Eric Heitz 2019</a>)</span>.
Este tipo de ruido proporcionan valores aleatorios por píxel para crear
patrones de ruido. Lo bueno de este tipo de patrones es que pueden ser
tratados con mayor facilidad que otros tipos de ruido. <span
class="citation" data-cites="nvidia-blue-noise">(<a
href="#ref-nvidia-blue-noise" role="doc-biblioref">Alan Wolfe
2021</a>)</span>. Esto es debido a su distribución uniforme, que produce
menos instancias de errores de alta frecuencia, lo cual hace que sean
fáciles de difuminar y acumular <span class="citation"
data-cites="alain-filtering">(<a href="#ref-alain-filtering"
role="doc-biblioref">Galvan 2022e</a>)</span>. Este tipo de ruido puede
ser creado eficientemente mediante generadores de secuencias
cuasi-aleatorias como las de Sobol.</p>
<div id="fig:blue-noise" class="fignos">
<figure>
<img loading="lazy" src="./img/07/Bluenoise.png" style="width:40.0%"
alt="Figura 64: En comparación con el ruido blanco, el ruido azul resulta más fácil de difuminar. Fuente: Galvan (2022e)" />
<figcaption aria-hidden="true"><span>Figura 64:</span> En comparación
con el ruido blanco, el ruido azul resulta más fácil de difuminar.
Fuente: <span class="citation" data-cites="alain-filtering">Galvan (<a
href="#ref-alain-filtering"
role="doc-biblioref">2022e</a>)</span></figcaption>
</figure>
</div>
<p>Del área del <strong>procesamiento de señales</strong> se han filtros
Gaussianos, Medianos y Guiados para promediar regiones de baja varianza.
En los últimos años, una nueva variante de este tipo de técnicas ha
surgido como consecuencia del <em>machine learning</em>, el cual
estudiaremos más tarde.</p>
<p>Las <strong>técnicas de acumulación</strong> permiten reutilizar
información anterior para suavizar el resultado final. Este tipo de
técnicas empezaron originalmente en 1988 con <em>irradiance caching</em>
<span class="citation" data-cites="irradiance-caching">(<a
href="#ref-irradiance-caching" role="doc-biblioref">Greg Ward
1988</a>)</span>.</p>
<p>Un método muy sencillo que hemos estudiado es la <a
href="#antialiasing-mediante-jittering-y-acumulación-temporal">acumulación
temporal</a>, pero requiere que la cámara se quede estática. En la
práctica se puede lo mismo con una escena en movimiento. Algunos de los
algoritmos más famosos son <em>Spatio-Temporal Variance Guided
Filter</em> (SVGF), <em>Spatial Denoising</em>, Adaptive SVGF (A-SVGF) y
<em>Temporally dense ray tracing</em>.</p>
<p>Un algoritmo que se ha utilizado exitosamente en los últimos años es
<em>Spatiotemporal importance Resampling</em> (ReSTIR) <span
class="citation" data-cites="restir">(<a href="#ref-restir"
role="doc-biblioref">Benedikt Bitterli 2020</a>)</span>, el cual es
capaz de procesar millones de luces dinámicas en tiempo real en alta
calidad sin necesidad de introducir estructuras de datos excesivamente
complejas. Está basado en el algoritmo <em>Sampling Importance
Resampling</em> (STIR). Este último se puede estudiar a fondo en <span
class="citation" data-cites="stir">(<a href="#ref-stir"
role="doc-biblioref">Wolfe 2022</a>)</span></p>
<div id="fig:restir" class="fignos">
<figure>
<img loading="lazy" src="./img/07/restir.png"
alt="Figura 65: ReSTIR en acción. Fuente: Benedikt Bitterli (2020)" />
<figcaption aria-hidden="true"><span>Figura 65:</span> ReSTIR en acción.
Fuente: <span class="citation" data-cites="restir">Benedikt Bitterli (<a
href="#ref-restir" role="doc-biblioref">2020</a>)</span></figcaption>
</figure>
</div>
<p>La mayor parte de estas técnicas requieren el uso de un <em>motion
buffer</em>, el cual calcula el cambio en la posición de un vértice de
un frame a otro. En la práctica suelen venir acompañados de <em>motion
vectors</em>, los cuales también son usados para técnicas como temporal
antialiasing (TAA) o temporal upscaling <span class="citation"
data-cites="temporal-supersampling">(<a
href="#ref-temporal-supersampling" role="doc-biblioref">Karis
2014</a>)</span> (TAA es una técnica de muestreo).</p>
<h3 data-number="7.1.2"
id="machine-learning-y-técnicas-de-super-sampling"><span
class="header-section-number">7.1.2</span> <em>Machine Learning</em> y
técnicas de <em>super sampling</em></h3>
<p>Para cualquier persona que haya seguido el mundo de la tecnología en
los últimos años, no debe resultarle sorprendente la afirmación de que
las técnicas basadas en machine learning están de moda. Nuestro campo de
investigación, el transporte de luz, también se ha visto beneficiado por
ellas.</p>
<p>En la actualidad el propósito de las redes neuronales suele ser
reconstruir la imagen desde una resolución considerablemente menor: en
vez de renderizar una imagen a, por ejemplo, 4K, se computa primero a
1080p o menor y luego se reescala mediante inteligencia artificial. Las
tecnologías más famosas que existen de este estilo son Nvidia Deep
Learning Super Sampling 2.0 <span class="citation" data-cites="dlss">(<a
href="#ref-dlss" role="doc-biblioref">Nvidia 2020a</a>)</span> e Intel
XeSS <span class="citation" data-cites="xess">(<a href="#ref-xess"
role="doc-biblioref">Intel 2022b</a>)</span>. AMD lanzó recientemente
FidelityFX Super Resolution 2.0 (FSR) <span class="citation"
data-cites="fsr">(<a href="#ref-fsr" role="doc-biblioref">AMD
2022</a>)</span>, pero no utiliza redes neuronales. Sin embargo, la
consideraremos en esta sección, pues sus resultados son muy
similares.</p>
<p><strong>DLSS</strong> recibe imágenes a baja resolución y <em>motion
vectors</em>, produciendo imágenes a alta resolución mediante un
autoenconder convolucional. Los resultados son espectaculares, y
consiguen un rendimiento mucho mayor que a resolución nativa sin perder
calidad de imagen. En algunos casos, la reconstrucción acaba teniendo
mayor nitidez que la imagen original, pues la red neuronal aplica
antialiasing en el proceso.</p>
<div id="fig:df-dlss" class="fignos">
<figure>
<img loading="lazy" src="./img/07/Control.jpg" style="width:80.0%"
alt="Figura 66: Control, de Remedy Games. Uno de los primeros videojuegos que integraron ray tracing en tiempo real basado en DX12. El rendimiento se duplica al utilizar DLSS. Fuente: Digital Foundry (2020a).   Izquierda: DLSS 2.0 con resolución interna a 1080p. Derecha: 4K nativo." />
<figcaption aria-hidden="true"><span>Figura 66:</span> Control, de
Remedy Games. Uno de los primeros videojuegos que integraron ray tracing
en tiempo real basado en DX12. El rendimiento se duplica al utilizar
DLSS. Fuente: <span class="citation" data-cites="df-dlss">Digital
Foundry (<a href="#ref-df-dlss" role="doc-biblioref">2020a</a>)</span>.
<br> <strong>Izquierda</strong>: DLSS 2.0 con resolución interna a
1080p. <strong>Derecha</strong>: 4K nativo.</figcaption>
</figure>
</div>
<p><strong>Intel XeSS</strong> funciona de manera similar a DLSS, aunque
todavía no se conocen los detalles. Su lanzamiento es extremadamente
reciente, por lo que se están explorando los resultados. Se puede leer
una entrevista realizada por Digital Foundry a los autores del proyecto
en <span class="citation" data-cites="df-xess">(<a href="#ref-df-xess"
role="doc-biblioref">Digital Foundry 2021c</a>)</span>.</p>
<p>Los beneficios de estas técnicas son evidentes. Tal y como
descubrimos en la comparativa, la <a href="#resolución">resolución</a>
afecta en gran medida al rendimiento. Cuantos más píxeles tenga la
imagen, mayor será el número de muestras que debamos tomar; y por lo
tanto, mayor el coste de renderizar un frame. Bajando la resolución
conseguimos una imagen con menos ruido pero poco apta para las pantallas
de hoy en día. Haciendo <em>super sampling</em> solventamos este
problema. Si la reconstrucción es de suficiente calidad, estaremos
consiguiendo rendimiento superior a la resolución nativa.</p>
<div id="fig:dlss-fsr-taa" class="fignos">
<figure>
<img loading="lazy" src="./img/07/Comparación.jpg"
alt="Figura 67: Comparativa entre las diferentes técnicas de reconstrucción a 4K. Presta atención a cómo son reconstruidas las vallas, así como el texto de los globos. Fuente: Digital Foundry (2022b).   Izquierda: Nvidia DLSS 2.3. Centro: AMD FSR 2.0. Derecha: Temporal antialiasing." />
<figcaption aria-hidden="true"><span>Figura 67:</span> Comparativa entre
las diferentes técnicas de reconstrucción a 4K. Presta atención a cómo
son reconstruidas las vallas, así como el texto de los globos. Fuente:
<span class="citation" data-cites="df-fsr">Digital Foundry (<a
href="#ref-df-fsr" role="doc-biblioref">2022b</a>)</span>. <br>
<strong>Izquierda</strong>: Nvidia DLSS 2.3. <strong>Centro</strong>:
AMD FSR 2.0. <strong>Derecha</strong>: Temporal
antialiasing.</figcaption>
</figure>
</div>
<h2 data-number="7.2" id="la-industria-del-videojuego"><span
class="header-section-number">7.2</span> La industria del
videojuego</h2>
<p>Con la llegada de ray tracing en tiempo gracias a la arquitectura
Turing en 2018, un mundo de nuevas posibilidades se abrió ante los ojos
de la industria. La más beneficiada fue, sin lugar a dudas, la de los
videojuegos debido a sus limitaciones del frame budget.</p>
<h3 data-number="7.2.1" id="productos-comerciales"><span
class="header-section-number">7.2.1</span> Productos comerciales</h3>
<p>Es mandatorio que la imagen se produzca en un margen de tiempo muy
reducido; como máximo, de 33 milisegundos. Teniendo en cuenta esta
estrechísimo margen, la mayor parte de las empresas se optó por una
<strong>solución híbrida</strong>: en vez de utilizar ray tracing
puramente, este algoritmo se reserva para ciertas partes de la pipeline
de procesamiento. Entre estas se encuentran los
<strong>reflejos</strong> y la <strong>iluminación global</strong> <span
class="citation" data-cites="khonos-best-practices">(<a
href="#ref-khonos-best-practices" role="doc-biblioref">Mihut
2020</a>)</span>.</p>
<p>En vez de recaer en técnicas antiguas como reflejos en espacio de
pantalla (<em>screen-space</em>) se utiliza una forma reducida de ray
tracing para computar estos efectos. En el proceso final de la pipeline
híbrida de rasterización y ray tracing se combina el resultado de ambas
técnicas para producir la imagen final.</p>
<div id="fig:spiderman-reflections" class="fignos">
<figure>
<img loading="lazy" src="./img/07/HybridRT.jpg"
alt="Figura 68: En vez de utilizar screen-space reflections, los cuales sufren de los problemas clásicos de rasterización debido a su naturaleza (como el fallback a los cubemaps cuando el ángulo es demasiado agudo), ray tracing resuelve estos efectos de forma magistral. Fuente: (Digital Foundry 2021a)" />
<figcaption aria-hidden="true"><span>Figura 68:</span> En vez de
utilizar <em>screen-space reflections</em>, los cuales sufren de los
problemas clásicos de rasterización debido a su naturaleza (como el
<em>fallback</em> a los cubemaps cuando el ángulo es demasiado agudo),
ray tracing resuelve estos efectos de forma magistral. Fuente: <span
class="citation" data-cites="df-spiderman">(<a href="#ref-df-spiderman"
role="doc-biblioref">Digital Foundry 2021a</a>)</span></figcaption>
</figure>
</div>
<p>Algunos productos comerciales que destacan por su uso de ray tracing
híbrido son <em>Ratchet &amp; Clank: Rift Apart</em> y
<em>Spiderman</em> [Figura <a href="#fig:spiderman-reflections">68</a>]
de Insommiac Games, <em>Cyberpunk 2077</em> de CD Project Red, Control
de Remedy <span class="citation" data-cites="df-dlss">(Figura <a
href="#ref-df-dlss" role="doc-biblioref">Digital Foundry
2020a</a>)</span>.</p>
<p>No obstante, existen algunas implementaciones de motores que utilizan
únicamente path tracing como algoritmo de renderizado. Entre estos, se
cuentran <em>Quake II RTX</em> <span class="citation"
data-cites="Q2RTX">(<a href="#ref-Q2RTX" role="doc-biblioref">Nvidia
2018</a>)</span> o <em>Minecraft RTX</em>, del cual se puede encontrar
un análisis técnico en el blog de <span class="citation"
data-cites="alain-minecraft">(<a href="#ref-alain-minecraft"
role="doc-biblioref">Galvan 2020</a>)</span>. Los resultados gráficos
son espectaculares, aunque se necesita un hardware gráfico considerable
para poder ejecutarlos.</p>
<h3 data-number="7.2.2" id="consolas-de-nueva-generación"><span
class="header-section-number">7.2.2</span> Consolas de nueva
generación</h3>
<p>Parte de la transición de la industria a ray tracing se debe a las
capacidades de las consolas de la actual generación: <strong>Playstation
5</strong> de Sony <span class="citation" data-cites="ps5">(<a
href="#ref-ps5" role="doc-biblioref">Wikipedia 2022e</a>)</span>, y
<strong>Xbox Series X</strong> y <strong>Series S</strong> de Microsoft
<span class="citation" data-cites="series-x">(<a href="#ref-series-x"
role="doc-biblioref">Wikipedia 2022g</a>)</span>. Ambas fueron lanzadas
a finales del año 2020. Las dos consolas utilizan la arquitectura de AMD
denominada RDNA 2 de AMD, la misma que se usa en sus gráficas de
escritorio de última generación. PS5 empaqueta 36 unidades de cómputo
(<em>Compute Units</em> CUs), con una potencia de 10.23 TFLOPS para
operaciones en coma flotante de 32 bits y 21.46 TFLOPS para las de 16.
Por otra parte, Series X monta 52 CUs con una potencia teórica de 12.16
TFLOPSl, mientras que Series S reduce el número de CUs a 20.</p>
<p>En verano de 2020 Mark Cerny habló sobre cómo se utilizaría las
capacidades de la arquitectura en el futuro de la consola <span
class="citation" data-cites="cerny-ps5">(<a href="#ref-cerny-ps5"
role="doc-biblioref">Cerny 2020</a>)</span>. Entre estos usos, destaca
el audio, la iluminación global, las sombras, los reflejos e incluso ray
puro.</p>
<p>Hace media década, el hecho de tener ray tracing en tiempo real en
gráficas de consumidor parecía imposible; más aún en consolas. Resulta,
por tanto, una evolución enorme en una industria que se ha convertido en
una de las más importantes del mundo.</p>
<h2 data-number="7.3" id="unreal-engine-5-y-lumen"><span
class="header-section-number">7.3</span> Unreal Engine 5 y Lumen</h2>
<p>Unreal Engine 5 <span class="citation" data-cites="UE5">(<a
href="#ref-UE5" role="doc-biblioref">Epic Games 2022a</a>)</span> es la
última generación del Motor <em>Unreal Engine</em> creado por Epic
Games. Es un software extremadamente complejo que empaqueta múltiples
funciones, como animación, físicas, renderizado, audio, y un largo
etcétera. Se usa tanto en la industria de la animación como en la de los
videojuegos.</p>
<p>Una de las novedades de esta versión es la integración de un sistema
de iluminación global, llamado <strong>Lumen</strong>. Junto a
<strong>Nanite</strong>, que es un sistema de geometría virtualizada, es
la caracaterística más importante que han presentado. Recientemente han
detallado cómo funciona Lumen <span class="citation"
data-cites="lumen">(<a href="#ref-lumen" role="doc-biblioref">Epic Games
2022b</a>)</span>, así que estudiaremos qué técnicas han utilizado.</p>
<div id="fig:lumen1" class="fignos">
<figure>
<img loading="lazy" src="./img/07/Lumen3.png" style="width:85.0%"
alt="Figura 69: Habitación interior iluminada por el sistema Lumen" />
<figcaption aria-hidden="true"><span>Figura 69:</span> Habitación
interior iluminada por el sistema Lumen</figcaption>
</figure>
</div>
<p>La <a href="#iluminación-global">iluminación global</a> es el efecto
que produce la luz cuando los fotones que la componen rebotan dentro de
una escena. En el pasado (i.e., rasterización) se ha simulado mediante
técnicas como <em>lightmap baking</em>, oclusión ambiental y similares.
Sin embargo, como ya explicamos en secciones anteriores, estas técnicas
presentan grandes limitaciones.</p>
<p>Lumen simula infinitos rebotes de la luz en una escena en tiempo
real, actualizando la iluminación automáticamente Esto funciona tanto
para interiores como exteriores, puesto que simula la luz proveniente
del cielo. Además, el sistema tiene en cuenta materiales emisivos y la
niebla volumétrica.</p>
<div id="fig:lumen2" class="fignos">
<figure>
<img loading="lazy" src="./img/07/Lumen4.jpg" style="width:85.0%"
alt="Figura 70: Una escena exterior con materiales emisivos en Unreal Engine 5. Fuente: Epic Games (2022b)" />
<figcaption aria-hidden="true"><span>Figura 70:</span> Una escena
exterior con materiales emisivos en Unreal Engine 5. Fuente: <span
class="citation" data-cites="lumen">Epic Games (<a href="#ref-lumen"
role="doc-biblioref">2022b</a>)</span></figcaption>
</figure>
</div>
<p>Como era de esperar, Lumen utiliza ray tracing para hacer el cálculo
de la iluminación. Para conseguirlo, el sistema crea una versión
simplificada de la escena en la cual resulta más fácil hacer
intersecciones de rayos con objetos.</p>
<p>Por defecto se ha optado una forma de ray tracing basada en software,
pues de esta manera se soporta más tipos de hardware. Se combinan
<em>Mesh Distance Fields</em> (mallas de polígonos que resultan
eficientes a la hora de intersecar) mezcladas en un <em>Global Distance
Field</em> en este tipo de ray tracing. No obstante, su versión de ray
tracing basado en hardware es mandatoria para conseguir algunos efectos
como reflejos especulares perfectos, pues de otra manera no se podrían
conseguir en tiempo real.</p>
<p>Una optimización clave para que este sistema funcione de forma
eficiente es el uso de <em>surface caching</em>. Trabajando en conjunto
junto a Nanite, Lumen crea un atlas de texturas de aquellas mallas
presentes alrededor del jugador de forma que se cubran las caras
visibles desde algún punto de vista. Esto simplifica el coste de la
evaluación del material.</p>
<div id="fig:lumen3" class="fignos">
<figure>
<img loading="lazy" src="./img/07/Lumen1.jpg" style="width:100.0%"
alt="Figura 71: Visualización de Mesh Distance Fields de una escena. Fuente: Epic Games (2022b)" />
<figcaption aria-hidden="true"><span>Figura 71:</span> Visualización de
<em>Mesh Distance Fields</em> de una escena. Fuente: <span
class="citation" data-cites="lumen">Epic Games (<a href="#ref-lumen"
role="doc-biblioref">2022b</a>)</span></figcaption>
</figure>
</div>
<p>Para el paso final del cálculo de la radiancia se usa un algoritmo
basado en radiance caching presentado en <span class="citation"
data-cites="siggraph2021">(<a href="#ref-siggraph2021"
role="doc-biblioref">Siggraph 2021</a>)</span>. Como no resulta viable
tirar rayos a diestro y siniestro por toda la escena, Lumen toma una
pequeña porción de muestras y combina los cálculos de radiancia con
información aportada por el material.</p>
<div id="fig:lumen4" class="fignos">
<figure>
<img loading="lazy" src="./img/07/Lumen5.png" style="width:100.0%"
alt="Figura 72: Lumen combina los cálculos de la iluminación global con la información del material para crear la imagen final. Fuente: Epic Games (2022b)" />
<figcaption aria-hidden="true"><span>Figura 72:</span> Lumen combina los
cálculos de la iluminación global con la información del material para
crear la imagen final. Fuente: <span class="citation"
data-cites="lumen">Epic Games (<a href="#ref-lumen"
role="doc-biblioref">2022b</a>)</span></figcaption>
</figure>
</div>
<p>Otro punto clave del sistema es la elección de las direcciones que se
van a trazar. Para escogerlas, se comprueba qué partes del último frame
resultaron muy brillantes. Entonces, se mandan rayos hacia esas zonas,
pues esto producirá resultados menos ruidos en este frame. Esta técnica
la hemos estudiado, y se llama muestreo por importancia (de la luz
entrante en este caso).</p>
<blockquote>
<p>Este punto enlaza con nuestra <a
href="#comparativa-con-in-one-weekend">comparativa con In One
Weekend</a>. Una de las conclusiones más interesantes que obtuvimos fue
que utilizar muestreo por importancia de las fuentes de iluminación
proporcionaba unos resultados considerablemente mejores que una
estrategia de muestreo basada en direcciones aleatorias. Aunque en este
caso no se muestrean las fuentes de luz, sí que se utilizan partes de la
escena con mucha radiancia, por lo que el fundamento es similar.</p>
</blockquote>
<div id="fig:lumen5" class="fignos">
<figure>
<img loading="lazy" src="./img/07/Lumen6.png"
alt="Figura 73: Lumen utiliza las partes más brillantes del último frame para la distribución de las nuevas direcciones de un rayo tras el impacto. Fuente: Epic Games (2022b)" />
<figcaption aria-hidden="true"><span>Figura 73:</span> Lumen utiliza las
partes más brillantes del último frame para la distribución de las
nuevas direcciones de un rayo tras el impacto. Fuente: <span
class="citation" data-cites="lumen">Epic Games (<a href="#ref-lumen"
role="doc-biblioref">2022b</a>)</span></figcaption>
</figure>
</div>
<p>Para calcular los reflejos de superficies se ha optado por promediar
la radiancia de puntos en el mismo vecindario y acumulación temporal de
frames para suavizar el resultado.</p>
<div id="fig:lumen6" class="fignos">
<figure>
<img loading="lazy" src="./img/07/Lumen2.jpg"
alt="Figura 74: Nuestro método principal de reducción de ruido fue la acumulación temporal de frames. Lumen utiliza una forma mucho más avanzada para lograr el mismo resultado con sus reflejos. Fuente: Epic Games (2022b)" />
<figcaption aria-hidden="true"><span>Figura 74:</span> Nuestro método
principal de reducción de ruido fue la acumulación temporal de frames.
Lumen utiliza una forma mucho más avanzada para lograr el mismo
resultado con sus reflejos. Fuente: <span class="citation"
data-cites="lumen">Epic Games (<a href="#ref-lumen"
role="doc-biblioref">2022b</a>)</span></figcaption>
</figure>
</div>
<p>Finalmente, es importante destacar que las imágenes no son generadas
a una gran resolución. Para conseguir una imagen digna de los tiempos
actuales, los motores modernos utilizan una resolución interna cercana a
720p o 1080p que es escalada mediante algún método de
<em>upsampling</em>. En este caso, se ha optado por <em>Temporal Super
Resolution</em>, una técnica que comenzó con Unreal Engine 4 y ha
evolucionado en esta versión. Su comportamiento es similar a DLSS o
FSR.</p>
<p>Todas estas optimizaciones son esenciales para hacer que el motor
corra en tiempo real. Aunque el rendimiento es extremadamente
dependiente del tipo de contenido que se esté visualizando y los
parámetros del sistema, Unreal Engine 5 puede correr a 60 FPS en
consolas de la generación actual.</p>
<h1 data-number="8" id="metodología-de-trabajo"><span
class="header-section-number">8</span> Metodología de trabajo</h1>
<p>Cualquier proyecto de una envergadura considerable necesita ser
planificado con antelación. En este capítulo vamos a hablar de cómo se
ha realizado este trabajo: mostraremos las herramientas usadas, los
ciclos de desarrollo, integración entre documentación y path tracer, y
otras influencias que han afectado al producto final.</p>
<h2 data-number="8.1" id="influencias"><span
class="header-section-number">8.1</span> Influencias</h2>
<p>Antes de comenzar con la labor, primero uno se debe hacer una simple
pregunta:</p>
<blockquote>
<p><em>“Y esto, ¿por qué me importa?”</em></p>
</blockquote>
<p>Dar una respuesta contundente a este tipo de cuestiones nunca es
fácil. Sin embargo, sí que puedo proporcionar motivos por los que he
querido escribir sobre ray tracing.</p>
<p>Una de las principales influencias ha sido <span class="citation"
data-cites="digital-foundry">(<a href="#ref-digital-foundry"
role="doc-biblioref">Digital Foundry 2022a</a>)</span>. Este grupo de
divulgación se dedica al estudio de las técnicas utilizadas en el mundo
de los videojuegos. El inicio de la era del ray tracing en tiempo real
les llevó a dedicar una serie de vídeos y artículos a esta tecnología, y
a las diferentes maneras en las que se ha implementado. Se puede ver un
ejemplo en <span class="citation" data-cites="digital-foundry-2020">(<a
href="#ref-digital-foundry-2020" role="doc-biblioref">Digital Foundry
2020b</a>)</span>.</p>
<iframe width="784" height="441" src="https://www.youtube.com/embed/6bqA8F6B6NQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
<p>Dado que esta área combina tanto informática, matemáticas y una
visión artística, ¿por qué no explorarlo a fondo?</p>
<p>Ahora que se ha decidido el tema, es hora de ver cómo atacarlo.</p>
<p>Soy un fiel creyente del aprendizaje mediante el juego. Páginas como
<em>Explorable Explanations</em> <span class="citation"
data-cites="explorable-explanations">(<a
href="#ref-explorable-explanations" role="doc-biblioref">Ncase
2022</a>)</span>, el blog de <span class="citation"
data-cites="ciechanowski">(<a href="#ref-ciechanowski"
role="doc-biblioref">Ciechanowski 2022</a>)</span>, el proyecto <em>The
napkin</em> <span class="citation" data-cites="napkin">(<a
href="#ref-napkin" role="doc-biblioref">Chen 2022</a>)</span> o el
divulgador 3Blue1Brown <span class="citation"
data-cites="3blue1brown">(<a href="#ref-3blue1brown"
role="doc-biblioref">Sanderson 2022</a>)</span> repercuten
inevitablemente en la manera en la que te planteas cómo comunicar textos
científicos. Por ello, aunque esto a fin de cuentas es un trabajo de fin
de grado de una carrera, quería ver hasta dónde era capaz de
llevarlo.</p>
<p>Otro punto importante es la <em>manera</em> de escribir. No me gusta
especialmente la escritura formal. Prefiero ser distendido. Por suerte,
parece que el mundo científico se está volviendo más informal <span
class="citation" data-cites="nature-2016">(<a href="#ref-nature-2016"
role="doc-biblioref">Nature 2016</a>)</span>, así que no soy el único
que aprueba esta tendencia. Además, la estructura clásica de un escrito
matemático de “teorema, lema, demostración, corolario” no me agrada
especialmente. He intentado preservar su estructura, pero sin ser tan
explícito. Estos dos puntos, en conjunto, suponen un balance entre
formalidad y distensión difícil de mantener.</p>
<h2 data-number="8.2" id="ciclos-de-desarrollo"><span
class="header-section-number">8.2</span> Ciclos de desarrollo</h2>
<p>Este proyecto está compuesto por 2 grandes pilares: documentación –lo
que estás leyendo, ya sea en PDF o en la web– y software. Podemos
distinguir varias fases en la planificación del proyecto, que quedan
resumidas en el diagrama de Grant [Figuras <a
href="#fig:grantt_1">75</a>, <a href="#fig:grantt_2">76</a>]</p>
<p>Para comenzar, durante el verano de 2021 se implementarían los tres
libros de Shirley de la “serie In One Weekend”: In One Weekend <span
class="citation" data-cites="Shirley2020RTW1">(<a
href="#ref-Shirley2020RTW1" role="doc-biblioref">Shirley
2020a</a>)</span>, The Next Week <span class="citation"
data-cites="Shirley2020RTW2">(<a href="#ref-Shirley2020RTW2"
role="doc-biblioref">Shirley 2020b</a>)</span>, y The Rest of your Life
<span class="citation" data-cites="Shirley2020RTW3">(<a
href="#ref-Shirley2020RTW3" role="doc-biblioref">Shirley
2020c</a>)</span>. De esta forma, asentaríamos las bases del proyecto,
acelerando así el aprendizaje.</p>
<div id="fig:grantt_1" class="fignos">
<figure>
<img loading="lazy" src="./img/08/Diagrama%20de%20Grantt%201.png"
alt="Figura 75: Diagrama de Grantt de la primera parte del desarrollo. Los libros de Peter Shirley servirían como introducción al trabajo" />
<figcaption aria-hidden="true"><span>Figura 75:</span> Diagrama de
Grantt de la primera parte del desarrollo. Los libros de Peter Shirley
servirían como introducción al trabajo</figcaption>
</figure>
</div>
<p>Tras esto, comenzaría a desarrollarse el motor por GPU. Cuando se
consiguiera una base sólida, se empezaría a alternar entre escritura de
la memoria y el software. Es importante documentar lo que se realiza,
pues no se puede implementar algo que no se entiende.</p>
<div id="fig:grantt_2" class="fignos">
<figure>
<img loading="lazy" src="./img/08/Diagrama%20de%20Grantt%202.png"
alt="Figura 76: Diagrama de Grantt de la segunda parte del desarrollo. Durante el segundo cuatrimestre se trabajaría tanto la memoria como el software" />
<figcaption aria-hidden="true"><span>Figura 76:</span> Diagrama de
Grantt de la segunda parte del desarrollo. Durante el segundo
cuatrimestre se trabajaría tanto la memoria como el
software</figcaption>
</figure>
</div>
<p>Sin embargo, esto era únicamente una planificación. Como todos
sabemos, en la práctica los planes no suelen salir a la perfección. ¿Ha
sido este un caso de una preparación desastrosa?</p>
<p>Por fortuna, la idea inicial <strong>se ha asemejado mucho a la
realidad</strong>. Algunas fases han sido más rápidas que otras,
mientras que otras partes han costado más trabajo. Los tipos de commits
hechos al repositorio ayudan a clasificar el tipo de trabajo, pero, en
resumidas cuentas:</p>
<ul>
<li>In One Weekend terminó de desarrollarse considerablemente antes, el
21 de agosto. A excepción de la última parte del desarrollo de la
memoria, no requiso más tiempo.</li>
<li>El diseño y la integración continua tuvieron dos fases: un sprint
inicial donde se deja asentado el 70% del trabajo, y pequeñas mejoras
incrementales en los siguientes meses. Debemos destacar que, conforme se
mejoraba el diseño de la memoria, se añadían nuevas herramientras
necesarias para su construcción. Es por ello que la integración continua
requiso de varios arreglos, tanto al dockerfile como a los Actions.</li>
<li>La implementación inicial del motor necesitó un tiempo
considerablemente menor del previsto. No obstante, el tiempo de
perfeccionamiento aumentó, y el desarrollo final concluyó cerca del 20
de mayo de 2022. Prácticamente todas las características básicas fueron
implementadas, a excepción de algunos detalles. No dio mucho tiempo a
extenderlo más allá de esto.</li>
</ul>
<p>Con respecto a la metodología de trabajo que se ha seguido es podemos
decir que es, esencialmente, <strong>una versión de Agile muy
laxa</strong> <span class="citation" data-cites="beck2001agile">(<a
href="#ref-beck2001agile" role="doc-biblioref">Beck et al.
2001</a>)</span>. Apoyándonos en las herramientas ofrecidas por <a
href="#github">Github</a>, diseñamos un sistema de requisitos mediante
issues, tanto para la memoria como para el software. Más adelante
veremos más a fondo cómo esta herramienta ha facilitado enormemente el
desarrollo.</p>
<h2 data-number="8.3" id="presupuesto"><span
class="header-section-number">8.3</span> Presupuesto</h2>
<p>A la hora de desarrollar un proyecto de software, es importante
realizar una estimación del coste del trabajo. En otro caso, se corre el
riesgo de que no se pueda llegar a cumplir el objetivo. En este caso,
<strong>el proyecto ha tenido un coste de 10314 euros</strong>, que
puede desglosarse de la siguiente manera:</p>
<ul>
<li>El <strong>coste del software</strong> en total ha sido de <strong>0
€</strong>. Las herramientas utilizadas para el desarrollo son todas de
código abierto, por lo que su uso es gratuito. Aunque se comentarán
individualmente en una sección posterior, las más importantes han sido:
<ul>
<li>Pandoc.</li>
<li>LaTeX.</li>
<li>Vulkan.</li>
<li>NVIDIA DesignWorks Samples framework.</li>
<li>Visual Studio Code.</li>
<li>Git y Github (repositorios, Actions, Issues, Projects).</li>
<li>Figma.</li>
<li>Docker.</li>
</ul></li>
<li>Con respecto al <strong>hardware</strong>, el precio total asciende
a <strong>2214 €</strong>. Se han utilizado dos máquinas principalmente:
una <em>custom build</em> de última generación para soportar el software
y un portátil para trabajar en remoto:
<ul>
<li>PC <em>custom build</em> (1414 €):
<ul>
<li><strong>CPU</strong>: Intel core i5 12600K (310 €).</li>
<li><strong>Disipador de CPU</strong>: Arctic Freezer 34 eSports DUO (50
€).</li>
<li><strong>Placa base</strong>: B660M DS3H AX DDR4 (130 €).</li>
<li><strong>GPU</strong>: KFA2 GeForce RTX 2070 Super (500 €).</li>
<li><strong>RAM</strong>: Crucial Ballistix 2x8GB DDR4 3200 MHz (82
€).</li>
<li><strong>Caja</strong>: NZXT S340 (70 €).</li>
<li><strong>NVME SSD</strong>: Kioxia Exceria Plus G2 (72 €).</li>
<li><strong>SATA3 SSD</strong>: Kingston A400 SSD 480 GB (50 €).</li>
<li><strong>HDD</strong>: WDC 500 GB (40 €).</li>
<li><strong>Fuente de alimentación</strong>: Corsair RM650x 80 PLUS Gold
(110 €).</li>
</ul></li>
<li><strong>Portátil</strong>: Xiaomi Mi Notebook Pro (8250U) (800
€).</li>
</ul></li>
<li>Atendiendo al apartado de recursos humanos, se estima un coste total
de <strong>8100 €</strong>. Se calcula a partir de que, como alumno,
recibo un sueldo de 18 €/h. Teniendo en cuenta que el número de créditos
del Trabajo de Fin de Grado son 18, y que un crédito son 25 horas de
estudio individual, se ha trabajado un total de 450 horas en el
proyecto.</li>
</ul>
<h2 data-number="8.4" id="diseño"><span
class="header-section-number">8.4</span> Diseño</h2>
<p>El diseño juega un papel fundamental en este proyecto. Todos los
elementos visuales han sido escogidos con cuidado, de forma que se
preserve la estética.</p>
<p>Se ha creado <strong>un diseño que preserve el equilibrio entre la
profesionalidad y la distensión</strong>.</p>
<h3 data-number="8.4.1" id="bases-del-diseño"><span
class="header-section-number">8.4.1</span> Bases del diseño</h3>
<p>Para la documentación en versión PDF, usamos como base la
<em>template</em> <span class="citation" data-cites="eisvogel">(<a
href="#ref-eisvogel" role="doc-biblioref">Wagler 2022</a>)</span>. Esta
es una elegante plantilla fácil de usar para LaTeX. Uno de sus puntos
fuertes es la personalización, la cual aprovecharemos para darle un
toque diferente.</p>
<p>La web utiliza como base el estilo generado por Pandoc, el
microframework de CSS <span class="citation" data-cites="bamboo">(<a
href="#ref-bamboo" role="doc-biblioref">Anh 2022</a>)</span> y unas
modificaciones personales.</p>
<h3 data-number="8.4.2" id="tipografías"><span
class="header-section-number">8.4.2</span> Tipografías</h3>
<p>Un apartado al que se le debe prestar especial énfasis es a la
combinación de tipografías. A fin de cuentas, esto es un libro; así que
escoger un tipo de letra correcto facilitará al lector comprender los
conceptos. Puede parecer trivial a priori, pero es importante.</p>
<p>Para este trabajo, se han escogido las siguientes tipografías:</p>
<ul>
<li><strong>Crimson Pro</strong>, por <span class="citation"
data-cites="crimson-pro">(<a href="#ref-crimson-pro"
role="doc-biblioref">Bailly 2022</a>)</span>: una tipografía serif
clara, legible y contemporánea. Funciona muy bien en densidades más
bajas, como 11pt. Es ideal para la versión en PDF. Además, liga
estupendamente con Source Sans Pro, utilizada para los títulos en la
plantilla Eisvogel.</li>
<li><strong>Fraunces</strong>, por <span class="citation"
data-cites="fraunces">(<a href="#ref-fraunces"
role="doc-biblioref">Undercase Type and Zimbardi 2022</a>)</span>: de
lejos, la fuente más interesante de todo este proyecto. Es una
soft-serif <em>old style</em>, pensada para títulos y similares (lo que
se conoce como <em>display</em>). Es usada en los títulos de la web. Una
de sus propiedades más curiosas es que modifica activamente los glifos
dependiendo del valor del <em>optical size axis</em>, el peso y
similares. Recomiendo echarle un ojo a su repositorio de Github, pues
incluyen detalles sobre la implementación.</li>
<li><strong>Rubik</strong>, por <span class="citation"
data-cites="rubik">(<a href="#ref-rubik" role="doc-biblioref">Hubert and
Fischer 2022</a>)</span>: La elección de Rubik es peculiar. Por sí sola,
no casa con el proyecto. Sin embargo, combinada con Fraunces,
proporcionan un punto de elegancia y familiaridad a la web. Su principal
fuerte es la facilidad para la comprensión lectora en pantallas, algo
que buscamos para la página web.</li>
<li><strong>Julia Mono</strong>, por <span class="citation"
data-cites="julia-mono">(<a href="#ref-julia-mono"
role="doc-biblioref">Cormullion 2022</a>)</span>: monoespaciada, pensada
para computación científica. Llevo usándola bastante tiempo, y combia
bien con Crimson Pro.</li>
<li><strong>Jetbrains Mono</strong>, por <span class="citation"
data-cites="jetbrains-mono">(<a href="#ref-jetbrains-mono"
role="doc-biblioref">Nurullin 2022</a>)</span>: otra tipografía
monoespaciada open source muy sólida, producida por la compañía
Jetbrains. Se utiliza en la web para los bloques de código.</li>
</ul>
<p>Todas estas fuentes permiten un uso no comercial gratuito.</p>
<div id="fig:tipografías" class="fignos">
<figure>
<img loading="lazy" src="./img/08/Tipografías.png" style="width:70.0%"
alt="Figura 77: Showcase de las tipografías utilizadas" />
<figcaption aria-hidden="true"><span>Figura 77:</span> Showcase de las
tipografías utilizadas</figcaption>
</figure>
</div>
<h3 data-number="8.4.3" id="paleta-de-colores"><span
class="header-section-number">8.4.3</span> Paleta de colores</h3>
<p>A fin de mantener consistencia, se ha creado una paleta de colores
específica.</p>
<div id="fig:paleta_de_colores" class="fignos">
<figure>
<img loading="lazy" src="./img/08/Paleta%20de%20colores.png" width="400"
alt="Figura 78: La paleta de colores del proyecto" />
<figcaption aria-hidden="true"><span>Figura 78:</span> La paleta de
colores del proyecto</figcaption>
</figure>
</div>
<p>El principal objetivo es <strong>transmitir tranquilidad</strong>,
pero a la misma vez, <strong>profesionalidad</strong>. De nuevo,
buscamos la idea de profesionalidad distendida que ya hemos repetido un
par de veces.</p>
<p>Partiendo del rojo que traía Eisvogel (lo que para nosotros sería el
rojo primario), se han creado el resto. En principio, con 5 tonalidades
diferentes nos basta. Todas ellas vienen acompañadas de sus respectivas
variaciones oscuras, muy oscuras, claras y muy claras. Corresponderían a
los <code>color-100, color-300, color-500, color-700, color-900</code>
que estamos acostumbrados en diseño web. Para la escala de grises, se
han escogido 7 colores en vez de 9. Son más que suficientes para lo que
necesitamos. Puedes encontrar las definiciones en el fichero de estilos,
ubicado en <code>./docs/headers/style.css</code>.</p>
<p>Todos los colores que puedes ver en este documento se han extraído de
la paleta. ¡La consistencia es clave!</p>
<h2 data-number="8.5" id="flujo-de-trabajo-y-herramientas"><span
class="header-section-number">8.5</span> Flujo de trabajo y
herramientas</h2>
<p>Encontrar una herramienta que se adapte a un <em>workflow</em> es
complicado. Aunque hay muchos programas maravilosos, debemos hacerlos
funcionar en conjunto. En este apartado, vamos a describir cuáles son
las que hemos usado.</p>
<p>Principalmente destacan tres de ellas: <strong>Github</strong>,
<strong>Pandoc</strong> y <strong>Figma</strong>. La primera tendrá <a
href="#github">su propia sección</a>, así que hablaremos de las
otras.</p>
<h3 data-number="8.5.1" id="pandoc"><span
class="header-section-number">8.5.1</span> Pandoc</h3>
<p><strong>Pandoc</strong> <span class="citation"
data-cites="pandoc">(<a href="#ref-pandoc"
role="doc-biblioref">MacFarlane 2021</a>)</span> es una estupendísima de
conversión de documentos. Se puede usar para convertir un tipo de
archivo a otro. En este caso, se usa para convertir una serie de
ficheros Markdown (los capítulos) a un fichero HTML (la web) y a PDF. Su
punto más fuerte es que permite escribir LaTeX de forma simplificada,
como si se tratara de <em>sugar syntax</em>. Combina la simplicidad de
Markdown y la correctitud de LaTeX.</p>
<p>Su funcionamiento en este proyecto es el siguiente: Primero, recoge
los capítulos que se encuentra en <code>docs/chapters</code>, usando una
serie de cabeceras en YAML que especifican ciertos parámetros (como
autor, fecha, título, etc.), así como scripts de Lua. Estas caceberas se
encuentran en <code>docs/headers</code>. En particular:</p>
<ol type="1">
<li><code>meta.md</code> recoge los parámetros base del trabajo.</li>
<li><code>pdf.md</code> y <code>web.md</code> contienen algunas
definiciones específicas de sus respectivos formatos. Por ejemplo, el
YAML del PDF asigna las variables disponibles de la plantilla Eisvogel;
mientras que para la web se incluyen las referencias a algunas
bibliotecas de Javascript necesarias o los estilos
(<code>docs/headers/style.css</code>, usando como base Bamboo.</li>
<li><code>math.md</code> contiene las definiciones de LaTeX.</li>
<li>Se utilizan algunos filtros específicos de Lua para simplificar la
escritura. En específico, <code>standard-code.lua</code> formatea
correctamente los bloques de código para la web.</li>
</ol>
<p>Un fichero Makefile (<code>docs/Makefile</code>) contiene varias
órdenes para generar ambos formatos. Tienen varios parámetros
adicionales de por sí, como puede ser la bibliografía
(<code>docs/chapters/bibliography.bib</code>).</p>
<h3 data-number="8.5.2" id="figma"><span
class="header-section-number">8.5.2</span> Figma</h3>
<p><strong>Figma</strong> <span class="citation" data-cites="figma">(<a
href="#ref-figma" role="doc-biblioref">Figma 2022</a>)</span> es otro de
esos programas que te hace preguntarte por qué es gratis. Es una
aplicación en la web usada para diseño gráfico. Es muy potente,
intuitiva, y genera unos resultados muy buenos en poco tiempo. Todos los
diseños de este trabajo se han hecho con esta herramienta.</p>
<div id="fig:figma" class="fignos">
<figure>
<img loading="lazy" src="./img/08/Figma.png"
alt="Figura 79: Tablón principal del proyecto de Figma, a día 15 de abril de 2022" />
<figcaption aria-hidden="true"><span>Figura 79:</span> Tablón principal
del proyecto de Figma, a día 15 de abril de 2022</figcaption>
</figure>
</div>
<p>Una de las características más útiles es poder exportar rápidamente
la imagen. Esto permite hacer cambios rápidos y registrarlos en el
repositorio fácilmente. Además, permite instalar plugins. Uno de ellos
ha resultado especialmente útil: Latex Complete <span class="citation"
data-cites="latex-complete">(<a href="#ref-latex-complete"
role="doc-biblioref">Krieger 2022</a>)</span>. Esto nos permite
incrustar código LaTeX en el documento en forma de SVG.</p>
<h3 data-number="8.5.3" id="otros-programas"><span
class="header-section-number">8.5.3</span> Otros programas</h3>
<p>Como es normal, hay muchos otros programas que han intervenido en el
desarrollo. Estos son algunos de ellos:</p>
<ul>
<li>El editor por excelencia Visual Studio Code <span class="citation"
data-cites="vscode">(<a href="#ref-vscode"
role="doc-biblioref">Microsoft 2022</a>)</span>. Ha facilitado en gran
medida el desarrollo de la aplicación y la documentación. En particular,
se ha usado una extensión denominada <em>Trigger Task on Save</em> <span
class="citation" data-cites="trigger-task">(<a href="#ref-trigger-task"
role="doc-biblioref">Gruntfuggly 2022</a>)</span> que compila la
documentación HTML automáticamente al guardar un fichero. ¡Muy útil y
rápido!</li>
<li><strong>Vectary</strong> <span class="citation"
data-cites="vectary">(<a href="#ref-vectary"
role="doc-biblioref">Vectary 2022</a>)</span> para hacer los diseños en
3D fácilmente. Permite exportar una escena rápidamente a png para
editarla en Figma.</li>
<li>Como veremos más adelante, la documentación se compila en el
repositorio usando un contenedor de <strong>Docker</strong> <span
class="citation" data-cites="docker">(<a href="#ref-docker"
role="doc-biblioref">Docker 2022</a>)</span></li>
<li>Cualquier proyecto informático debería usar <code>git</code>. Este
no es una excepción.</li>
</ul>
<h2 data-number="8.6" id="github"><span
class="header-section-number">8.6</span> Github</h2>
<p>La página <strong>Github</strong> <span class="citation"
data-cites="github">(<a href="#ref-github" role="doc-biblioref">Github
2022</a>)</span> ha alojado prácticamente todo el contenido del trabajo;
desde el programa, hasta la documentación online. El repositorio se
puede consultar en <code>github.com/Asmilex/Raytracing</code> <span
class="citation" data-cites="asmilex-raytracing-repo">(<a
href="#ref-asmilex-raytracing-repo" role="doc-biblioref">Millán
2022b</a>)</span>.</p>
<p>Se ha escogido Github en vez de sus competidores por los siguientes
motivos:</p>
<ol type="1">
<li>Llevo usándola toda la carrera. Es mi página de hosting de
repositorios favorita.</li>
<li>Los repositorios de Nvidia se encontraban en Github, por lo que
resulta más fácil sincronizarlos.</li>
<li>La documentación se puede desplegar usando Github Pages.</li>
<li>Las Github Actions son particularmente cómodas y sencillas de
usar.</li>
</ol>
<p>Entremos en detalle en algunos de los puntos anteriores:</p>
<h3 data-number="8.6.1"
id="integración-continua-con-github-actions-y-github-pages"><span
class="header-section-number">8.6.1</span> Integración continua con
Github Actions y Github Pages</h3>
<p>Cuando hablamos de <strong>integración continua</strong>, nos
referimos a ciertos programas que corren en un repositorio y se encargan
de hacer ciertas transformaciones al código, de forma que este se
prepare para su presentación final. En esencia, automatizan algunas
tareas habituales de un desarrollo de software. <span class="citation"
data-cites="jj">(<a href="#ref-jj" role="doc-biblioref">Merelo
2021</a>)</span></p>
<p>En este trabajo lo usaremos para compilar la documentación. De esta
forma, no necesitamos lidiar con “proyecto final”, “proyecto final
definitivo”, “proyecto final final v2”, etc. Simplemente, cuando
registremos un cambio en los ficheros Markdown (lo que se conoce en git
como un <code>commit</code>), y lo subamos a Github (acción de
<code>push</code>), se ejecutará un denominado <code>Action</code> que
operará sobre nuestros archivos.</p>
<p>Tendremos dos tipos de <code>Actions</code>: uno que se encarga de
compilar la web, y otro el PDF. En esencia, operan de la siguiente
manera:</p>
<ol type="1">
<li>Comprueba si se ha modificado algún fichero <code>.md</code> en el
último commit subido. Si no es el caso, para.</li>
<li>Si sí se ha modificado, accede a la carpeta del repositorio y
compila la documentación mediante <code>pandoc</code>.
<ol type="1">
<li>La web se genera en <code>docs/index.html</code>. Publica la web a
Github Pages.</li>
<li>El PDF se crea en <code>docs/TFG.pdf</code></li>
</ol></li>
<li>Commitea los archivos y termina.</li>
</ol>
<div id="fig:github_actions" class="fignos">
<figure>
<img loading="lazy" src="./img/08/Github%20Actions.png"
alt="Figura 80: La pestaña de Github Actions permite controlar con facilidad el resultado de un workflow y cuánto tarda en ejecutarse" />
<figcaption aria-hidden="true"><span>Figura 80:</span> La pestaña de
Github Actions permite controlar con facilidad el resultado de un
workflow y cuánto tarda en ejecutarse</figcaption>
</figure>
</div>
<p>El workflow de la web corre automáticamente, mientras que para
generar el PDF hace falta activación manual. Aunque no es <em>del
todo</em> correcto almacenar ficheros binarios en un repositorio de git,
no me resulta molesto personalmente. Así que, cuando considero que es el
momento oportuno, lo hago manualmente. Además, también se activa por
cada <em>release</em> que se crea.</p>
<div id="fig:actions-workflow" class="fignos">
<figure>
<img loading="lazy" src="./img/08/Actions.png"
alt="Figura 81: Diagrama con los workflows" />
<figcaption aria-hidden="true"><span>Figura 81:</span> Diagrama con los
workflows</figcaption>
</figure>
</div>
<p>Volviendo a la web, Github permite alojar páginas web para un
repositorio. Activando el parámetro correcto en las opciones del
repositorio, y configurándolo debidamente, conseguimos que lea el
archivo <code>index.html</code> generado por el Action y lo despliegue.
Esto es potentísimo: con solo editar una línea de código y subir los
cambios, conseguimos que la web se actualice al instante.</p>
<p>Para generar los archivos nos hace falta una distribución de LaTeX,
Pandoc, y todas las dependencias (como filtros). Como no encontré ningún
contenedor que sirviera mi propósito, decidí crear uno. Se encuentra en
el repositorio de Dockerhub <span class="citation"
data-cites="asmilex-raytracing-docker">(<a
href="#ref-asmilex-raytracing-docker" role="doc-biblioref">Millán
2022a</a>)</span>. Esta imagen está basada en
<code>dockershelf/latex:full</code> <span class="citation"
data-cites="dockershelf">(<a href="#ref-dockershelf"
role="doc-biblioref">Dockershelf 2022</a>)</span>. Por desgracia, es
<em>muy</em> pesada para ser un contenedor. Desafortunadamente, una
instalación de LaTeX ocupa una cantidad de espacio considerable; y para
compilar el PDF necesitamos una muy completa, por lo que debemos lidiar
con este <em>overhead</em>. Puedes encontrar el Dockerfile en
<code>./Dockerfile</code>.</p>
<h3 data-number="8.6.2" id="issues-y-github-projects"><span
class="header-section-number">8.6.2</span> Issues y Github Projects</h3>
<p>Las tareas pendientes se gestionan mediante issues. Cada vez que se
tenga un objetivo particular para el desarrollo, se anota un issue.
Cuando se genere un commit que avance dicha tarea, se etiqueta con el
número correspondiente al issue. De esta forma, todas las confirmaciones
relacionadas con la tarea quedan recogidas en la página web.</p>
<p>Esto permite una gestión muy eficiente de los principales problemas y
objetivos pendientes de la aplicación.</p>
<div id="fig:github_issues" class="fignos">
<figure>
<img loading="lazy" src="./img/08/Issues.png" style="width:80.0%"
alt="Figura 82: Pestaña de issues, día 16 de abril de 2022" />
<figcaption aria-hidden="true"><span>Figura 82:</span> Pestaña de
issues, día 16 de abril de 2022</figcaption>
</figure>
</div>
<p>Los issues se agrupan en <em>milestones</em>, o productos mínimamente
viables. Estos issues suelen estar relacionados con algún apartado
importante del desarrollo.</p>
<div id="fig:github_milestones" class="fignos">
<figure>
<img loading="lazy" src="./img/08/Milestones.png" style="width:80.0%"
alt="Figura 83: Los milestones agrupan una serie de issues relacionados con un punto clave del desarrollo" />
<figcaption aria-hidden="true"><span>Figura 83:</span> Los
<em>milestones</em> agrupan una serie de issues relacionados con un
punto clave del desarrollo</figcaption>
</figure>
</div>
<p>De esta forma, podemos ver todo lo que queda pendiente para la fecha
de entrega.</p>
<p>Para añadir mayor granularidad a la gestión de tareas y proporcionar
una vista informativa, se utiliza Github Projects. En esencia, esta
aplicación es un acompañante del repositorio estilo Asana.</p>
<div id="fig:github_projects" class="fignos">
<figure>
<img loading="lazy" src="./img/08/Projects.png" style="width:80.0%"
alt="Figura 84: Projects agrupa los issues y les asigna prioridades" />
<figcaption aria-hidden="true"><span>Figura 84:</span> Projects agrupa
los issues y les asigna prioridades</figcaption>
</figure>
</div>
<p>Una de las alternativas que se planteó al inicio fue
<strong>Linear</strong> <span class="citation" data-cites="linear">(<a
href="#ref-linear" role="doc-biblioref">Linear 2022</a>)</span>, una
aplicación de gestión de issues similar a Projects. Sin embargo, la
conveniencia de tener Projects integrado en Github supuso un punto a
favor para este gestor. De todas formas, el equipo de desarrollo se
compone de una persona, así que no hace falta complicar excesivamente el
workflow.</p>
<p>El desarrollo general de la documentación no ha seguido este sistema
de issues, pues está sujeta a cambios constantes y cada commit está
marcado con <code>[:notebook:]</code>. No obstante, ciertos problemas
relacionados con ella, como puede ser el formato de entrega, sí que
quedan recogidos como un issue.</p>
<p>Finalmente, cuando se produce un cambio significativo en la
aplicación (como puede ser una refactorización, una implementación
considerablemente más compleja…) se genera una nueva rama. Cuando se ha
cumplido el objetivo, se <em>mergea</em> la rama con la principal
<code>main</code> mediante un <em>pull request</em>. Esto proporciona un
mecanismo de robustez ante cambios complejos.</p>
<h3 data-number="8.6.3" id="estilo-de-commits"><span
class="header-section-number">8.6.3</span> Estilo de commits</h3>
<p>Una de los detalles que has podido apreciar si has entrado al
repositorio es un estilo de commit un tanto inusual. Aunque parece un
detalle de lo más insustancial, añadir emojis a los mensajes de commits
añade un toque particular al repositorio, y permite identificar
rápidamente el tipo de cambio.</p>
<p>Cada uno tiene un significado particular. En esta tabla se recogen
sus significados:</p>
<div id="fig:github_emojis" class="fignos">
<figure>
<img loading="lazy" src="./img/08/Commits.png"
alt="Figura 85: Los emojis permiten reconocer el objetivo de cada commit. Esta tabla recoge el significado de cada uno" />
<figcaption aria-hidden="true"><span>Figura 85:</span> Los emojis
permiten reconocer el objetivo de cada commit. Esta tabla recoge el
significado de cada uno</figcaption>
</figure>
</div>
<h1 data-number="9" id="glosario-de-términos"><span
class="header-section-number">9</span> Glosario de términos</h1>
<blockquote>
<p><em>It’s dangerous to go alone, take this.</em></p>
</blockquote>
<p>Tener en mente <em>todos</em> los conceptos y sus expresiones que
aparecen en un libro como este es prácticamente imposible. Tampoco hay
necesidad de ello, realmente. ¡Vaya desperdicio de cabeza! Por eso, aquí
tienes recopilada una lista con todos los elementos importantes y un
enlace a sus secciones correspondientes.</p>
<h2 data-number="9.1" id="notación"><span
class="header-section-number">9.1</span> Notación</h2>
<table>
<colgroup>
<col style="width: 15%" />
<col style="width: 84%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Concepto</strong></th>
<th style="text-align: left;"><strong>Notación</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Escalares</strong></td>
<td style="text-align: left;">Letras minúsculas. Generalmente, <span
class="math inline">\(a, b, c, k, \dots\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Puntos</strong></td>
<td style="text-align: left;">Letras minúsculas en negrita.
Generalmente, <span class="math inline">\(\mathbf{p}, \mathbf{q},
\dots\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Vectores</strong></td>
<td style="text-align: left;">Letras minúsculas en negrita: <span
class="math inline">\(\mathbf{v, w, n}, \dots\)</span>. Si están
normalizados, se les pone gorrito (por ejemplo, <span
class="math inline">\(\hat{\mathbf{n}}\)</span>)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Matrices</strong></td>
<td style="text-align: left;">Letras mayúsculas en negrita: <span
class="math inline">\(\mathbf{M}\)</span>. Por columnas.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Producto escalar</strong></td>
<td style="text-align: left;"><span class="math inline">\(\mathbf{v}
\cdot \mathbf{w}\)</span>. Si es el producto escalar de un vector
consigo mismo, a veces pondremos <span
class="math inline">\(\mathbf{v}^2\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Producto vectorial</strong></td>
<td style="text-align: left;"><span class="math inline">\(\mathbf{v}
\times \mathbf{w}\)</span></td>
</tr>
</tbody>
</table>
<h2 data-number="9.2" id="bases-de-ray-tracing"><span
class="header-section-number">9.2</span> <a href="#las-bases">Bases de
Ray Tracing</a></h2>
<table>
<colgroup>
<col style="width: 19%" />
<col style="width: 80%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Concepto</strong></th>
<th style="text-align: left;"><strong>Expresiones y
comentarios</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><a
href="#eq:rayo"><strong>Rayo</strong></a></td>
<td style="text-align: left;"><span class="math inline">\(P(t) = o +
td\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><a href="#eq:rayo"><strong>Ray
casting</strong></a></td>
<td style="text-align: left;">Disparar un rayo hacia la escena virtual,
de forma que impacta con alguna superficie.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><a
href="#eq:normal_superficie"><strong>Normal en un
punto</strong></a></td>
<td style="text-align: left;">Vector perpendicular a cualquier punto de
la superficie: <span class="math inline">\(\mathbf{n} = \nabla F(P) =
\left( \frac{\partial F(P)}{\partial x}, \frac{\partial F(P)}{\partial
y}, \frac{\partial F(P)}{\partial z}\right )\)</span></td>
</tr>
</tbody>
</table>
<h2 data-number="9.3" id="transporte-de-luz-1"><span
class="header-section-number">9.3</span> <a
href="#transporte-de-luz">Transporte de luz</a></h2>
<table>
<colgroup>
<col style="width: 19%" />
<col style="width: 80%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Concepto</strong></th>
<th style="text-align: left;"><strong>Expresiones y
comentarios</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><a href="#unidades-básicas"><strong>Carga
de energía</strong></a></td>
<td style="text-align: left;">Energía de un fotón. <br> <span
class="math inline">\(Q = hf = \frac{hc}{\lambda}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><a href="#potencia"><strong>Flujo radiante
o potencia</strong></a></td>
<td style="text-align: left;">Tasa de producción de energía de una
fuente de luz. <br> <span class="math inline">\(\begin{aligned}&amp;
\Phi = \frac{dQ}{dt} \\ &amp; \Phi =
\int_{A}\int_{H^2(\mathbf{n})}{L_o(p, \omega) d\omega^\bot
dA}\end{aligned}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><a href="#irradiancia"><strong>Irradiancia
o radiancia emitida</strong></a></td>
<td style="text-align: left;">Flujo radiente que recibe una superficie.
<br> <span class="math inline">\(\begin{aligned} &amp; E =
\frac{\Phi}{A} \\ &amp; E(p) = \frac{d\Phi}{dA} \\ &amp; E(p,
\mathbf{n}) = \int_{\Omega}{L_i(p, \omega) \left\lvert cos\theta
\right\rvert d\omega} \\ &amp; E(p, \mathbf{n}) =
\int_{0}^{2\pi}\int_{0}^{\pi/2}{L_i(p, \theta, \phi) \cos\theta\
\sin\theta\ d\theta\ d\phi} \\ &amp; E(p, \mathbf{n}) =
\int_{A}{L\cos\theta\ \frac{\cos\theta_o}{r^2}dA}
\end{aligned}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><a href="#ángulos-sólidos"><strong>Ángulo
sólido</strong></a>, derivada [<a href="#eq:d_omega">11</a>] [<a
href="#eq:diferencial_angulo_solido">16</a>]</td>
<td style="text-align: left;">Medida del campo de visión de un objeto
desde un cierto punto. <br> <span class="math inline">\(\sigma =
\frac{A}{r^2}\)</span> <br> <span class="math inline">\(d\omega =
\sin\theta\ d\theta\ d\phi\)</span> <br> <span
class="math inline">\(d\omega = \frac{dA\cos\theta}{r^2}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><a
href="#eq:coordenadas_esféricas"><strong>Coordenadas
esféricas</strong></a></td>
<td style="text-align: left;">Sistema de coordenadas <span
class="math inline">\((r, \theta, \phi)\)</span> usado habitualmente
para describir posiciones en una esfera. <br> <span
class="math inline">\(x = \sin\theta\cos\phi,\ y = \sin\theta\sin\phi,\
z = \cos\theta\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><a
href="#intensidad_radiante"><strong>Intensidad
radiante</strong></a></td>
<td style="text-align: left;">Densidad angular de flujo radiante. <br>
<span class="math inline">\(I = \frac{d\Phi}{d\omega}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><a
href="#radiancia"><strong>Radiancia</strong></a></td>
<td style="text-align: left;">Flujo radiante emitido en un cierto cono
de direcciones. <br> <span class="math inline">\(\begin{aligned} &amp;
L(p, \omega) = \frac{dE_\omega(p)}{d\omega} \\ &amp; L(p, \omega) =
\frac{d^2\Phi(p, \omega)}{d\omega\ dA^\bot} = \frac{d^2\Phi(p,
\omega)}{d\omega\ dA\ \cos\theta} \\ &amp; L^+(p, \omega) = \lim_{t \to
0^+}{L(p + t\mathbf{n}_p, \omega)} \\ &amp; L^-(p, \omega) = \lim_{t \to
0^-}{L(p + t\mathbf{n}_p, \omega)} \end{aligned}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><a href="#radiancia"><strong>Radiancia
incidente</strong></a></td>
<td style="text-align: left;">Radiancia que recibe una superficie desde
la dirección <span class="math inline">\(\omega\)</span>. <br> <span
class="math inline">\(\begin{aligned} L_i(p, \omega) = \begin{cases}
L^+(p, -\omega) &amp; \text{si } \omega \cdot \mathbf{n}_p &gt; 0 \\
L^-(p, -\omega) &amp; \text{si } \omega \cdot \mathbf{n}_p &lt; 0
\end{cases}\end{aligned}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><a href="#radiancia"><strong>Radiancia
reflejada o de salida</strong></a></td>
<td style="text-align: left;">Radiancia que emite una superficie hacia
una dirección <span class="math inline">\(\omega\)</span>. <br> <span
class="math inline">\(\begin{aligned} &amp; L_o(p, \omega) =
\begin{cases} L^+(p, \omega) &amp; \text{si } \omega \cdot \mathbf{n}_p
&gt; 0 \\ L^-(p, \omega) &amp; \text{si } \omega \cdot \mathbf{n}_p &lt;
0 \end{cases} \end{aligned}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><a
href="#la-función-de-distribución-de-reflectancia-bidireccional-brdf"><strong>BRDF</strong></a></td>
<td style="text-align: left;">Cómo se dispersa la luz cuando impacta una
superficie dependiendo de la dirección. <br> <span
class="math inline">\(\begin{aligned}&amp; f_r(p, \omega_o \leftarrow
\omega_i) = \frac{dL_o(p, \omega_o)}{dE(p, \omega_i)} = \frac{dL_o(p,
\omega_o)}{L_i(p, \omega_i) \cos\theta_i\
d\omega_i}\end{aligned}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><a
href="#la-función-de-distribución-de-transmitancia-bidireccional-btdf"><strong>BTDF</strong></a></td>
<td style="text-align: left;">Cómo se transmite la luz cuando impacta
una superficie dependiendo de la dirección. <br> <span
class="math inline">\(f_t(p, \omega_o \leftarrow \omega_i)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><a
href="#la-función-de-distribución-de-dispersión-bidireccional-bsdf"><strong>BSDF</strong></a></td>
<td style="text-align: left;">Combinación de la BRDF y la BTDF. <br>
<span class="math inline">\(f(p, \omega_o \leftarrow
\omega_i)\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><a
href="#la-función-de-distribución-de-dispersión-bidireccional-bsdf"><strong>Ecuación
de dispersión</strong></a></td>
<td style="text-align: left;">Ecuación para la radiancia emitida hacia
una dirección en un cierto punto. <br> <span
class="math inline">\(\begin{aligned}L_o(p, \omega_o) =
\int_{\mathbb{S}^2}{f(p, \omega_o \leftarrow \omega_i)L_i(p,
\omega_i)\left\lvert \cos\theta_i \right\rvert
d\omega_i}\end{aligned}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Hemisferio de direcciones
alrededor de un vector</strong></td>
<td style="text-align: left;"><span
class="math inline">\(H^2(\mathbf{n})\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><a
href="#reflectancia-hemisférica"><strong>Albedo o reflectancia
hemisférica</strong></a></td>
<td style="text-align: left;">Proporción de luz reflejada por una
superficie. <br> <span class="math inline">\(\rho_{hd}(\omega_o) =
\int_{H^2(n)}{f_r(p, \omega_o \leftarrow \omega_i) \left\lvert
\cos\theta_i \right\rvert\ d\omega_i}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><a
href="#tipos-de-dispersión"><strong>Tipos de
materiales</strong></a></td>
<td style="text-align: left;">Difusos, especulares brillantes,
especulares perfectos, retrorreflectores</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><a
href="#reflexión"><strong>Reflexión</strong></a></td>
<td style="text-align: left;">Efecto de la luz cuando impacta un espejo.
<br> <span class="math inline">\(\mathbf{r} = \mathbf{i} - 2 (\mathbf{i}
\cdot \mathbf{n}) \mathbf{n}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><a href="#ley-de-snell"><strong>Ley de
Snell</strong></a></td>
<td style="text-align: left;">Relación entre los ángulos de un rayo al
cambiar de medio. <br> <span class="math inline">\(\eta_1 \sin\theta_1 =
\eta_2 \sin\theta_2\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><a
href="#la-aproximación-de-schlick"><strong>Aproximación de
Schlick</strong></a></td>
<td style="text-align: left;">Simplificación de las ecuaciones de
Fresnel.<br> <span class="math inline">\(R(\theta_1) = R_0 + (1 - R_0)(1
- \cos\theta_1)^5\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><a
href="#la-rendering-equation"><strong>Rendering equation o ecuación del
transporte de luz</strong></a></td>
<td style="text-align: left;">La ecuación más importante de la
informática gráfica. Describe analíticamente la cantidad de luz de un
punto en función de su entorno. <br> <span class="math inline">\(L_o(p,
\omega_o) = L_e(p, \omega_o) + \int_{H^2(\mathbf{n})}{f(p, \omega_o
\leftarrow \omega_i) L_i(p, \omega_i) \cos\theta_i\
d\omega_i}\)</span></td>
</tr>
</tbody>
</table>
<h2 data-number="9.4" id="métodos-de-monte-carlo-1"><span
class="header-section-number">9.4</span> <a
href="#métodos-de-monte-carlo">Métodos de Monte Carlo</a></h2>
<table>
<colgroup>
<col style="width: 14%" />
<col style="width: 85%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Concepto</strong></th>
<th style="text-align: left;"><strong>Expresiones</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><a
href="#repaso-de-probabilidad"><strong>Variable
aleatoria</strong></a></td>
<td style="text-align: left;">Regla que asigna un valor numérico a cada
caso de un proceso de azar. <br> Usualmente, <span
class="math inline">\(X, Y, \xi\)</span>…</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Función masa de
probabilidad</strong> (discretas) o <strong>de densidad</strong>
(continuas)</td>
<td style="text-align: left;">Función que permite conocer la
probabilidad de un suceso. <br> Discretas: <span
class="math inline">\(P\left[ X \in \text{Conjunto} \right]\)</span>
<br> Continuas: <span class="math inline">\(f_X, p_X\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Función de
distribución</strong></td>
<td style="text-align: left;">Probabilidad de que una variable aleatoria
se quede por debajo de un cierto valor. <br> <span
class="math inline">\(F_X(x) = P\left[ X \le x \right]\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><a
href="#esperanza-y-varianza-de-una-variable-aleatoria"><strong>Esperanza</strong></a></td>
<td style="text-align: left;">Generalización de la media ponderada para
una cierta variable aleatoria. <br> <span class="math inline">\(E\left[
X \right]\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><a
href="#esperanza-y-varianza-de-una-variable-aleatoria"><strong>Varianza</strong></a></td>
<td style="text-align: left;">Medida de la dispersión de la distribución
de una variable aleatoria. <br> <span class="math inline">\(Var\left[ X
\right]\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><a
href="#estimadores"><strong>Estimador</strong></a></td>
<td style="text-align: left;">Función de la muestra de una variable
aleatoria que toma valores en el conjunto de parámetros de una
distribución. <br> <span class="math inline">\(T(X_1, \dots, X_n) =
\theta \in \Theta\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><a
href="#monte-carlo-básico"><strong>Estimador de Monte
Carlo</strong></a></td>
<td style="text-align: left;">Estimador cuya esperanza es la media de
una variable aleatoria. <br> <span class="math inline">\(\hat\mu_N =
\frac{1}{N} \sum_{i = 1}^{N}{Y_i}\)</span> <span
class="math inline">\(\Rightarrow E\left[ \hat\mu_N \right] =
\mu\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><a
href="#integración-de-monte-carlo"><strong>Estimador de Monte Carlo
(para una integral)</strong></a></td>
<td style="text-align: left;">Estimador de Monte Carlo que permite
conocer el valor de una integral <span class="math inline">\(\int_S f(x)
p_X(x) dx\)</span> a partir de muestras de una variable aleatoria <span
class="math inline">\(X \sim p_X\)</span> <br> Simple: <span
class="math inline">\(\hat{I}_N = \frac{1}{N} \sum_{i =
1}^{N}{f(X_i)}\)</span> <br> Por importancia: <span
class="math inline">\(\tilde{I}_N = \frac{1}{N}
\sum_{i=1}^N{\frac{f(X_i)p_X(X_i)}{q_X(X_i)}}, \quad X_i \sim
q_X\)</span> <br> Transporte de luz: <span
class="math inline">\(\tilde{I}_N = \frac{1}{N}
\sum_{i=1}^N{\frac{f(X_i)}{p_X(X_i)}}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Orden de convergencia del
estimador de Monte Carlo</strong></td>
<td style="text-align: left;"><span
class="math inline">\(\mathcal{O}(N^{-1/2})\)</span></td>
</tr>
</tbody>
</table>
<h2 data-number="9.5" id="construyamos-un-path-tracer-1"><span
class="header-section-number">9.5</span> <a
href="#construyamos-un-path-tracer">Construyamos un path tracer</a></h2>
<table>
<colgroup>
<col style="width: 41%" />
<col style="width: 58%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Concepto</strong></th>
<th style="text-align: left;"><strong>Expresiones</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Rasterización</strong></td>
<td style="text-align: left;">Técnica de producción de imágenes por
ordenador en la que la geometría virtual es proyectada en un plano
2D.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Ray tracing</strong></td>
<td style="text-align: left;">Algoritmo basado en la generación de rayos
de luz que permite generar imágenes más realistas que en
rasterización</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Path tracing</strong></td>
<td style="text-align: left;">Algoritmo basado en ray tracing en el que
se simulan múltiples caminos de luz.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><a
href="#estimando-la-rendering-equation-con-monte-carlo"><strong>Ecuación
del transporte de luz estimada por Monte Carlo</strong></a></td>
<td style="text-align: left;"><span class="math inline">\(\frac{1}{N}
\sum_{j = 1}^{N}{\frac{f(p, \omega_o \leftarrow \omega_j) L_i(p,
\omega_j) \cos\theta_j}{p(\omega_j)}}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Rendering engine o
motor</strong></td>
<td style="text-align: left;">Software diseñado para producir imágenes
de entornos virtuales utilizando hardware específico.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>GPU</strong> (Graphics Processing
Unit)</td>
<td style="text-align: left;">Tarjeta gráfica; dispositivo especializado
en la aceleración de cálculos necesarios en la producción de
imágenes.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><a
href="#frameworks-y-API-de-ray-tracing-en-tiempo-real"><strong>Graphic
API</strong></a></td>
<td style="text-align: left;">Interaz de programación de aplicaciones
para la creación de imágenes por ordenador.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><a
href="#frameworks-y-API-de-ray-tracing-en-tiempo-real"><strong>Vulkan</strong></a></td>
<td style="text-align: left;">API gráfica de código abierto desarrollada
por Khonos.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><a
href="#estructuras-de-aceleración"><strong>Estructura de
aceleración</strong></a></td>
<td style="text-align: left;">Forma de representar la geometría de una
escena de manera que se optimicen las intersecciones de objetos y
rayos</td>
</tr>
<tr class="even">
<td style="text-align: left;"><a
href="#estructuras-de-aceleración"><strong>Bounding Volume Hierarchy
(BVH)</strong></a></td>
<td style="text-align: left;">Tipo de estructura de aceleración basada
en cajas delimitantes (<em>bounding boxes</em>)</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><a
href="#estructuras-de-aceleración"><strong>Axis-Aligned Bounding Box
(AABB)</strong></a></td>
<td style="text-align: left;">Tipo de BVH alineada con los ejes
virtuales.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><a
href="#bottom-level-acceleration-structure-blas"><strong>Bottom-Level
Acceleration Structure (BLAS)</strong></a></td>
<td style="text-align: left;">Estructura utilizada por Vulkan para
almacenar la geometría de un objeto individual.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><a
href="#top-level-acceleration-structure-tlas"><strong>Top-Level
Acceleration Structure (TLAS)</strong></a></td>
<td style="text-align: left;">Estructura utilizada por Vulkan para
guardar la información de las instancias de un objeto.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Shader</strong></td>
<td style="text-align: left;">Programa que se ejecuta en una tarjeta
gráfica.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><a href="#tipos-de-shaders"><strong>Ray
generation shader</strong></a></td>
<td style="text-align: left;">Shader que se encarga de la generación
inicial de rayos.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><a
href="#tipos-de-shaders"><strong>Closest hit shader</strong></a></td>
<td style="text-align: left;">Shader que se ejecuta para el primer
impacto de un rayo con una geometría válida.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><a
href="#tipos-de-shaders"><strong>Any-hit shader</strong></a></td>
<td style="text-align: left;">Similar al closest hit. Shader que corre
en cada intersección.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><a href="#tipos-de-shaders"><strong>Miss
shader</strong></a></td>
<td style="text-align: left;">Shader que se ejecuta cuando no se impacta
ninguna geometría.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><a
href="#tipos-de-shaders"><strong>Intersection shader</strong></a></td>
<td style="text-align: left;">Shader que se encarga de computar
intersecciones con objetos.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><a
href="#la-shader-binding-table"><strong>Shader Binding Table
(SBT)</strong></a></td>
<td style="text-align: left;">Estructura específica de ray tracing que
permite guardar referencias a shaders y sus parámetros para ser
ejecutadas cuando se impacte una geometría específica.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><a
href="#traspaso-de-información-entre-shaders"><strong>Push
constant</strong></a></td>
<td style="text-align: left;">Estructura que almacena elementos
constantes comunes para todos los tipos de shaders.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><a
href="#traspaso-de-información-entre-shaders"><strong>Payload</strong></a></td>
<td style="text-align: left;">Estructura que permite traspasar
información variable entre diferentes shaders.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><a
href="#materiales-y-objetos"><strong>Wavefront</strong></a></td>
<td style="text-align: left;">Formato de archivo que permite abstraer
objetos virtuales y sus tipos de materiales.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><a
href="#antialiasing-mediante-jittering-y-acumulación-temporal"><strong>Antialiasing</strong></a></td>
<td style="text-align: left;">Técnica para suavizar los dientes de
sierra generados por líneas diagonales.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><a
href="#corrección-de-gamma"><strong>Corrección de
gamma</strong></a></td>
<td style="text-align: left;">Operación utilizada en fotografía para
corregir la luminancia con el fin de compensar la percepción no lineal
del brillo por parte de los humanos.</td>
</tr>
</tbody>
</table>
<h2 data-number="9.6" id="análisis-de-rendimiento-1"><span
class="header-section-number">9.6</span> <a
href="#análisis-de-rendimiento">Análisis de rendimiento</a></h2>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 66%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Concepto</strong></th>
<th style="text-align: left;"><strong>Expresiones</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><a
href="#requisitos-de-ray-tracing-en-tiempo-real"><strong>Frame
time</strong></a></td>
<td style="text-align: left;">Tiempo que tarda un motor en renderizar
una imagen. Medido en milisegundos.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><a
href="#requisitos-de-ray-tracing-en-tiempo-real"><strong>Frame
rate</strong></a></td>
<td style="text-align: left;">Tasa de imágenes por segundo (FPS,
<em>frames per second</em>) que es capaz de producir un motor. Es la
inversa del frame time.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><a
href="#iluminación-global"><strong>Iluminación global</strong></a></td>
<td style="text-align: left;">Fenómeno físico producido por el rebote
constante de fotones en un entorno. Es difícil simular sin path
tracing.</td>
</tr>
</tbody>
</table>
<h1 class="unnumbered" id="bibliografía">Bibliografía</h1>
<div id="refs" class="references csl-bib-body hanging-indent"
role="doc-bibliography">
<div id="ref-Marrs2021" class="csl-entry" role="doc-biblioentry">
Adam Marrs, Peter Shirley, and I. Wald, eds. 2021. <em>Ray Tracing Gems
II</em>. Apress. <a
href="https://doi.org/10.1007/978-1-4842-7185-8">https://doi.org/10.1007/978-1-4842-7185-8</a>.
</div>
<div id="ref-nvidia-blue-noise" class="csl-entry"
role="doc-biblioentry">
Alan Wolfe, T. A.-M., Nathan Morrical. 2021. <span>“Rendering in real
time with spatiotemporal blue noise textures, part 1.”</span> 2021.
Accessed May 30, 2022. <a
href="https://developer.nvidia.com/blog/rendering-in-real-time-with-spatiotemporal-blue-noise-textures-part-1/">https://developer.nvidia.com/blog/rendering-in-real-time-with-spatiotemporal-blue-noise-textures-part-1/</a>.
</div>
<div id="ref-fsr" class="csl-entry" role="doc-biblioentry">
AMD. 2022. <span>“AMD FidelityFX™ super resolution.”</span> 2022.
Accessed May 30, 2022. <a
href="https://www.amd.com/en/technologies/fidelityfx-super-resolution">https://www.amd.com/en/technologies/fidelityfx-super-resolution</a>.
</div>
<div id="ref-berkeley-mc-lecture" class="csl-entry"
role="doc-biblioentry">
Anderson, E. C. 1999. <span>“Monte carlo methods and importance
sampling.”</span> 1999. Accessed May 9, 2022. <a
href="https://ib.berkeley.edu/labs/slatkin/eriq/classes/guest_lect/mc_lecture_notes.pdf">https://ib.berkeley.edu/labs/slatkin/eriq/classes/guest_lect/mc_lecture_notes.pdf</a>.
</div>
<div id="ref-bamboo" class="csl-entry" role="doc-biblioentry">
Anh, T. N. T. 2022. <span>“Bamboo CSS.”</span> 2022. Accessed May 7,
2022. <a
href="https://github.com/rilwis/bamboo">https://github.com/rilwis/bamboo</a>.
</div>
<div id="ref-arneback-2019" class="csl-entry" role="doc-biblioentry">
Arnebäck. 2019. <span>“An explanation of the rendering equation.”</span>
January 10, 2019. Accessed April 9, 2022. <a
href="https://www.youtube.com/watch?v=eo_MTI-d28s">https://www.youtube.com/watch?v=eo_MTI-d28s</a>.
</div>
<div id="ref-crimson-pro" class="csl-entry" role="doc-biblioentry">
Bailly, J. L. 2022. <span>“Crimson pro typography.”</span> 2022.
Accessed May 7, 2022. <a
href="https://fonts.google.com/specimen/Crimson+Pro">https://fonts.google.com/specimen/Crimson+Pro</a>.
</div>
<div id="ref-beck2001agile" class="csl-entry" role="doc-biblioentry">
Beck, K., M. Beedle, A. van Bennekum, A. Cockburn, W. Cunningham, M.
Fowler, J. Grenning, et al. 2001. <span>“Manifesto for Agile Software
Development.”</span> Accessed April 20, 2022. <a
href="http://www.agilemanifesto.org/">http://www.agilemanifesto.org/</a>.
</div>
<div id="ref-restir" class="csl-entry" role="doc-biblioentry">
Benedikt Bitterli, M. P., Chris Wyman. 2020. <span>“Spatiotemporal
reservoir resampling for real-time ray tracing with dynamic direct
lighting.”</span> July 19, 2020. Accessed May 30, 2022. <a
href="https://research.nvidia.com/publication/2020-07_spatiotemporal-reservoir-resampling-real-time-ray-tracing-dynamic-direct">https://research.nvidia.com/publication/2020-07_spatiotemporal-reservoir-resampling-real-time-ray-tracing-dynamic-direct</a>.
</div>
<div id="ref-berkeley-cs184" class="csl-entry" role="doc-biblioentry">
Berkeley cs184. 2022. <span>“Computer graphics and imaging.”</span>
2022. Accessed March 20, 2022. <a
href="https://cs184.eecs.berkeley.edu/sp22">https://cs184.eecs.berkeley.edu/sp22</a>.
</div>
<div id="ref-cerny-ps5" class="csl-entry" role="doc-biblioentry">
Cerny, M. 2020. <span>“The road to PS5.”</span> 2020. Accessed May 30,
2022. <a
href="https://www.youtube.com/watch?v=ph8LyNIT9sg">https://www.youtube.com/watch?v=ph8LyNIT9sg</a>.
</div>
<div id="ref-napkin" class="csl-entry" role="doc-biblioentry">
Chen, E. 2022. <span>“The napkin project.”</span> 2022. Accessed May 7,
2022. <a
href="https://web.evanchen.cc/napkin.html">https://web.evanchen.cc/napkin.html</a>.
</div>
<div id="ref-ciechanowski" class="csl-entry" role="doc-biblioentry">
Ciechanowski, B. 2022. <span>“Lights and shadows.”</span> 2022. Accessed
May 7, 2022. <a
href="https://ciechanow.ski/lights-and-shadows/">https://ciechanow.ski/lights-and-shadows/</a>.
</div>
<div id="ref-julia-mono" class="csl-entry" role="doc-biblioentry">
Cormullion. 2022. <span>“JuliaMono - a monospaced font for scientific
and technical computing.”</span> 2022. Accessed May 7, 2022. <a
href="https://juliamono.netlify.app">https://juliamono.netlify.app</a>.
</div>
<div id="ref-cornell-box-compare" class="csl-entry"
role="doc-biblioentry">
Cornell University. 1998. <span>“Cornell box comparison.”</span> April
30, 1998. Accessed May 16, 2022. <a
href="http://www.graphics.cornell.edu/online/box/compare.html">http://www.graphics.cornell.edu/online/box/compare.html</a>.
</div>
<div id="ref-cornell-box-original" class="csl-entry"
role="doc-biblioentry">
———. 2005. <span>“Photograhic images of the cornell box.”</span>
February 2, 2005. Accessed May 16, 2022. <a
href="http://www.graphics.cornell.edu/online/box/data.html">http://www.graphics.cornell.edu/online/box/data.html</a>.
</div>
<div id="ref-quantumfracture-2021" class="csl-entry"
role="doc-biblioentry">
Crespo, J. L. 2021. <span>“Ya, en serio, ¿qué es la luz?”</span>
December 10, 2021. Accessed April 22, 2022. <a
href="https://www.youtube.com/watch?v=DkcEAz09Buo">https://www.youtube.com/watch?v=DkcEAz09Buo</a>.
</div>
<div id="ref-crytek-2020" class="csl-entry" role="doc-biblioentry">
Crytek. 2020. <span>“Crysis remastered brings ray tracing to current-gen
consoles.”</span> September 11, 2020. Accessed April 17, 2022. <a
href="https://www.cryengine.com/news/view/crysis-remastered-brings-ray-tracing-to-current-gen-consoles">https://www.cryengine.com/news/view/crysis-remastered-brings-ray-tracing-to-current-gen-consoles</a>.
</div>
<div id="ref-df-dlss" class="csl-entry" role="doc-biblioentry">
Digital Foundry. 2020a. <span>“Control vs DLSS 2.0: Can 540p match 1080p
image quality? Full ray tracing on RTX 2060?”</span> April 4, 2020.
Accessed May 30, 2022. <a
href="https://www.youtube.com/watch?v=YWIKzRhYZm4">https://www.youtube.com/watch?v=YWIKzRhYZm4</a>.
</div>
<div id="ref-digital-foundry-2020" class="csl-entry"
role="doc-biblioentry">
———. 2020b. <span>“Cyberpunk 2077 PC: What does ray tracing deliver...
And is it worth it?”</span> December 19, 2020. Accessed April 10, 2022.
<a
href="https://www.youtube.com/watch?v=6bqA8F6B6NQ">https://www.youtube.com/watch?v=6bqA8F6B6NQ</a>.
</div>
<div id="ref-df-spiderman" class="csl-entry" role="doc-biblioentry">
———. 2021a. <span>“Spider-man remastered PS5 vs PS4 pro + performance
ray tracing 60fps mode tested!”</span> 2021. Accessed May 30, 2022. <a
href="https://www.youtube.com/watch?v=o2HrOxwH0_Q">https://www.youtube.com/watch?v=o2HrOxwH0_Q</a>.
</div>
<div id="ref-df-global-illumination" class="csl-entry"
role="doc-biblioentry">
———. 2021b. <span>“Tech focus: Global illumination - what it is, how
does it work and why do we need it?”</span> July 24, 2021. Accessed May
20, 2022. <a
href="https://www.youtube.com/watch?v=yEkryaaAsBU">https://www.youtube.com/watch?v=yEkryaaAsBU</a>.
</div>
<div id="ref-df-xess" class="csl-entry" role="doc-biblioentry">
———. 2021c. <span>“The big interview: How intel alchemist GPUs and XeSS
upscaling will change PC gaming.”</span> August 24, 2021. Accessed May
30, 2022. <a
href="https://www.eurogamer.net/digitalfoundry-2021-the-big-intel-interview-how-intel-alchemist-gpus-and-xess-upscaling-will-change-the-market">https://www.eurogamer.net/digitalfoundry-2021-the-big-intel-interview-how-intel-alchemist-gpus-and-xess-upscaling-will-change-the-market</a>.
</div>
<div id="ref-digital-foundry" class="csl-entry" role="doc-biblioentry">
———. 2022a. <span>“Canal de youtube de digital foundry.”</span> 2022.
Accessed May 7, 2022. <a
href="https://www.youtube.com/user/DigitalFoundry">https://www.youtube.com/user/DigitalFoundry</a>.
</div>
<div id="ref-df-fsr" class="csl-entry" role="doc-biblioentry">
———. 2022b. <span>“AMD FidelityFX super resolution 2.0 - FSR 2.0 vs
native vs DLSS - the DF tech review.”</span> May 13, 2022. Accessed May
30, 2022. <a
href="https://www.youtube.com/watch?v=y2RR2770H8E">https://www.youtube.com/watch?v=y2RR2770H8E</a>.
</div>
<div id="ref-docker" class="csl-entry" role="doc-biblioentry">
Docker. 2022. <span>“Docker - the world’s most popular container
platform.”</span> 2022. Accessed May 7, 2022. <a
href="https://www.docker.com/">https://www.docker.com/</a>.
</div>
<div id="ref-dockershelf" class="csl-entry" role="doc-biblioentry">
Dockershelf. 2022. <span>“Repository for docker images of latex. Test
driven, lightweight and reliable. Rebuilt daily.”</span> 2022. Accessed
May 7, 2022. <a
href="https://hub.docker.com/r/dockershelf/latex">https://hub.docker.com/r/dockershelf/latex</a>.
</div>
<div id="ref-UE5" class="csl-entry" role="doc-biblioentry">
Epic Games. 2022a. <span>“Unreal engine 5.”</span> 2022. Accessed May
29, 2022. <a
href="https://www.unrealengine.com/en-US/unreal-engine-5">https://www.unrealengine.com/en-US/unreal-engine-5</a>.
</div>
<div id="ref-lumen" class="csl-entry" role="doc-biblioentry">
———. 2022b. <span>“Unreal engine 5 goes all-in on dynamic global
illumination with lumen.”</span> May 27, 2022. Accessed May 29, 2022. <a
href="https://www.unrealengine.com/en-US/tech-blog/unreal-engine-5-goes-all-in-on-dynamic-global-illumination-with-lumen">https://www.unrealengine.com/en-US/tech-blog/unreal-engine-5-goes-all-in-on-dynamic-global-illumination-with-lumen</a>.
</div>
<div id="ref-blue-noise" class="csl-entry" role="doc-biblioentry">
Eric Heitz, L. B. 2019. <span>“Distributing monte carlo errors as a blue
noise in screen space by permuting pixel seeds between frames.”</span>
2019. Accessed May 30, 2022. <a
href="https://eheitzresearch.wordpress.com/772-2/">https://eheitzresearch.wordpress.com/772-2/</a>.
</div>
<div id="ref-Ertl_theray" class="csl-entry" role="doc-biblioentry">
Ertl, T., W. Heidrich, M. D. (editors, N. A. Carr, J. D. Hall, and J. C.
Hart. 2022. <span>“The Ray Engine.”</span>
</div>
<div id="ref-pellacini-marschner-2017" class="csl-entry"
role="doc-biblioentry">
Fabio Pellacini, S. M. 2022. <span>“Fundamentals of computer
graphics.”</span> 2022. Accessed April 9, 2022. <a
href="https://pellacini.di.uniroma1.it/teaching/graphics17b/">https://pellacini.di.uniroma1.it/teaching/graphics17b/</a>.
</div>
<div id="ref-figma" class="csl-entry" role="doc-biblioentry">
Figma. 2022. <span>“Figma - a design tool for digital art.”</span> 2022.
Accessed May 7, 2022. <a
href="https://www.figma.com/">https://www.figma.com/</a>.
</div>
<div id="ref-RT-history" class="csl-entry" role="doc-biblioentry">
Freniere, E. R., and J. Tourtellott. 1997. <span>“<span
class="nocase">Brief history of generalized ray tracing</span>.”</span>
In <em>Lens Design, Illumination, and Optomechanical Modeling</em>,
edited by R. Barry Johnson, Richard C. Juergens, Paul R. Yoder Jr.,
Robert E. Fischer, R. Barry Johnson, Richard C. Juergens, Warren J.
Smith, and Paul R. Yoder Jr., 3130:170–78. International Society for
Optics; Photonics; SPIE. <a
href="https://doi.org/10.1117/12.284059">https://doi.org/10.1117/12.284059</a>.
</div>
<div id="ref-alain-minecraft" class="csl-entry" role="doc-biblioentry">
Galvan, A. 2020. <span>“Frame analysis - minecraft RTX beta.”</span>
2020. Accessed May 30, 2022. <a
href="https://alain.xyz/blog/frame-analysis-minecraftrtx">https://alain.xyz/blog/frame-analysis-minecraftrtx</a>.
</div>
<div id="ref-alain-API" class="csl-entry" role="doc-biblioentry">
———. 2022a. <span>“A comparison of modern graphics APIs.”</span> 2022.
Accessed May 13, 2022. <a
href="https://alain.xyz/blog/comparison-of-modern-graphics-apis">https://alain.xyz/blog/comparison-of-modern-graphics-apis</a>.
</div>
<div id="ref-alain-materials" class="csl-entry" role="doc-biblioentry">
———. 2022b. <span>“Advances in material models.”</span> 2022. Accessed
May 7, 2022. <a
href="https://alain.xyz/blog/advances-in-material-models">https://alain.xyz/blog/advances-in-material-models</a>.
</div>
<div id="ref-alain-debug" class="csl-entry" role="doc-biblioentry">
———. 2022c. <span>“Graphics debugging.”</span> 2022. Accessed May 29,
2022. <a
href="https://alain.xyz/blog/graphics-debugging">https://alain.xyz/blog/graphics-debugging</a>.
</div>
<div id="ref-alain-denoising" class="csl-entry" role="doc-biblioentry">
———. 2022d. <span>“Ray tracing denoising.”</span> 2022. Accessed May 30,
2022. <a
href="https://alain.xyz/blog/ray-tracing-denoising">https://alain.xyz/blog/ray-tracing-denoising</a>.
</div>
<div id="ref-alain-filtering" class="csl-entry" role="doc-biblioentry">
———. 2022e. <span>“Ray tracing filtering.”</span> 2022. Accessed May 30,
2022. <a
href="https://alain.xyz/blog/ray-tracing-filtering">https://alain.xyz/blog/ray-tracing-filtering</a>.
</div>
<div id="ref-galvin-no-date" class="csl-entry" role="doc-biblioentry">
Galvin. n.d. <span>“Random variables.”</span> Accessed March 20, 2022.
<a
href="https://www3.nd.edu/~dgalvin1/10120/10120_S16/Topic17_8p4_Galvin_class.pdf">https://www3.nd.edu/~dgalvin1/10120/10120_S16/Topic17_8p4_Galvin_class.pdf</a>.
</div>
<div id="ref-brdf-bp" class="csl-entry" role="doc-biblioentry">
Giesen, F. 2009. <span>“Phong and blinn-phong normalization
factors.”</span> 2009. Accessed June 10, 2022. <a
href="http://www.farbrausch.de/~fg/stuff/phong.pdf">http://www.farbrausch.de/~fg/stuff/phong.pdf</a>.
</div>
<div id="ref-github" class="csl-entry" role="doc-biblioentry">
Github. 2022. <span>“Github - where the world builds software.”</span>
2022. Accessed May 7, 2022. <a
href="https://github.com">https://github.com</a>.
</div>
<div id="ref-irradiance-caching" class="csl-entry"
role="doc-biblioentry">
Greg Ward, R. C., Francis Rubinstein. 1988. <span>“A ray tracing
solution to diffuse interreflection.”</span> 1988. Accessed May 30,
2022. <a
href="https://floyd.lbl.gov/radiance/papers/sg88/paper.html">https://floyd.lbl.gov/radiance/papers/sg88/paper.html</a>.
</div>
<div id="ref-trigger-task" class="csl-entry" role="doc-biblioentry">
Gruntfuggly. 2022. <span>“Trigger task on save.”</span> 2022. Accessed
May 7, 2022. <a
href="https://marketplace.visualstudio.com/items?itemName=Gruntfuggly.triggertaskonsave">https://marketplace.visualstudio.com/items?itemName=Gruntfuggly.triggertaskonsave</a>.
</div>
<div id="ref-GemsII-Reflexion" class="csl-entry" role="doc-biblioentry">
Haines, E. 2021. <span>“Reflection and Refraction Formulas.”</span> In
<em>Ray Tracing Gems <span>II</span></em>, edited by Adam Marrs Peter
Shirley and Ingo Wald. Apress. <a
href="https://doi.org/10.1007/978-1-4842-7185-8">https://doi.org/10.1007/978-1-4842-7185-8</a>.
</div>
<div id="ref-Haines2019" class="csl-entry" role="doc-biblioentry">
Haines, E., and T. Akenine-Möller, eds. 2019. <em>Ray Tracing Gems</em>.
Apress. <a
href="https://doi.org/10.1007/978-1-4842-4427-2">https://doi.org/10.1007/978-1-4842-4427-2</a>.
</div>
<div id="ref-rubik" class="csl-entry" role="doc-biblioentry">
Hubert, and C. Fischer Meir Sadan. 2022. <span>“Rubik
typography.”</span> 2022. Accessed May 7, 2022. <a
href="https://fonts.google.com/specimen/Rubik">https://fonts.google.com/specimen/Rubik</a>.
</div>
<div id="ref-metodos-monte-carlo" class="csl-entry"
role="doc-biblioentry">
Illana, J. I. 2013. <span>“Métodos de monte carlo.”</span> 2013.
Accessed May 8, 2022. <a
href="https://www.ugr.es/~jillana/Docencia/FM/mc.pdf">https://www.ugr.es/~jillana/Docencia/FM/mc.pdf</a>.
</div>
<div id="ref-intel-arc" class="csl-entry" role="doc-biblioentry">
Intel. 2022a. <span>“A new player has entered the game.”</span> 2022.
Accessed May 13, 2022. <a
href="https://www.intel.com/content/www/us/en/products/docs/arc-discrete-graphics/overview.html">https://www.intel.com/content/www/us/en/products/docs/arc-discrete-graphics/overview.html</a>.
</div>
<div id="ref-xess" class="csl-entry" role="doc-biblioentry">
Intel. 2022b. <span>“Xe super sampling: AI-enhanced upscaling.”</span>
2022. Accessed May 30, 2022. <a
href="https://www.intel.com/content/www/us/en/products/docs/arc-discrete-graphics/xess.html">https://www.intel.com/content/www/us/en/products/docs/arc-discrete-graphics/xess.html</a>.
</div>
<div id="ref-cornell-box-glossy" class="csl-entry"
role="doc-biblioentry">
"Jensen, H. W. "1996". <span>“"Global Illumination Using Photon
Maps".”</span> In <em>"Rendering Techniques ’96"</em>, edited by Xavier
"Pueyo and Peter" Schröder, "21–30". "Vienna": "Springer Vienna".
</div>
<div id="ref-Jensen2001" class="csl-entry" role="doc-biblioentry">
Jensen, H. W. 2001. <em>Realistic Image Synthesis Using Photon
Mapping</em>. A K Peters/<span>CRC</span> Press. <a
href="https://doi.org/10.1201/9780429294907">https://doi.org/10.1201/9780429294907</a>.
</div>
<div id="ref-kajiya" class="csl-entry" role="doc-biblioentry">
Kajiya, J. T. 1986. <span>“The Rendering Equation.”</span> <em>SIGGRAPH
Comput. Graph.</em> 20 (4): 143–50. <a
href="https://doi.org/10.1145/15886.15902">https://doi.org/10.1145/15886.15902</a>.
</div>
<div id="ref-temporal-supersampling" class="csl-entry"
role="doc-biblioentry">
Karis, B. 2014. <span>“Unreal engine 4 - high quality temporal
supersampling.”</span> 2014. Accessed May 30, 2022. <a
href="https://de45xmedrsdbp.cloudfront.net/Resources/files/TemporalAA_small-59732822.pdf">https://de45xmedrsdbp.cloudfront.net/Resources/files/TemporalAA_small-59732822.pdf</a>.
</div>
<div id="ref-latex-complete" class="csl-entry" role="doc-biblioentry">
Krieger, M. 2022. <span>“Latex complete - typeset math in your
designs.”</span> 2022. Accessed May 7, 2022. <a
href="https://www.figma.com/community/plugin/793023817364007801/LaTeX-Complete">https://www.figma.com/community/plugin/793023817364007801/LaTeX-Complete</a>.
</div>
<div id="ref-linear" class="csl-entry" role="doc-biblioentry">
Linear. 2022. <span>“Linear - the issue tracking tool you’ll enjoy
using.”</span> 2022. Accessed May 7, 2022. <a
href="https://linear.app/">https://linear.app/</a>.
</div>
<div id="ref-pandoc" class="csl-entry" role="doc-biblioentry">
MacFarlane, J. 2021. <span>“Pandoc, a universal document
converter.”</span> 2021. Accessed May 7, 2022. <a
href="https://pandoc.org">https://pandoc.org</a>.
</div>
<div id="ref-GemsII-Fresnel" class="csl-entry" role="doc-biblioentry">
Majercik, Z. 2021. <span>“The Schlick Fresnel Approximation.”</span> In
<em>Ray Tracing Gems <span>II</span></em>, edited by Adam Marrs Peter
Shirley and Ingo Wald. Apress. <a
href="https://doi.org/10.1007/978-1-4842-7185-8">https://doi.org/10.1007/978-1-4842-7185-8</a>.
</div>
<div id="ref-McGuire2018GraphicsCodex" class="csl-entry"
role="doc-biblioentry">
McGuire, M. 2021. <em>The Graphics Codex</em>. 2.17 ed. Casual Effects.
<a href="https://graphicscodex.com">https://graphicscodex.com</a>.
</div>
<div id="ref-jj" class="csl-entry" role="doc-biblioentry">
Merelo. 2021. <span>“Infraestructura virtual.”</span> 2021. Accessed
April 16, 2022. <a
href="http://jj.github.io/IV/documentos/temas/Integracion_continua">http://jj.github.io/IV/documentos/temas/Integracion_continua</a>.
</div>
<div id="ref-vscode" class="csl-entry" role="doc-biblioentry">
Microsoft. 2022. <span>“Visual studio code - code editing.
redefined.”</span> 2022. Accessed May 7, 2022. <a
href="https://code.visualstudio.com/">https://code.visualstudio.com/</a>.
</div>
<div id="ref-khonos-best-practices" class="csl-entry"
role="doc-biblioentry">
Mihut, A. 2020. <span>“Vulkan ray tracing best practices for hybrid
rendering.”</span> November 23, 2020. Accessed May 30, 2022. <a
href="https://www.khronos.org/blog/vulkan-ray-tracing-best-practices-for-hybrid-rendering">https://www.khronos.org/blog/vulkan-ray-tracing-best-practices-for-hybrid-rendering</a>.
</div>
<div id="ref-asmilex-raytracing-docker" class="csl-entry"
role="doc-biblioentry">
Millán, A. 2022a. <span>“Docker del TFG.”</span> 2022. Accessed May 7,
2022. <a
href="https://hub.docker.com/r/asmilex/raytracing">https://hub.docker.com/r/asmilex/raytracing</a>.
</div>
<div id="ref-asmilex-raytracing-repo" class="csl-entry"
role="doc-biblioentry">
———. 2022b. <span>“Repositorio del TFG.”</span> 2022. Accessed May 7,
2022. <a
href="https://github.com/Asmilex/Raytracing">https://github.com/Asmilex/Raytracing</a>.
</div>
<div id="ref-video" class="csl-entry" role="doc-biblioentry">
———. 2022c. <span>“Path tracing showcase.”</span> May 18, 2022. Accessed
May 22, 2022. <a
href="https://www.youtube.com/watch?v=pXrD3K69MqE">https://www.youtube.com/watch?v=pXrD3K69MqE</a>.
</div>
<div id="ref-GemsI-IS" class="csl-entry" role="doc-biblioentry">
Moreau, P., and P. Clarberg. 2019. <span>“Importance Sampling of Many
Lights on the GPU.”</span> In <em>Ray Tracing Gems</em>, edited by Eric
Haines and Tomas Akenine-Möller. Apress. <a
href="https://doi.org/10.1007/978-1-4842-4427-2">https://doi.org/10.1007/978-1-4842-4427-2</a>.
</div>
<div id="ref-xkcd-size" class="csl-entry" role="doc-biblioentry">
Munroe, R. n.d. <span>“XKCD - angular size.”</span> Accessed May 7,
2022. <a href="https://xkcd.com/1276/">https://xkcd.com/1276/</a>.
</div>
<div id="ref-nature-2016" class="csl-entry" role="doc-biblioentry">
Nature. 2016. <span>“Scientific language is becoming more
informal.”</span> 2016. Accessed April 10, 2022. <a
href="https://doi.org/10.1038/539140a">https://doi.org/10.1038/539140a</a>.
</div>
<div id="ref-explorable-explanations" class="csl-entry"
role="doc-biblioentry">
Ncase. 2022. <span>“Explorable explanations.”</span> 2022. Accessed May
7, 2022. <a href="https://explorabl.es/">https://explorabl.es/</a>.
</div>
<div id="ref-jetbrains-mono" class="csl-entry" role="doc-biblioentry">
Nurullin, P. 2022. <span>“JetBrains mono, a typeface for
developers.”</span> 2022. Accessed May 7, 2022. <a
href="https://www.jetbrains.com/es-es/lp/mono/">https://www.jetbrains.com/es-es/lp/mono/</a>.
</div>
<div id="ref-Q2RTX" class="csl-entry" role="doc-biblioentry">
Nvidia. 2018. <span>“Quake II RTX.”</span> 2018. Accessed May 30, 2022.
<a
href="https://github.com/NVIDIA/Q2RTX">https://github.com/NVIDIA/Q2RTX</a>.
</div>
<div id="ref-dlss" class="csl-entry" role="doc-biblioentry">
Nvidia. 2020a. <span>“NVIDIA DLSS 2.0: A big leap in AI
rendering.”</span> 2020. Accessed May 30, 2022. <a
href="https://www.nvidia.com/en-us/geforce/news/nvidia-dlss-2-0-a-big-leap-in-ai-rendering/">https://www.nvidia.com/en-us/geforce/news/nvidia-dlss-2-0-a-big-leap-in-ai-rendering/</a>.
</div>
<div id="ref-nvidia-best-practices" class="csl-entry"
role="doc-biblioentry">
Nvidia. 2020b. <span>“Best practices: Using nvidia RTX ray
tracing.”</span> October 10, 2020. Accessed May 13, 2022. <a
href="https://developer.nvidia.com/blog/best-practices-using-nvidia-rtx-ray-tracing/">https://developer.nvidia.com/blog/best-practices-using-nvidia-rtx-ray-tracing/</a>.
</div>
<div id="ref-nvpro-samples-tutorial" class="csl-entry"
role="doc-biblioentry">
———. 2022a. <span>“Nvidia DesignWorks KHR tutorial.”</span> 2022.
Accessed May 13, 2022. <a
href="https://github.com/nvpro-samples/vk_raytracing_tutorial_KHR">https://github.com/nvpro-samples/vk_raytracing_tutorial_KHR</a>.
</div>
<div id="ref-nvpro-samples" class="csl-entry" role="doc-biblioentry">
———. 2022b. <span>“Nvidia DesignWorks samples.”</span> 2022. Accessed
May 13, 2022. <a
href="https://github.com/nvpro-samples">https://github.com/nvpro-samples</a>.
</div>
<div id="ref-turing-arquitecture" class="csl-entry"
role="doc-biblioentry">
Nvidia, H. M., Emmett Kilgariff. 2018. <span>“NVIDIA turing architecture
in-depth.”</span> September 14, 2018. Accessed May 26, 2022. <a
href="https://developer.nvidia.com/blog/nvidia-turing-architecture-in-depth/">https://developer.nvidia.com/blog/nvidia-turing-architecture-in-depth/</a>.
</div>
<div id="ref-overvoorde-2022" class="csl-entry" role="doc-biblioentry">
Overvoorde, A. 2022. <span>“Introduction - vulkan tutorial.”</span>
2022. Accessed April 18, 2022. <a
href="https://vulkan-tutorial.com/">https://vulkan-tutorial.com/</a>.
</div>
<div id="ref-mcbook" class="csl-entry" role="doc-biblioentry">
Owen, A. B. 2013. <em>Monte Carlo Theory, Methods and Examples</em>. <a
href="https://artowen.su.domains/mc/">https://artowen.su.domains/mc/</a>.
</div>
<div id="ref-PBRT3e" class="csl-entry" role="doc-biblioentry">
Pharr, M., W. Jakob, and G. Humphreys. 2016. <span>“Physically based
rendering: From theory to implementation (3rd ed.).”</span> San
Francisco, CA, USA: Morgan Kaufmann Publishers Inc. November 2016. <a
href="https://www.pbr-book.org/3ed-2018/contents">https://www.pbr-book.org/3ed-2018/contents</a>.
</div>
<div id="ref-10.1145/566654.566640" class="csl-entry"
role="doc-biblioentry">
Purcell, T. J., I. Buck, W. R. Mark, and P. Hanrahan. 2002. <span>“Ray
Tracing on Programmable Graphics Hardware.”</span> <em>ACM Trans.
Graph.</em> 21 (3): 703–12. <a
href="https://doi.org/10.1145/566654.566640">https://doi.org/10.1145/566654.566640</a>.
</div>
<div id="ref-gamma-correction" class="csl-entry" role="doc-biblioentry">
Quílez, Í. 2013. <span>“Outdoors lightning.”</span> 2013. Accessed May
16, 2022. <a
href="https://iquilezles.org/articles/outdoorslighting/">https://iquilezles.org/articles/outdoorslighting/</a>.
</div>
<div id="ref-quasi-monte-carlo" class="csl-entry"
role="doc-biblioentry">
Roberts, M. 2018. <span>“The unreasonable effectiveness of quasirandom
sequences.”</span> 2018. Accessed May 9, 2022. <a
href="http://extremelearning.com.au/unreasonable-effectiveness-of-quasirandom-sequences/">http://extremelearning.com.au/unreasonable-effectiveness-of-quasirandom-sequences/</a>.
</div>
<div id="ref-3blue1brown" class="csl-entry" role="doc-biblioentry">
Sanderson, G. 2022. <span>“3Blue1Brown.”</span> 2022. Accessed May 7,
2022. <a
href="https://www.3blue1brown.com/">https://www.3blue1brown.com/</a>.
</div>
<div id="ref-https://doi.org/10.1111/1467-8659.1330233"
class="csl-entry" role="doc-biblioentry">
Schlick, C. 1994. <span>“An Inexpensive BRDF Model for Physically-Based
Rendering.”</span> <em>Computer Graphics Forum</em> 13 (3): 233–46.
https://doi.org/<a
href="https://doi.org/10.1111/1467-8659.1330233">https://doi.org/10.1111/1467-8659.1330233</a>.
</div>
<div id="ref-scratchapixel-2019" class="csl-entry"
role="doc-biblioentry">
Scratchapixel. 2019. <span>“Learn computer graphics from
scratch!”</span> 2019. Accessed April 17, 2022. <a
href="https://www.scratchapixel.com/index.php?redirect">https://www.scratchapixel.com/index.php?redirect</a>.
</div>
<div id="ref-Shirley2020RTW1" class="csl-entry" role="doc-biblioentry">
Shirley, P. 2020a. <span>“Ray tracing in one weekend.”</span> 2020. <a
href="https://raytracing.github.io/books/RayTracingInOneWeekend.html">https://raytracing.github.io/books/RayTracingInOneWeekend.html</a>.
</div>
<div id="ref-Shirley2020RTW2" class="csl-entry" role="doc-biblioentry">
———. 2020b. <span>“Ray tracing: The next week.”</span> 2020. <a
href="https://raytracing.github.io/books/RayTracingTheNextWeek.html">https://raytracing.github.io/books/RayTracingTheNextWeek.html</a>.
</div>
<div id="ref-Shirley2020RTW3" class="csl-entry" role="doc-biblioentry">
———. 2020c. <span>“Ray tracing: The rest of your life.”</span> 2020. <a
href="https://raytracing.github.io/books/RayTracingTheRestOfYourLife.html">https://raytracing.github.io/books/RayTracingTheRestOfYourLife.html</a>.
</div>
<div id="ref-ShirleyRRT" class="csl-entry" role="doc-biblioentry">
Shirley, P., and R. K. Morley. 2003. <em>Realistic Ray Tracing</em>. 2nd
ed. USA: A. K. Peters, Ltd. <a
href="https://www.taylorfrancis.com/books/mono/10.1201/9780429294891/realistic-ray-tracing-peter-shirley-keith-morley">https://www.taylorfrancis.com/books/mono/10.1201/9780429294891/realistic-ray-tracing-peter-shirley-keith-morley</a>.
</div>
<div id="ref-siggraph2021" class="csl-entry" role="doc-biblioentry">
Siggraph. 2021. <span>“Advances in real-time rendering in gaming
courses.”</span> 2021. Accessed May 29, 2022. <a
href="http://advances.realtimerendering.com/s2021/index.html">http://advances.realtimerendering.com/s2021/index.html</a>.
</div>
<div id="ref-Szirmay-Kalos00monte-carlomethods" class="csl-entry"
role="doc-biblioentry">
Szirmay-Kalos, L. 2000. <span>“Monte-Carlo Methods in Global
Illumination.”</span> <a
href="https://doi.org/10.1.1.36.361">https://doi.org/10.1.1.36.361</a>.
</div>
<div id="ref-vulkan" class="csl-entry" role="doc-biblioentry">
The Khronos Vulkan Working Group. 2022. <span>“Vulkan® 1.2.210 - a
specification (with KHR extensions).”</span> March 29, 2022. Accessed
April 1, 2022. <a
href="- https://www.khronos.org/registry/vulkan/specs/1.2-khr-extensions/html/chap1.html">-
https://www.khronos.org/registry/vulkan/specs/1.2-khr-extensions/html/chap1.html</a>.
</div>
<div id="ref-fraunces" class="csl-entry" role="doc-biblioentry">
Undercase Type, P. C., and F. Zimbardi. 2022. <span>“Fraunces
typography.”</span> 2022. Accessed May 7, 2022. <a
href="https://fonts.google.com/specimen/Fraunces">https://fonts.google.com/specimen/Fraunces</a>.
</div>
<div id="ref-carlos-path-tracing" class="csl-entry"
role="doc-biblioentry">
Ureña, C. 2021. <span>“Realismo e iluminación global.”</span> 2021. <a
href="https://lsi2.ugr.es/curena/">https://lsi2.ugr.es/curena/</a>.
</div>
<div id="ref-shader-binding-table" class="csl-entry"
role="doc-biblioentry">
Usher, W. 2019. <span>“The RTX shader binding table three ways.”</span>
November 20, 2019. Accessed May 13, 2022. <a
href="https://www.willusher.io/graphics/2019/11/20/the-sbt-three-ways">https://www.willusher.io/graphics/2019/11/20/the-sbt-three-ways</a>.
</div>
<div id="ref-GemsII-SBT" class="csl-entry" role="doc-biblioentry">
———. 2021. <span>“The Shader Binding Table Demystified.”</span> In
<em>Ray Tracing Gems <span>II</span></em>, edited by Adam Marrs Peter
Shirley and Ingo Wald. Apress. <a
href="https://doi.org/10.1007/978-1-4842-7185-8">https://doi.org/10.1007/978-1-4842-7185-8</a>.
</div>
<div id="ref-proton" class="csl-entry" role="doc-biblioentry">
Valve Software. 2022a. <span>“Proton.”</span> 2022. Accessed May 16,
2022. <a
href="https://github.com/ValveSoftware/Proton">https://github.com/ValveSoftware/Proton</a>.
</div>
<div id="ref-valve" class="csl-entry" role="doc-biblioentry">
———. 2022b. <span>“Valve software.”</span> 2022. Accessed May 16, 2022.
<a
href="https://www.valvesoftware.com/es/">https://www.valvesoftware.com/es/</a>.
</div>
<div id="ref-robust-monte-carlo" class="csl-entry"
role="doc-biblioentry">
Veach, E. December 1997. <span>“Robust monte carlo methods for light
transport simulation.”</span> December 1997. Accessed May 9, 2022. <a
href="https://graphics.stanford.edu/papers/veach_thesis/">https://graphics.stanford.edu/papers/veach_thesis/</a>.
</div>
<div id="ref-vectary" class="csl-entry" role="doc-biblioentry">
Vectary. 2022. <span>“Vectary - bringing unlimited creativity to 3D
design.”</span> 2022. Accessed May 7, 2022. <a
href="https://www.vectary.com/">https://www.vectary.com/</a>.
</div>
<div id="ref-eisvogel" class="csl-entry" role="doc-biblioentry">
Wagler, P. 2022. <span>“Eisvogel pandoc template.”</span> 2022. Accessed
May 7, 2022. <a
href="https://github.com/Wandmalfarbe/pandoc-latex-template">https://github.com/Wandmalfarbe/pandoc-latex-template</a>.
</div>
<div id="ref-disney-brdfs" class="csl-entry" role="doc-biblioentry">
Walt Disney Animation Studios. 2019. <span>“BRDF explorer.”</span> 2019.
Accessed May 7, 2022. <a
href="https://github.com/wdas/brdf">https://github.com/wdas/brdf</a>.
</div>
<div id="ref-Whitted1979AnII" class="csl-entry" role="doc-biblioentry">
Whitted, T. 1979. <span>“An Improved Illumination Model for Shaded
Display.”</span> In <em>SIGGRAPH ’79</em>.
</div>
<div id="ref-directx-12" class="csl-entry" role="doc-biblioentry">
Wikipedia. 2022a. <span>“DirectX raytracing.”</span> 2022. Accessed May
16, 2022. <a
href="https://en.wikipedia.org/wiki/DirectX_Raytracing">https://en.wikipedia.org/wiki/DirectX_Raytracing</a>.
</div>
<div id="ref-wikipedia-nvidia" class="csl-entry" role="doc-biblioentry">
Wikipedia. 2022b. <span>“List of nvidia graphics processing
units.”</span> 2022. Accessed May 13, 2022. <a
href="https://en.wikipedia.org/wiki/List_of_Nvidia_graphics_processing_units">https://en.wikipedia.org/wiki/List_of_Nvidia_graphics_processing_units</a>.
</div>
<div id="ref-optix" class="csl-entry" role="doc-biblioentry">
Wikipedia. 2022c. <span>“OptiX.”</span> 2022. Accessed May 16, 2022. <a
href="https://en.wikipedia.org/wiki/OptiX">https://en.wikipedia.org/wiki/OptiX</a>.
</div>
<div id="ref-pinhole" class="csl-entry" role="doc-biblioentry">
Wikipedia. 2022d. <span>“Pinhole camera.”</span> 2022. Accessed May 26,
2022. <a
href="https://en.wikipedia.org/wiki/Pinhole_camera">https://en.wikipedia.org/wiki/Pinhole_camera</a>.
</div>
<div id="ref-ps5" class="csl-entry" role="doc-biblioentry">
———. 2022e. <span>“PlayStation 5.”</span> 2022. Accessed May 30, 2022.
<a
href="https://en.wikipedia.org/wiki/PlayStation_5?oldformat=true">https://en.wikipedia.org/wiki/PlayStation_5?oldformat=true</a>.
</div>
<div id="ref-wikipedia-radeon" class="csl-entry" role="doc-biblioentry">
———. 2022f. <span>“Radeon.”</span> 2022. Accessed May 13, 2022. <a
href="https://en.wikipedia.org/wiki/Radeon">https://en.wikipedia.org/wiki/Radeon</a>.
</div>
<div id="ref-series-x" class="csl-entry" role="doc-biblioentry">
———. 2022g. <span>“Xbox series x and series s.”</span> 2022. Accessed
May 30, 2022. <a
href="https://en.wikipedia.org/wiki/Xbox_Series_X_and_Series_S?oldformat=true">https://en.wikipedia.org/wiki/Xbox_Series_X_and_Series_S?oldformat=true</a>.
</div>
<div id="ref-wikipedia-contributors-2022O" class="csl-entry"
role="doc-biblioentry">
———. 2022h. <span>“Differential geometry of surfaces.”</span> January
14, 2022. Accessed April 22, 2022. <a
href="https://en.wikipedia.org/wiki/Differential_geometry_of_surfaces">https://en.wikipedia.org/wiki/Differential_geometry_of_surfaces</a>.
</div>
<div id="ref-wikipedia-contributors-2022G" class="csl-entry"
role="doc-biblioentry">
———. 2022i. <span>“Barycentric coordinate system.”</span> March 14,
2022. Accessed April 22, 2022. <a
href="https://en.wikipedia.org/wiki/Barycentric_coordinate_system">https://en.wikipedia.org/wiki/Barycentric_coordinate_system</a>.
</div>
<div id="ref-history-photography" class="csl-entry"
role="doc-biblioentry">
———. 2022j. <span>“History of photography.”</span> March 18, 2022.
Accessed April 22, 2022. <a
href="https://en.wikipedia.org/wiki/History_of_photography">https://en.wikipedia.org/wiki/History_of_photography</a>.
</div>
<div id="ref-stir" class="csl-entry" role="doc-biblioentry">
Wolfe, A. 2022. <span>“Sampling importance resampling.”</span> March 2,
2022. Accessed May 30, 2022. <a
href="https://blog.demofox.org/2022/03/02/sampling-importance-resampling/">https://blog.demofox.org/2022/03/02/sampling-importance-resampling/</a>.
</div>
<div id="ref-tuwien" class="csl-entry" role="doc-biblioentry">
Zsolnai-Fehér, K. 2022. <span>“Lecture rendering.”</span> 2022. Accessed
May 26, 2022. <a
href="https://www.cg.tuwien.ac.at/courses/Rendering/VU.SS2019.html">https://www.cg.tuwien.ac.at/courses/Rendering/VU.SS2019.html</a>.
</div>
</div>
<section class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>No entraremos en detalle sobre la
naturaleza de la luz. Sin embargo, si te pica la curiosidad, hay muchos
divulgadores como <span class="citation"
data-cites="quantumfracture-2021">(<a href="#ref-quantumfracture-2021"
role="doc-biblioref">Crespo 2021</a>)</span> que han tratado el tema con
suficiente profundidad.<a href="#fnref1" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>Recuerda que estamos omitiendo la
longitud de onda <span class="math inline">\(\lambda\)</span>.<a
href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>En su defecto, si tenemos una función
de densidad <span class="math inline">\(p_X\)</span>, podemos hallar la
función de distribución haciendo <span class="math inline">\(F_X(x) =
P[X &lt; x] = \int_{x_{min}}^{x}{p_X(t)dt}\)</span>.<a href="#fnref3"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4" role="doc-endnote"><p>Esto no es del todo cierto. Aunque
generalmente suelen ser excepciones debido al coste computacional de RT
en tiempo real, existen algunas implementaciones que son capaces de
correrlo por software. Notablemente, el motor de Crytek, CryEngine, es
capaz de mover ray tracing basado en hardware y en software <span
class="citation" data-cites="crytek-2020">(<a href="#ref-crytek-2020"
role="doc-biblioref">Crytek 2020</a>)</span><a href="#fnref4"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5" role="doc-endnote"><p>Afortunadamente, esto tampoco es
completamente cierto. La compañía desarrolladora y distribuidora de
videojuegos Valve Corporation <span class="citation"
data-cites="valve">(<a href="#ref-valve" role="doc-biblioref">Valve
Software 2022b</a>)</span> ha creado una pieza de software fascinante:
Proton <span class="citation" data-cites="proton">(<a href="#ref-proton"
role="doc-biblioref">Valve Software 2022a</a>)</span>. Proton utiliza
Wine para emular software en Linux que solo puede correr en plataformas
Windows. La versión 2.5 añadió soporte para traducción de bindings de
DXR a KHR, lo que permite utilizar DirectX12 ray tracing en sistemas
basados en Linux. El motivo de este software es expandir el mercado de
videojuegos disponibles en su consola, la Steam Deck.<a href="#fnref5"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
</body>
</html>
