<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Andrés Millán Muñoz" />
  <meta name="keywords" content="raytracing, ray tracing, Monte
Carlo, Monte Carlo integration, radiometry, path tracing, Vulkan" />
  <title>Los fundamentos de Ray Tracing</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
  <link rel="stylesheet" href="https://unpkg.com/bamboo.css/dist/light.min.css">
  <link rel="stylesheet" href="./headers/style.css">
  <link rel="icon" type="image/x-icon" href="./img/favicon.svg">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.26.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.26.0/plugins/autoloader/prism-autoloader.min.js"></script>
  
  <!-- pandoc-eqnos: equation style -->
  <style>
    .eqnos { display: inline-block; position: relative; width: 100%; }
    .eqnos br { display: none; }
    .eqnos-number { position: absolute; right: 0em; top: 50%; line-height: 0; }
  </style>
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Los fundamentos de Ray Tracing</h1>
<p class="author">Andrés Millán Muñoz</p>
</header>
<nav id="TOC" role="doc-toc">
<h2 id="toc-title">Tabla de contenidos</h2>
<ul>
<li><a href="#abstract">Abstract</a></li>
<li><a href="#a-brief-overview">A brief overview</a></li>
<li><a href="#dedicatoria">Dedicatoria</a></li>
<li><a href="#introducción"><span class="toc-section-number">1</span>
Introducción</a>
<ul>
<li><a href="#nota-histórica"><span
class="toc-section-number">1.1</span> Nota histórica</a></li>
<li><a href="#qué-es-ray-tracing"><span
class="toc-section-number">1.2</span> ¿Qué es ray tracing?</a></li>
<li><a href="#objetivos-del-trabajo"><span
class="toc-section-number">1.3</span> Objetivos del trabajo</a></li>
<li><a href="#técnicas-empleadas-para-la-resolución"><span
class="toc-section-number">1.4</span> Técnicas empleadas para la
resolución</a></li>
<li><a href="#principales-fuentes-consultadas"><span
class="toc-section-number">1.5</span> Principales fuentes
consultadas</a></li>
</ul></li>
<li><a href="#las-bases"><span class="toc-section-number">2</span> Las
bases</a>
<ul>
<li><a href="#eligiendo-direcciones"><span
class="toc-section-number">2.1</span> Eligiendo direcciones</a></li>
<li><a href="#intersecciones-rayo---objeto"><span
class="toc-section-number">2.2</span> Intersecciones rayo - objeto</a>
<ul>
<li><a href="#superficies-implícitas"><span
class="toc-section-number">2.2.1</span> Superficies implícitas</a></li>
<li><a href="#superficies-paramétricas"><span
class="toc-section-number">2.2.2</span> Superficies
paramétricas</a></li>
<li><a href="#intersecciones-con-esferas"><span
class="toc-section-number">2.2.3</span> Intersecciones con
esferas</a></li>
<li><a href="#intersecciones-con-triángulos"><span
class="toc-section-number">2.2.4</span> Intersecciones con
triángulos</a>
<ul>
<li><a href="#coordenadas-baricéntricas"><span
class="toc-section-number">2.2.4.1</span> Coordenadas
baricéntricas</a></li>
<li><a href="#calculando-la-intersección"><span
class="toc-section-number">2.2.4.2</span> Calculando la
intersección</a></li>
</ul></li>
</ul></li>
</ul></li>
<li><a href="#transporte-de-luz"><span
class="toc-section-number">3</span> Transporte de luz</a>
<ul>
<li><a href="#introducción-a-la-radiometría"><span
class="toc-section-number">3.1</span> Introducción a la radiometría</a>
<ul>
<li><a href="#potencia"><span class="toc-section-number">3.1.1</span>
Potencia</a></li>
<li><a href="#irradiancia"><span class="toc-section-number">3.1.2</span>
Irradiancia</a></li>
<li><a href="#ángulos-sólidos"><span
class="toc-section-number">3.1.3</span> Ángulos sólidos</a></li>
<li><a href="#intensidad-radiante"><span
class="toc-section-number">3.1.4</span> Intensidad radiante</a></li>
<li><a href="#radiancia"><span class="toc-section-number">3.1.5</span>
Radiancia</a></li>
<li><a href="#integrales-radiométricas"><span
class="toc-section-number">3.1.6</span> Integrales radiométricas</a>
<ul>
<li><a href="#una-nueva-expresión-de-la-irradiancia-y-el-flujo"><span
class="toc-section-number">3.1.6.1</span> Una nueva expresión de la
irradiancia y el flujo</a></li>
<li><a href="#integrando-sobre-área"><span
class="toc-section-number">3.1.6.2</span> Integrando sobre área</a></li>
</ul></li>
<li><a href="#fotometría-y-radiometría"><span
class="toc-section-number">3.1.7</span> Fotometría y
radiometría</a></li>
</ul></li>
<li><a href="#dispersión-de-luz"><span
class="toc-section-number">3.2</span> Dispersión de luz</a>
<ul>
<li><a
href="#la-función-de-distribución-de-reflectancia-bidireccional-brdf"><span
class="toc-section-number">3.2.1</span> La función de distribución de
reflectancia bidireccional (BRDF)</a></li>
<li><a
href="#la-función-de-distribución-de-transmitancia-bidireccional-btdf"><span
class="toc-section-number">3.2.2</span> La función de distribución de
transmitancia bidireccional (BTDF)</a></li>
<li><a
href="#la-función-de-distribución-de-dispersión-bidireccional-bsdf"><span
class="toc-section-number">3.2.3</span> La función de distribución de
dispersión bidireccional (BSDF)</a></li>
<li><a href="#reflectancia-hemisférica"><span
class="toc-section-number">3.2.4</span> Reflectancia
hemisférica</a></li>
</ul></li>
<li><a href="#modelos-ópticos-de-materiales"><span
class="toc-section-number">3.3</span> Modelos ópticos de materiales</a>
<ul>
<li><a href="#tipos-de-dispersión"><span
class="toc-section-number">3.3.1</span> Tipos de dispersión</a></li>
<li><a href="#reflexión"><span class="toc-section-number">3.3.2</span>
Reflexión</a>
<ul>
<li><a href="#reflexión-especular-perfecta"><span
class="toc-section-number">3.3.2.1</span> Reflexión especular
perfecta</a></li>
<li><a href="#reflexión-difusa-o-lamberiana"><span
class="toc-section-number">3.3.2.2</span> Reflexión difusa o
lamberiana</a></li>
<li><a href="#reflexión-especular-no-perfecta"><span
class="toc-section-number">3.3.2.3</span> Reflexión especular no
perfecta</a></li>
</ul></li>
<li><a href="#refracción"><span class="toc-section-number">3.3.3</span>
Refracción</a>
<ul>
<li><a href="#ley-de-snell"><span
class="toc-section-number">3.3.3.1</span> Ley de Snell</a></li>
<li><a href="#ecuaciones-de-fresnel"><span
class="toc-section-number">3.3.3.2</span> Ecuaciones de Fresnel</a></li>
<li><a href="#la-aproximación-de-schlick"><span
class="toc-section-number">3.3.3.3</span> La aproximación de
Schlick</a></li>
</ul></li>
<li><a href="#materiales-híbridos"><span
class="toc-section-number">3.3.4</span> Materiales híbridos</a></li>
<li><a href="#otros-modelos"><span
class="toc-section-number">3.3.5</span> Otros modelos</a>
<ul>
<li><a href="#oren---nayar"><span
class="toc-section-number">3.3.5.1</span> Oren - Nayar</a></li>
<li><a href="#ggx"><span class="toc-section-number">3.3.5.2</span>
GGX</a></li>
</ul></li>
</ul></li>
<li><a href="#la-rendering-equation"><span
class="toc-section-number">3.4</span> La rendering equation</a></li>
</ul></li>
<li><a href="#métodos-de-monte-carlo"><span
class="toc-section-number">4</span> Métodos de Monte Carlo</a>
<ul>
<li><a href="#repaso-de-probabilidad"><span
class="toc-section-number">4.1</span> Repaso de probabilidad</a>
<ul>
<li><a href="#variables-aleatorias-discretas"><span
class="toc-section-number">4.1.1</span> Variables aleatorias
discretas</a></li>
<li><a href="#variables-aleatorias-continuas"><span
class="toc-section-number">4.1.2</span> Variables aleatorias
continuas</a></li>
<li><a href="#esperanza-y-varianza-de-una-variable-aleatoria"><span
class="toc-section-number">4.1.3</span> Esperanza y varianza de una
variable aleatoria</a></li>
<li><a href="#teoremas-importantes"><span
class="toc-section-number">4.1.4</span> Teoremas importantes</a></li>
<li><a href="#estimadores"><span class="toc-section-number">4.1.5</span>
Estimadores</a></li>
</ul></li>
<li><a href="#el-estimador-de-monte-carlo"><span
class="toc-section-number">4.2</span> El estimador de Monte Carlo</a>
<ul>
<li><a href="#monte-carlo-básico"><span
class="toc-section-number">4.2.1</span> Monte Carlo básico</a></li>
<li><a href="#integración-de-monte-carlo"><span
class="toc-section-number">4.2.2</span> Integración de Monte Carlo</a>
<ul>
<li><a href="#un-ejemplo-práctico-en-r"><span
class="toc-section-number">4.2.2.1</span> Un ejemplo práctico en
R</a></li>
</ul></li>
</ul></li>
<li><a href="#técnicas-de-reducción-de-varianza"><span
class="toc-section-number">4.3</span> Técnicas de reducción de
varianza</a>
<ul>
<li><a href="#muestreo-por-importancia"><span
class="toc-section-number">4.3.1</span> Muestreo por importancia</a>
<ul>
<li><a href="#muestreo-por-importancia-en-transporte-de-luz"><span
class="toc-section-number">4.3.1.1</span> Muestreo por importancia en
transporte de luz</a></li>
</ul></li>
<li><a href="#muestreo-por-importancia-múltiple"><span
class="toc-section-number">4.3.2</span> Muestreo por importancia
múltiple</a>
<ul>
<li><a
href="#muestreo-por-importancia-múltiple-en-transporte-de-luz"><span
class="toc-section-number">4.3.2.1</span> Muestreo por importancia
múltiple en transporte de luz</a></li>
</ul></li>
<li><a
href="#otras-técnicas-de-reducción-de-varianza-en-transporte-de-luz"><span
class="toc-section-number">4.3.3</span> Otras técnicas de reducción de
varianza en transporte de luz</a>
<ul>
<li><a href="#ruleta-rusa"><span
class="toc-section-number">4.3.3.1</span> Ruleta rusa</a></li>
<li><a
href="#next-event-estimation-o-muestreo-directo-de-fuentes-de-luz"><span
class="toc-section-number">4.3.3.2</span> Next event estimation, o
muestreo directo de fuentes de luz</a></li>
<li><a href="#quasi-monte-carlo"><span
class="toc-section-number">4.3.3.3</span> Quasi-Monte Carlo</a></li>
</ul></li>
</ul></li>
<li><a href="#escogiendo-puntos-aleatorios"><span
class="toc-section-number">4.4</span> Escogiendo puntos aleatorios</a>
<ul>
<li><a href="#método-de-la-transformada-inversa"><span
class="toc-section-number">4.4.1</span> Método de la transformada
inversa</a></li>
<li><a href="#método-del-rechazo"><span
class="toc-section-number">4.4.2</span> Método del rechazo</a></li>
</ul></li>
</ul></li>
<li><a href="#construyamos-un-path-tracer"><span
class="toc-section-number">5</span> ¡Construyamos un path tracer!</a>
<ul>
<li><a href="#el-algoritmo-de-path-tracing"><span
class="toc-section-number">5.1</span> El algoritmo de path tracing</a>
<ul>
<li><a href="#estimando-la-rendering-equation-con-monte-carlo"><span
class="toc-section-number">5.1.1</span> Estimando la rendering equation
con Monte Carlo</a></li>
<li><a href="#pseudocódigo-de-un-path-tracer"><span
class="toc-section-number">5.1.2</span> Pseudocódigo de un path
tracer</a></li>
<li><a href="#evitando-la-recursividad"><span
class="toc-section-number">5.1.3</span> Evitando la
recursividad</a></li>
</ul></li>
<li><a href="#requisitos-de-ray-tracing-en-tiempo-real"><span
class="toc-section-number">5.2</span> Requisitos de ray tracing en
tiempo real</a>
<ul>
<li><a href="#arquitecturas-de-gráficas"><span
class="toc-section-number">5.2.1</span> Arquitecturas de
gráficas</a></li>
<li><a href="#frameworks-y-api-de-ray-tracing-en-tiempo-real"><span
class="toc-section-number">5.2.2</span> Frameworks y API de ray tracing
en tiempo real</a></li>
</ul></li>
<li><a href="#setup-del-proyecto"><span
class="toc-section-number">5.3</span> Setup del proyecto</a>
<ul>
<li><a href="#vistazo-general-a-la-estructura"><span
class="toc-section-number">5.3.1</span> Vistazo general a la
estructura</a></li>
</ul></li>
<li><a href="#compilación-y-ejecución"><span
class="toc-section-number">5.4</span> Compilación y ejecución</a></li>
<li><a href="#estructuras-de-aceleración"><span
class="toc-section-number">5.5</span> Estructuras de aceleración</a>
<ul>
<li><a href="#botom-level-acceleration-structure-blas"><span
class="toc-section-number">5.5.1</span> Botom-Level Acceleration
Structure (BLAS)</a></li>
<li><a href="#top-level-acceleration-structure-tlas"><span
class="toc-section-number">5.5.2</span> Top-Level Acceleration Structure
(TLAS)</a></li>
</ul></li>
<li><a href="#la-ray-tracing-pipeline"><span
class="toc-section-number">5.6</span> La ray tracing pipeline</a>
<ul>
<li><a href="#descriptores-y-conceptos-básicos"><span
class="toc-section-number">5.6.1</span> Descriptores y conceptos
básicos</a></li>
<li><a href="#la-shader-binding-table"><span
class="toc-section-number">5.6.2</span> La Shader Binding Table</a></li>
<li><a href="#tipos-de-shaders"><span
class="toc-section-number">5.6.3</span> Tipos de shaders</a></li>
<li><a href="#traspaso-de-información-entre-shaders"><span
class="toc-section-number">5.6.4</span> Traspaso de información entre
shaders</a></li>
<li><a href="#creación-de-la-ray-tracing-pipeline"><span
class="toc-section-number">5.6.5</span> Creación de la ray tracing
pipeline</a></li>
</ul></li>
<li><a href="#materiales-y-objetos"><span
class="toc-section-number">5.7</span> Materiales y objetos</a></li>
<li><a href="#fuentes-de-luz"><span
class="toc-section-number">5.8</span> Fuentes de luz</a></li>
<li><a
href="#antialiasing-mediante-jittering-y-acumulación-temporal"><span
class="toc-section-number">5.9</span> Antialiasing mediante jittering y
acumulación temporal</a></li>
<li><a href="#corrección-de-gamma"><span
class="toc-section-number">5.10</span> Corrección de gamma</a></li>
</ul></li>
<li><a href="#análisis-de-rendimiento"><span
class="toc-section-number">6</span> Análisis de rendimiento</a>
<ul>
<li><a href="#usando-el-motor"><span
class="toc-section-number">6.1</span> Usando el motor</a>
<ul>
<li><a href="#cambio-de-escena"><span
class="toc-section-number">6.1.1</span> Cambio de escena</a></li>
</ul></li>
<li><a href="#path-tracing-showcase"><span
class="toc-section-number">6.2</span> Path tracing showcase</a>
<ul>
<li><a href="#materiales"><span class="toc-section-number">6.2.1</span>
Materiales</a></li>
<li><a href="#fuentes-de-luz-1"><span
class="toc-section-number">6.2.2</span> Fuentes de luz</a></li>
<li><a href="#iluminación-global"><span
class="toc-section-number">6.2.3</span> Iluminación global</a></li>
</ul></li>
<li><a href="#rendimiento"><span class="toc-section-number">6.3</span>
Rendimiento</a>
<ul>
<li><a href="#número-de-muestras"><span
class="toc-section-number">6.3.1</span> Número de muestras</a></li>
<li><a href="#profundidad-de-un-rayo"><span
class="toc-section-number">6.3.2</span> Profundidad de un rayo</a></li>
<li><a href="#acumulación-temporal"><span
class="toc-section-number">6.3.3</span> Acumulación temporal</a></li>
<li><a href="#resolución"><span class="toc-section-number">6.3.4</span>
Resolución</a></li>
<li><a href="#importance-sampling"><span
class="toc-section-number">6.3.5</span> Importance sampling</a></li>
</ul></li>
<li><a href="#comparativa-con-in-one-weekend"><span
class="toc-section-number">6.4</span> Comparativa con In One Weekend</a>
<ul>
<li><a href="#sobre-la-implementación-de-in-one-weekend"><span
class="toc-section-number">6.4.1</span> Sobre la implementación de In
One Weekend</a></li>
<li><a href="#tiempos-de-renderizado"><span
class="toc-section-number">6.4.2</span> Tiempos de renderizado</a>
<ul>
<li><a href="#por-número-de-muestras"><span
class="toc-section-number">6.4.2.1</span> Por número de
muestras</a></li>
<li><a href="#por-presupuesto-de-tiempo"><span
class="toc-section-number">6.4.2.2</span> Por presupuesto de
tiempo</a></li>
<li><a href="#conclusiones-de-la-comparativa"><span
class="toc-section-number">6.4.2.3</span> Conclusiones de la
comparativa</a></li>
</ul></li>
</ul></li>
</ul></li>
<li><a href="#conclusiones"><span class="toc-section-number">7</span>
Conclusiones</a>
<ul>
<li><a href="#posibles-mejoras"><span
class="toc-section-number">7.1</span> Posibles mejoras</a>
<ul>
<li><a href="#interfaces"><span class="toc-section-number">7.1.1</span>
Interfaces</a></li>
<li><a href="#nuevas-técnicas-de-reducción-de-ruido"><span
class="toc-section-number">7.1.2</span> Nuevas técnicas de reducción de
ruido</a></li>
</ul></li>
</ul></li>
<li><a href="#el-presente-y-futuro-de-rt"><span
class="toc-section-number">8</span> El presente y futuro de RT</a>
<ul>
<li><a href="#denoising"><span class="toc-section-number">8.1</span>
Denoising</a></li>
<li><a href="#filtering"><span class="toc-section-number">8.2</span>
Filtering</a></li>
<li><a href="#offline-renderers"><span
class="toc-section-number">8.3</span> Offline renderers</a></li>
<li><a href="#la-industria-del-videojuego"><span
class="toc-section-number">8.4</span> La industria del videojuego</a>
<ul>
<li><a href="#ray-tracing-híbrido"><span
class="toc-section-number">8.4.1</span> Ray tracing híbrido</a></li>
<li><a href="#productos-comerciales"><span
class="toc-section-number">8.4.2</span> Productos comerciales</a></li>
<li><a href="#unreal-engine-5"><span
class="toc-section-number">8.4.3</span> Unreal Engine 5</a></li>
<li><a href="#la-última-generación-de-consolas"><span
class="toc-section-number">8.4.4</span> La última generación de
consolas</a></li>
</ul></li>
<li><a href="#posibles-mejoras-del-trabajo"><span
class="toc-section-number">8.5</span> Posibles mejoras del trabajo</a>
<ul>
<li><a href="#blue-noise"><span
class="toc-section-number">8.5.0.1</span> Blue noise</a></li>
<li><a href="#forced-random-sampling"><span
class="toc-section-number">8.5.0.2</span> Forced random
sampling</a></li>
<li><a href="#sampling-importance-resampling"><span
class="toc-section-number">8.5.0.3</span> Sampling importance
resampling</a></li>
<li><a href="#low-discrepancy-sampling"><span
class="toc-section-number">8.5.0.4</span> Low discrepancy
sampling</a></li>
</ul></li>
</ul></li>
<li><a href="#metodología-de-trabajo"><span
class="toc-section-number">9</span> Metodología de trabajo</a>
<ul>
<li><a href="#influencias"><span class="toc-section-number">9.1</span>
Influencias</a></li>
<li><a href="#ciclos-de-desarrollo"><span
class="toc-section-number">9.2</span> Ciclos de desarrollo</a></li>
<li><a href="#presupuesto"><span class="toc-section-number">9.3</span>
Presupuesto</a></li>
<li><a href="#arquitectura-del-software"><span
class="toc-section-number">9.4</span> Arquitectura del software</a></li>
<li><a href="#diseño"><span class="toc-section-number">9.5</span>
Diseño</a>
<ul>
<li><a href="#bases-del-diseño"><span
class="toc-section-number">9.5.1</span> Bases del diseño</a></li>
<li><a href="#tipografías"><span class="toc-section-number">9.5.2</span>
Tipografías</a></li>
<li><a href="#paleta-de-colores"><span
class="toc-section-number">9.5.3</span> Paleta de colores</a></li>
</ul></li>
<li><a href="#flujo-de-trabajo-y-herramientas"><span
class="toc-section-number">9.6</span> Flujo de trabajo y
herramientas</a>
<ul>
<li><a href="#pandoc"><span class="toc-section-number">9.6.1</span>
Pandoc</a></li>
<li><a href="#figma"><span class="toc-section-number">9.6.2</span>
Figma</a></li>
<li><a href="#otros-programas"><span
class="toc-section-number">9.6.3</span> Otros programas</a></li>
</ul></li>
<li><a href="#github"><span class="toc-section-number">9.7</span>
Github</a>
<ul>
<li><a
href="#integración-continua-con-github-actions-y-github-pages"><span
class="toc-section-number">9.7.1</span> Integración continua con Github
Actions y Github Pages</a></li>
<li><a href="#issues-y-github-projects"><span
class="toc-section-number">9.7.2</span> Issues y Github
Projects</a></li>
<li><a href="#estilo-de-commits"><span
class="toc-section-number">9.7.3</span> Estilo de commits</a></li>
</ul></li>
</ul></li>
<li><a href="#glosario-de-términos"><span
class="toc-section-number">10</span> Glosario de términos</a>
<ul>
<li><a href="#notación"><span class="toc-section-number">10.1</span>
Notación</a></li>
<li><a href="#radiometría"><span class="toc-section-number">10.2</span>
<span>Radiometría</span></a></li>
</ul></li>
<li><a href="#bibliografía">Bibliografía</a></li>
</ul>
</nav>
<h1 class="unnumbered" id="abstract">Abstract</h1>
<p>En este trabajo se explorarán las técnicas modernas de informática
gráfica físicamente realistas basadas en <em>ray tracing</em> en tiempo
real. Para ello, se utilizarán métodos de integración de Monte Carlo con
el fin de disminuir el tiempo de cómputo.</p>
<p>Se diseñará un software basado en la interfaz de programación de
aplicaciones gráficas Vulkan, utilizando como soporte un entorno de
desarrollo de Nvidia conocido como nvpro-samples. El software
implementará un motor gráfico basado en <em>path tracing</em>. Este
motor será capaz de renderizar numerosas escenas, cambiar los parámetros
del algoritmo path tracing y modificar las fuentes de iluminación en
tiempo de ejecución.</p>
<p>Con el fin de explorar cómo afectan diferentes métodos al ruido final
de la imagen, se estudiarán algunas técnicas de reducción de varianza
como muestreo directo de fuentes de iluminación, muestreo por
importancia o acumulación temporal. Además, el motor desarrollado se
comparará con una implementación puramente en CPU basada en el software
creado en los libros de <span class="citation"
data-cites="Shirley2020RTW1">(<a href="#ref-Shirley2020RTW1"
role="doc-biblioref">Shirley 2020a</a>)</span> “Ray Tracing in One
Weekend series”. Se comporbarán las diferencias entre ambas versiones,
estudiando los puntos fuertes de cada una.</p>
<p><em>Palabras clave: raytracing, ray tracing, path tracing, métodos de
Monte Carlo, integración de Monte Carlo, transporte de luz, iluminación
global, Vulkan.</em></p>
<hr>
<h1 class="unnumbered" id="a-brief-overview">A brief overview</h1>
<blockquote>
<p>TODO</p>
</blockquote>
<p><em>Keywords: raytracing, ray tracing, path tracing, Monte Carlo
methods, Monte Carlo integration, light transport, global illumination,
Vulkan.</em></p>
<h1 class="unnumbered" id="dedicatoria">Dedicatoria</h1>
<p>¡Parece que has llegado un poco pronto! Si lo has hecho
voluntariamente, ¡muchas gracias! Este proyecto debería estar finalizado
en verano de 2022. Mientras tanto, actualizaré poco a poco el contenido.
Si quieres ir comprobando los progresos, puedes visitar <a
href="github.com/Asmilex/Raytracing">Asmilex/Raytracing</a> en Github
para ver el estado del desarrollo.</p>
<p>Aun así, hay mucha gente que me ha ayudado a sacar este proyecto
hacia delante.</p>
<p>Gracias, en primer lugar, a mi familia por permitirme acabar la
carrera. A Blanca, Cristina, Jorge, Jose OC, Lucas, Mari, Marina,
Mapachana y Paula, Sergio por ayudarme con el contenido, feedback del
desarrollo y guía de diseño.</p>
<h1 data-number="1" id="introducción"><span
class="header-section-number">1</span> Introducción</h1>
<p>Este trabajo puede visualizarse en la web <a
href="https://asmilex.github.io/Raytracing/">asmilex.github.io/Raytracing</a>
o en el <a
href="https://github.com/Asmilex/Raytracing/raw/main/docs/TFG.pdf">PDF</a>
disponible en el repositorio del trabajo <a
href="https://github.com/Asmilex/Raytracing">Asmilex/Raytracing</a>.</p>
<p>La página web contiene la versión más actualizada, además de recursos
adicionales como vídeos.</p>
<h2 data-number="1.1" id="nota-histórica"><span
class="header-section-number">1.1</span> Nota histórica</h2>
<p>Ser capaces de capturar un momento.</p>
<p>Desde siempre, este ha sido uno de los sueños de la humanidad. La
capacidad de retener lo que ven nuestros ojos comenzó con simples
pinturas ruprestres. Con el tiempo, el arte evolucionó, así como la
capacidad de retratar nuestra percepción con mayor fidelidad.</p>
<p>A inicios del siglo XVIII, se caputaron las primeras imágenes con una
cámara gracias a Nicéphore Niépce. Sería una imagen primitiva, claro;
pero era funcional. Gracias a la compañía Kodak, la fotografía se
extendió al consumidor rápidamente sobre 1890. Más tarde llegaría la
fotografía digital, la cual simplificaría muchos de los problemas de las
cámaras tradicionales.</p>
<p>Hablando de digital. Los ordenadores personales modernos nacieron
unos años más tarde. Los usuarios eran capaces de mostrar imágenes en
pantalla, que cambiaban bajo demanda. Y, entonces, nos hicimos una
pregunta…</p>
<p>¿Podríamos <strong>simular la vida real</strong> para mostrarla en
pantalla?</p>
<p>Como era de esperar, esto es complicado de lograr. Para conseguirlo,
hemos necesitado crear abstracciones de conceptos que nos resultan
naturales, como objetos, luces y seres vivos. <em>“Cosas”</em> que un
ordenador no entiende, y sin embargo, para nosotros
<em>funcionan</em>.</p>
<p>Así, nació la geometría, los puntos de luces, texturas, sombreados, y
otros elementos de un escenario digital. Pero, por muchas abstracciones
elegantes que tengamos, no nos basta. Necesitamos visualizarlas. Y como
podemos imaginarnos, esto es un proceso costoso.</p>
<p>La <strong>rasterización</strong> es el proceso mediante el cual
estos objetos tridimensionales se transforman en bidimensionales.
Proyectando acordemente el entorno a una cámara, conseguimos colorear un
pixel, de forma que represente lo que se ve en ese mundo.</p>
<blockquote>
<p>TODO insertar imagen rasterización.</p>
<p>NOTE ¿quizás debería extender un poco más esta parte? Parece que se
queda algo coja la explicación.</p>
</blockquote>
<p>Aunque esta técnica es bastante eficiente en términos de computación
y ha evolucionado mucho, rápidamente saturamos sus posibilidades.
Conceptos como <em>shadow maps</em>, <em>baked lightning</em>, o
<em>reflection cubemaps</em> intentan solventar lo que no es posible con
rasterización: preguntrarnos <em>qué es lo que se encuentra alrededor
nuestra</em>.</p>
<p>En parte, nos olvidamos de la intuitiva realidad, para centrarnos en
aquello computacionalmente viable.</p>
<p>Y, entonces, en 1960 el trazado de rayos con una simple idea
intuitiva.</p>
<h2 data-number="1.2" id="qué-es-ray-tracing"><span
class="header-section-number">1.2</span> ¿Qué es ray tracing?</h2>
<p>En resumidas cuentas, <em>ray tracing</em> (o trazado de rayos en
español), se basa en disparar fotones en forma de rayo desde nuestra
cámara digital y hacerlos rebotar en la escena.</p>
<p>De esta forma, simulamos cómo se comporta la luz. Al impactar en un
objeto, sufre un cambio en su trayectoria. Este cambio origina nuevos
rayos, que vuelven a dispersarse por la escena. Estos nuevos rayos
dependerán de las propiedades del objeto con el que hayan impactado. Con
el tiempo necesario, lo que veremos desde nuestra cámara será una
representación fotorealista de lo que habita en ese universo.</p>
<p>Esta técnica, tan estúpidamente intuitiva, se ha hecho famosa por su
simpleza y su elegancia. <em>Pues claro</em> que la respuesta a
“<em>¿Cómo simulamos fielmente una imagen en un ordenador?</em>” es
“<em>Representando la luz de forma realista</em>”.</p>
<p>Aunque, quizás intuitiva no sea la palabra. Podemos llamarla
<em>natural</em>, eso sí. A fin de cuentas, fue a partir del siglo XVIII
cuando empezamos a entender que podíamos capturar la luz. Nuestros
antepasados tenían teorías, pero no podían explicar por qué
<em>veíamos</em> el mundo.</p>
<p>Ahora sí que sabemos cómo funciona. Entendiendo el por qué lo hace
nos permitirá programarlo. Y, resulta que funciona impresionantemente
bien.</p>
<p>Atrás se quedan los <em>hacks</em> necesarios para rasterización. Los
cubemaps no son esenciales para los reflejos, y no necesitamos cámaras
virtuales para calcular sombras. Ray tracing permite simular fácilmente
efectos como reflejos, refracción, desenfoque de movimiento, aberración
cromática… Incluso fenómenos físicos propios de las particulas y las
ondas.</p>
<blockquote>
<p>Espera. Si tan bueno es, ¿por qué no lo usamos en todos lados?</p>
</blockquote>
<p>Por desgracia, el elefante en la sala es el rendimiento. Como era de
esperar, disparar rayos a diestro y siniestro es costoso. <strong>Muy
costoso</strong>.</p>
<p>A diferencia del universo, nosotros no nos podemos permitir el lujo
de usar fotones de tamaño infinitesimal y dispersiones casi infinitas.
Nos pasaríamos una eternidad esperando. Y para ver una imagen en nuestra
pantalla necesitaremos estar vivos, claro.</p>
<p>Debemos evitar la fuerza bruta. Dado que la idea es tan elegante, la
respuesta no está en el <em>“qué”</em>, sino en el <em>“cómo”</em>. Si
<strong>disparamos y dispersamos rayos con cabeza</strong> seremos
capaces de obtener lo que buscamos en un tiempo razonable.</p>
<p>Hace unos años, al hablar de tiempo razonable, nos referiríamos a
horas. Quizás días. Producir un <em>frame</em> podría suponer una
cantidad de tiempo impensable para un ordenador de consumidor. Hoy en
día también ocurre esto, claro está. Pero la tecnología evoluciona.</p>
<p>Podemos bajarlo a milisegundos.</p>
<p>Hemos entrado en la era del <strong>real time ray
tracing</strong>.</p>
<h2 data-number="1.3" id="objetivos-del-trabajo"><span
class="header-section-number">1.3</span> Objetivos del trabajo</h2>
<p>Los objetivos del trabajo iniciales son los siguientes:</p>
<ul>
<li>Análisis de los algoritmos modernos de visualización en 3D basados
en métodos de Monte Carlo.</li>
<li>Revisión de las técnicas de Monte Carlo, examinando puntos fuertes y
débiles de cada una. Se busca minimizar el error en la reconstrucción de
la imagen y minimizar el tiempo de ejecución.</li>
<li>Implementación de dichos algoritmos en hardware gráfico moderno
(GPUs) específicamente diseñado para aceleración de ray tracing.</li>
<li>Diseño e implementación de un software de síntesis de imágenes
realistas por path tracing y muestreo directo de fuentes de luz por
GPU.</li>
<li>Análisis del rendimiento del motor con respecto al tiempo de
ejecución y calidad de imagen.</li>
<li>Comparación del motor desarrollado con una implementación por
CPU.</li>
<li>Investigación de las técnicas modernas y sobre el futuro del
área.</li>
</ul>
<blockquote>
<p>TODO: determinar si lo siguiente es cierto.</p>
</blockquote>
<p>Afortunadamente, <strong>se ha conseguido realizar exitosamente cada
uno de los objetivos</strong>. Esta memoria cubrirá todo el trabajo que
ha sido necesario realizar para lograrlo.</p>
<h2 data-number="1.4" id="técnicas-empleadas-para-la-resolución"><span
class="header-section-number">1.4</span> Técnicas empleadas para la
resolución</h2>
<blockquote>
<p>TODO: echarle un ojo a esto cuando termine el trabajo.</p>
</blockquote>
<p>Además del antedicho algoritmo ray tracing y su versión más pura path
tracing, se han empleado técnicas de Monte Carlo para calcular la luz
resultante de un punto.</p>
<p>En particular, con respecto a la <a
href="#integración-de-monte-carlo">matemática</a> empleada, estudiaremos
diferentes formas de generar números aleatorios mediante distribuciones
particulares, <em>(multiple) importance sampling</em>, next event
estimation, …</p>
<p>En un área híbrida se encuentra la <a
href="#transporte-de-luz">radiometría</a>. Dado que estamos tratando con
transporte de luz, será esencial introducir los conceptos más
importantes de la radiometría. Trataremos con algunos términos como
irradiancia, ángulos sólidos, radiancia, funciones de distribuciones de
reflectancia y transmitancia bidireccionales, etc.</p>
<p>Finalmente, la parte <a
href="#construyamos-un-path-tracer">informática</a> usará en la API
gráfica Vulkan junto a un framework de Nvidia para acelerar la adopción
de ray tracing en KHR. Veremos qué se necesita para implementar ray
tracing en tiempo real, lo que nos llevará aprender sobre programación
en Vulkan, las estructuras de aceleración de nivel alto y bajo (TLAS y
BLAS), la Shader Binding Table, comunicación con CPU y GPU, etc.</p>
<p>Todo este programa estará alojado en Github. En el <a
href="#metodología-de-trabajo">apéndice</a>, aprenderemos cómo se ha
usado la plataforma para integrar la documentación, el código fuente y
los ciclos de desarrollo.</p>
<p>Como podemos ver, esta área relaciona íntimamente la matemática y la
informática, con un poco de física de por medio.</p>
<h2 data-number="1.5" id="principales-fuentes-consultadas"><span
class="header-section-number">1.5</span> Principales fuentes
consultadas</h2>
<p>Esencialmente, este trabajo ha sido posible gracias a los siguientes
recursos:</p>
<ul>
<li>La serie de libros de <em>Ray Tracing</em> de <em>Peter
Shirley</em>, conocidos como “Ray tracing In One Weekend Series” <span
class="citation" data-cites="Shirley2020RTW1">(<a
href="#ref-Shirley2020RTW1" role="doc-biblioref">Shirley
2020a</a>)</span>, <span class="citation"
data-cites="Shirley2020RTW2">(<a href="#ref-Shirley2020RTW2"
role="doc-biblioref">Shirley 2020b</a>)</span>, <span class="citation"
data-cites="Shirley2020RTW3">(<a href="#ref-Shirley2020RTW3"
role="doc-biblioref">Shirley 2020c</a>)</span>. El motor desarrollado en
estos libros es el que se utilizará para la comparación.</li>
<li>Physically Based Rendering: From Theory to Implementation (3rd ed.)
<span class="citation" data-cites="PBRT3e">(<a href="#ref-PBRT3e"
role="doc-biblioref">Pharr, Jakob, and Humphreys 2016</a>)</span>,
considerado como el santo grial de la informática gráfica moderna.</li>
<li>Ray Tracing Gems I y II <span class="citation"
data-cites="Haines2019">(<a href="#ref-Haines2019"
role="doc-biblioref">Haines and Akenine-Möller 2019</a>)</span>, <span
class="citation" data-cites="Marrs2021">(<a href="#ref-Marrs2021"
role="doc-biblioref">Adam Marrs and Wald 2021</a>)</span>, una colección
de papers esenciales sobre ray tracing publicada por Nvidia.</li>
<li>El autor <a href="https://users.cg.tuwien.ac.at/zsolnai/">Károly
Zsolnai, de Two Minute Papers</a>. No solo ha inspirado parte del
trabajo, sino que su curso sobre transporte de luz de la <a
href="https://www.cg.tuwien.ac.at/courses/Rendering/VU.SS2019.html">universidad
de Austria</a> ha sido una gran fuente de información para el
trabajo.</li>
</ul>
<blockquote>
<p>TODO: tengo que ver exactamente cómo cito esa fuente anterior.</p>
</blockquote>
<hr>
<h2 class="unlisted unnumbered" id="referencias">Referencias</h2>
<p><span class="citation" data-cites="wikipedia-contributors-2022A">(<a
href="#ref-wikipedia-contributors-2022A" role="doc-biblioref">Wikipedia:
history of photography 2022</a>)</span>, <span class="citation"
data-cites="wikipedia-contributors-2022B">(<a
href="#ref-wikipedia-contributors-2022B" role="doc-biblioref">Wikipedia:
Kodak 2022</a>)</span>, <span class="citation"
data-cites="wikipedia-contributors-2022C">(<a
href="#ref-wikipedia-contributors-2022C" role="doc-biblioref">Wikipedia:
Computer 2022</a>)</span>, <span class="citation"
data-cites="wikipedia-contributors-2022D">(<a
href="#ref-wikipedia-contributors-2022D" role="doc-biblioref">Wikipedia:
rendering (computer graphics) 2022</a>)</span>, <span class="citation"
data-cites="caulfield-2020">(<a href="#ref-caulfield-2020"
role="doc-biblioref">Caulfield 2020</a>)</span>, <span class="citation"
data-cites="wikipedia-contributors-2022E">(<a
href="#ref-wikipedia-contributors-2022E" role="doc-biblioref">tracing
2022</a>)</span>, <span class="citation"
data-cites="unknown-author-no-date">(<a
href="#ref-unknown-author-no-date"
role="doc-biblioref"><span>“Rendering”</span> n.d.</a>)</span>, <span
class="citation" data-cites="Haines2019">(<a href="#ref-Haines2019"
role="doc-biblioref">Haines and Akenine-Möller 2019</a>)</span></p>
<ul>
<li>https://www.cg.tuwien.ac.at/courses/Rendering/VU.SS2019.html</li>
</ul>
<h1 data-number="2" id="las-bases"><span
class="header-section-number">2</span> Las bases</h1>
<p>Empecemos por definir lo que es un rayo.</p>
<p>Un rayo <span class="citation" data-cites="Shirley2020RTW1">(<a
href="#ref-Shirley2020RTW1" role="doc-biblioref">Shirley
2020a</a>)</span> es una función <span class="math inline">\(P(t) = O +
tD\)</span>, donde <span class="math inline">\(O\)</span> es el origin,
<span class="math inline">\(D\)</span> la dirección, y <span
class="math inline">\(t \in \mathbb{R}\)</span>. Podemos considerarlo
una interpolación entre dos puntos en el espacio, donde <span
class="math inline">\(t\)</span> controla la posición en la que nos
encontramos.</p>
<p>Por ejemplo, si <span class="math inline">\(t = 0\)</span>,
obtendremos el origen. Si <span class="math inline">\(t = 1\)</span>,
obtendremos el punto correspondiente a la dirección. Usando valores
negativos vamos <em>hacia atrás</em>.</p>
<div id="fig:rayo_basico" class="fignos">
<figure>
<img loading="lazy" src="./img/01/Rayo%20básico.png" style="width:70.0%"
alt="Figura 1: El parámetro t nos permite controlar los puntos del rayo" />
<figcaption aria-hidden="true"><span>Figura 1:</span> El parámetro <span
class="math inline">\(t\)</span> nos permite controlar los puntos del
rayo</figcaption>
</figure>
</div>
<p>Dado que estos puntos estarán generalmente en <span
class="math inline">\(\mathbb{R}^3\)</span>, podemos escribirlo como</p>
<p><span class="math display">\[
P(t) = (O_x, O_y, O_z) + t (D_x, D_y, D_z)
\]</span></p>
<p>Estos rayos los <em>dispararemos</em> a través de una cámara virtual,
que estará enfocando a la escena. De esta forma, los haremos rebotar con
los objetos que se encuentren en el camino del rayo. A este proceso lo
llamaremos <strong>ray casting</strong>.</p>
<div id="fig:ray_casting" class="fignos">
<figure>
<img loading="lazy" src="./img/01/Ray%20casting.png" style="width:80.0%"
alt="Figura 2: Diagrama de ray casting" />
<figcaption aria-hidden="true"><span>Figura 2:</span> Diagrama de ray
casting</figcaption>
</figure>
</div>
<p>Generalmente, nos quedaremos con el primer objeto que nos encontremos
en su camino. Aunque, a veces, nos interesará saber todos con los que se
encuentre.</p>
<p>Cuando un rayo impacta con un objeto, adquirirá parte de las
propiedades lumínicas del punto de impacto. Por ejemplo, cuánta luz
proporciona la lámpara que tiene encima la esfera de la figura
anterior.</p>
<p>Una vez recojamos la información que nos interese, aplicaremos otro
raycast desde el nuevo punto de impacto, escogiendo una nueva dirección
determinada. Esta dirección dependerá del tipo de material del objeto.
Y, de hecho, algunos serán capaces de invocar varios rayos.</p>
<p>Por ejemplo, los espejos reflejan la luz casi de forma perfecta;
mientras que otros elementos como el agua o el cristal reflejan
<em>y</em> refractan luz, así que necesitaremos generar dos nuevos
raycast.</p>
<p>Usando suficientes rayos obtendremos la imagen de la escena. A este
proceso de <strong>ray casting recursivo</strong> es lo que se conoce
como ray tracing.</p>
<p>Como este proceso puede continuar indefinidamente, tendremos que
controlar la profundidad de la recursión. A mayor profundidad, mayor
calidad de imagen; pero también, mayor tiempo de ejecución.</p>
<h2 data-number="2.1" id="eligiendo-direcciones"><span
class="header-section-number">2.1</span> Eligiendo direcciones</h2>
<p>Una de las partes más importantes de ray tracing, y a la que quizás
dedicaremos más tiempo, es a la elección de la dirección.</p>
<p>Hay varios factores que entran en juego a la hora de decidir qué
hacemos cuando impactamos con un nuevo objeto:</p>
<ol type="1">
<li><strong>¿Cómo es la superficie del material?</strong> A mayor
rugosidad, mayor aleatoriedad en la dirección. Por ejemplo, no es lo
mismo el asfalto de una carretera que una lámina de aluminio
impecable.</li>
<li><strong>¿Cómo de fiel es nuestra geometría?</strong></li>
<li><strong>¿Dónde se encuentran las luces en la escena?</strong>
Dependiendo de la posición, nos interesará muestrear la luz con mayor
influencia.</li>
</ol>
<p>Estas cuestiones las exploraremos a fondo en las siguientes
secciones.</p>
<h2 data-number="2.2" id="intersecciones-rayo---objeto"><span
class="header-section-number">2.2</span> Intersecciones rayo -
objeto</h2>
<p>Como dijimos al principio del capítulo, representaremos un rayo
como</p>
<p><span class="math display">\[
\begin{aligned}
P(t) &amp; = (O_x, O_y, O_z) + t (D_x, D_y, D_z) = \\
&amp; = (O_x + t D_x, O_y + t D_y, O_y + t D_z)
\end{aligned}
\]</span></p>
<p>Por ejemplo, tomando <span class="math inline">\(O = (1, 3, 2), D =
(1, 2, 1)\)</span>:</p>
<ul>
<li>Para <span class="math inline">\(t = 0\)</span>, <span
class="math inline">\(P(t) = (1, 3, 2)\)</span>.</li>
<li>Para <span class="math inline">\(t = 1\)</span>, <span
class="math inline">\(P(t) = (1, 3, 2) + (1, 2, 1) = (2, 5,
3)\)</span>.</li>
</ul>
<p>Nos resultará especialmente útil limitar los valores que puede tomar
<span class="math inline">\(t\)</span>. Restringiremos los posibles
puntos del dominio de forma que <span class="math inline">\(t \in
[t_{min}, t_{max})\)</span>, con <span class="math inline">\(t_{min}
&lt; t_{max}\)</span>. En general, nos interesará separarnos de las
superficies un pequeño pero no despreciable <span
class="math inline">\(\varepsilon\)</span> para evitar errores de
redondeo.</p>
<div id="fig:limites_rayo" class="fignos">
<figure>
<img loading="lazy" src="./img/01/Límites%20de%20un%20rayo.png"
alt="Figura 3: Separarnos un poquito del origen evitará errores de coma flotante" />
<figcaption aria-hidden="true"><span>Figura 3:</span> Separarnos un
poquito del origen evitará errores de coma flotante</figcaption>
</figure>
</div>
<p>Una de las principales cuestiones que debemos hacernos es saber
cuándo un rayo impacta con una superficie. Lo definiremos
analíticamente.</p>
<h3 data-number="2.2.1" id="superficies-implícitas"><span
class="header-section-number">2.2.1</span> Superficies implícitas</h3>
<p>Generalmente, cuando hablemos de superficies, nos referiremos
superficies diferenciables <span class="citation"
data-cites="wikipedia-contributors-2022O">(<a
href="#ref-wikipedia-contributors-2022O" role="doc-biblioref">Wikipedia:
Differential geometry of surfaces 2022</a>)</span>, pues nos interesará
conocer el vector normal en cada punto.</p>
<p>Una superficie implícita es una superficie en un espacio euclidiano
definida como</p>
<p><span class="math display">\[
F(x, y, z) = 0
\]</span></p>
<p>Esta ecuación implícita define una serie de puntos del espacio <span
class="math inline">\(\mathbb{R}^3\)</span> que se encuentran en la
superficie.</p>
<p>Por ejemplo, la esfera se define como <span class="math inline">\(x^2
+ y^2 + z^2 - 1 = 0\)</span>.</p>
<p>Consideremos una superficie <span class="math inline">\(S\)</span> y
un punto regular de ella <span class="math inline">\(P\)</span>; es
decir, un punto tal que el gradiente de <span
class="math inline">\(F\)</span> en <span
class="math inline">\(P\)</span> no es 0. Se define el vector normal
<span class="math inline">\(\mathbf{n}\)</span> a la superficie en ese
punto como</p>
<p><span class="math display">\[
\mathbf{n} = \nabla F(P) = \left( \frac{\partial F(P)}{\partial x},
\frac{\partial F(P)}{\partial y}, \frac{\partial F(P)}{\partial z}\right
)
\]</span></p>
<blockquote>
<p>TODO: dibujo de la normal a una superficie.</p>
</blockquote>
<p>Dado un punto <span class="math inline">\(Q \in
\mathbb{R}^3\)</span>, queremos saber dónde interseca un rayo <span
class="math inline">\(P(t)\)</span>. Es decir, para qué <span
class="math inline">\(t\)</span> se cumple que <span
class="math inline">\(F(P(t)) = 0 \iff F(O + tD) = 0\)</span>.</p>
<p>Consideremos ahora un plano. Para ello, nos tomamos un punto <span
class="math inline">\(Q_0\)</span> del plano y un vector normal a la
superficie <span class="math inline">\(\mathbf{n}\)</span>. La ecuación
implícita del plano será <span class="citation"
data-cites="ShirleyRRT">(<a href="#ref-ShirleyRRT"
role="doc-biblioref">Shirley and Morley 2003</a>)</span></p>
<p><span class="math display">\[
F(Q) = (Q - Q_0) \cdot \mathbf{n} = 0
\]</span></p>
<p>Si pinchamos nuestro rayo en la ecuación,</p>
<p><span class="math display">\[
\begin{aligned}
F(P(t)) &amp; = (P(t) - Q_0) \cdot \mathbf{n} \\
        &amp; = (O + tD - Q_0) \cdot \mathbf{n} = 0 \\
\end{aligned}
\]</span></p>
<p>Resolviendo para <span class="math inline">\(t\)</span>, esto se da
si</p>
<p><span class="math display">\[
\begin{aligned}
O \cdot \mathbf{n} + tD \cdot \mathbf{n} - Q_0 \cdot \mathbf{n} &amp; =
0 &amp; \iff \\
tD \cdot \mathbf{n} &amp; = Q_0 \cdot \mathbf{n} - O \cdot \mathbf{n}
&amp; \iff \\
t &amp; = \frac{Q_0 \cdot \mathbf{n} - O \cdot \mathbf{n}}{D \cdot
\mathbf{n}}
\end{aligned}
\]</span></p>
<p>Es decir, hemos obtenido el único valor de <span
class="math inline">\(t\)</span> para el cual el rayo toca la
superficie.</p>
<p>Debemos tener en cuenta el caso para el cual <span
class="math inline">\(D \cdot \mathbf{n} = 0\)</span>. Esto solo se da
si la dirección y el vector normal a la superficie son paralelos.</p>
<blockquote>
<p>TODO: dibujo de dos rayos con un plano: uno corta a la superficie,
mientras que el otro es paralelo.</p>
</blockquote>
<h3 data-number="2.2.2" id="superficies-paramétricas"><span
class="header-section-number">2.2.2</span> Superficies paramétricas</h3>
<p>Otra forma de definir una superficie en el espacio es mediante un
subconjunto <span class="math inline">\(D \subset \mathbb{R}^2\)</span>
y una serie de funciones, <span class="math inline">\(f, g, h: D
\rightarrow \mathbb{R}^3\)</span>, de forma que</p>
<p><span class="math display">\[
(x, y, z) = \left( f(u, v), g(u, v), h(u, v) \right) \\
\]</span></p>
<blockquote>
<p>En informática gráfica, hacemos algo similar cuando mapeamos una
textura a una superficie. Se conoce como <strong>UV
mapping</strong>.</p>
</blockquote>
<p>Demos un par de ejemplos de superficies paramétricas: - El grafo de
una función <span class="math inline">\(f: D \rightarrow
\mathbb{R}^3\)</span>, <span class="math display">\[
G(f) = \left\{(x, y, f(x, y)) \,\middle|\,  (x, y) \in D\right\}
\]</span> define una superficie diferenciable siempre que <span
class="math inline">\(f\)</span> también lo sea. - Usando coordenadas
esféricas <span class="math inline">\((r, \theta, \phi)\)</span>,
podemos parametrizar la esfera como <span class="math inline">\((x, y,
z) = (\cos\phi\sin\theta, \sin\phi\sin\theta, \cos\theta)\)</span></p>
<blockquote>
<p>TODO añadir imagen de coordenadas esféricas. U otro capítulo con
coordenadas.</p>
<p>NOTE: estoy usando (radial, polar, azimuthal). <span
class="math inline">\(\theta\)</span> corresponde con la apertura con
respecto a la vertical</p>
</blockquote>
<p>El vector normal <span class="math inline">\(\mathbf{n}\)</span> a la
superficie en un punto <span class="math inline">\((u, v)\)</span> del
dominio viene dado por</p>
<p><span class="math display">\[
\mathbf{n}(u, v) =
        \left( \frac{\partial f}{\partial u}, \frac{\partial g}{\partial
u}, \frac{\partial h}{\partial u} \right)
                \times
        \left( \frac{\partial f}{\partial v}, \frac{\partial g}{\partial
v}, \frac{\partial h}{\partial v} \right)
\]</span></p>
<p>Encontrar el punto de intersección de una superficie paramétrica con
un rayo es sencillo. Basta con encontrar aquellos puntos <span
class="math inline">\((u, v)\)</span> y <span
class="math inline">\(t\)</span> para los que</p>
<p><span class="math display">\[
\begin{aligned}
O_x + tD_x &amp; = f(u, v) \\
O_y + tD_y &amp; = g(u, v) \\
O_z + tD_z &amp; = h(u, v) \\
\end{aligned}
\]</span></p>
<p>Es posible que el rayo no impacte en ningún punto. En ese caso, el
sistema de ecuaciones no tendría solución. Otra posibilidad es que
intersequen en varios puntos.</p>
<h3 data-number="2.2.3" id="intersecciones-con-esferas"><span
class="header-section-number">2.2.3</span> Intersecciones con
esferas</h3>
<p>Estudiemos ahora cómo intersecan una esfera con nuestro rayo. Una
esfera de centro <span class="math inline">\(C\)</span> y radio <span
class="math inline">\(r\)</span> viene dada por aquellos puntos <span
class="math inline">\(P = (x, y, z)\)</span> que cumplen</p>
<p><span class="math display">\[
(P - C) \cdot (P - C) = r^2
\]</span></p>
<p>Podemos reescribir esta ecuación en términos de sus coordenadas para
obtener</p>
<p><span class="math display">\[
(x - C_x)^2 + (y - C_y)^2 + (z - C_z)^2 = r^2
\]</span></p>
<p>Veamos para qué valores de <span class="math inline">\(t\)</span> de
nuestro rayo se cumple esa ecuación:</p>
<p><span class="math display">\[
\begin{aligned}
(P(t) - C) \cdot (P(t) - C) &amp; = r^2 &amp; \iff \\
(O + tD - C) \cdot (O + tD - C) &amp; = r^2 &amp; \iff \\
\end{aligned}
\]</span></p>
<p>Aplicando las propiedades del producto escalar de la conmutatividad
(<span class="math inline">\(a \cdot b = b \cdot a\)</span>) y la
distributiva (<span class="math inline">\(a \cdot (b + c) = a \cdot b +
a \cdot c\)</span>), podemos escribir</p>
<p><span class="math display">\[
\begin{aligned}
((O - C) + tD) \cdot ((O - C) + tD) &amp; = r^2 &amp; \iff \\
(O - C)^2 + 2 \cdot (O - C) \cdot tD + (tD)^2 &amp; = r^2 &amp; \iff \\
D^2t^2 + 2 D \cdot (O - C)t + (O - C)^2 - r^2 &amp; = 0 &amp; \iff \\
\end{aligned}
\]</span></p>
<p>Así que tenemos una ecuación de segundo grado. Resolviéndola, nos
salen nuestros puntos de intersección:</p>
<p><span class="math display">\[
t = \frac{
    - D \cdot (O - C) \pm \sqrt{(D \cdot (O - C))^2 - 4 (D^2)((O - C)^2
- r^2)}
}{
    2 D^2
}
\]</span></p>
<p>Debemos distinguir tres casos, atiendiendo al valor que toma el
discriminante <span class="math inline">\(\Delta = \small{(D \cdot (O -
C))^2 - 4 (D^2)((O - C)^2 - r^2)}\)</span>:</p>
<ol type="1">
<li>Si <span class="math inline">\(\Delta &lt; 0\)</span>, <span
class="math inline">\(\sqrt{\Delta} \notin \mathbb{R}\)</span>, y el
rayo no impacta con la esfera</li>
<li>Si <span class="math inline">\(\Delta = 0\)</span>, el rayo impacta
en un punto, que toma el valor <span class="math inline">\(t = \frac{-D
\cdot (O - C)}{2 D \cdot D}\)</span>. Digamos que <em>pegaría</em> justo
en el borde.</li>
<li>Si <span class="math inline">\(\Delta &gt; 0\)</span>, existen dos
soluciones. En ese caso, el rayo atraviesa la esfera.</li>
</ol>
<div id="fig:interseccion_esfera" class="fignos">
<figure>
<img loading="lazy" src="./img/01/Intersección%20rayo%20-%20esfera.png"
style="width:60.0%"
alt="Figura 4: Puntos de intersección con una esfera." />
<figcaption aria-hidden="true"><span>Figura 4:</span> Puntos de
intersección con una esfera.</figcaption>
</figure>
</div>
<p>Para estos dos últimos, si consideramos <span
class="math inline">\(t_0\)</span> cualquier solución válida, el vector
normal resultante viene dado por</p>
<p><span class="math display">\[
\mathbf{n} = 2 (P(t_0) - C)
\]</span></p>
<p>o, normalizando,</p>
<p><span class="math display">\[
\hat{\mathbf{n}} = \frac{(P(t_0) - C)}{r}
\]</span></p>
<h3 data-number="2.2.4" id="intersecciones-con-triángulos"><span
class="header-section-number">2.2.4</span> Intersecciones con
triángulos</h3>
<p>Este tipo de intersecciones serán las más útiles en nuestro path
tracer. Generalmente, nuestras geometrías estarán compuestas por mallas
de triángulos, así que conocer dónde impacta nuestro rayo será clave.
Empecemos por la base:</p>
<p>Un triángulo viene dado por tres puntos, <span
class="math inline">\(A, B\)</span>, y <span
class="math inline">\(C\)</span>; correspondientes a sus vértices. Para
evitar casos absurdos, supongamos que estos puntos son afinmente
independientes; es decir, que no están alineados.</p>
<h4 data-number="2.2.4.1" id="coordenadas-baricéntricas"><span
class="header-section-number">2.2.4.1</span> Coordenadas
baricéntricas</h4>
<p>Podemos describir los puntos contenidos en el plano que forman estos
vertices mediante <strong>coordenadas baricéntricas</strong>. Este
sistema de coordenadas expresa cada punto del plano como una combinación
convexa de los vértices <span class="citation"
data-cites="wikipedia-contributors-2022G">(<a
href="#ref-wikipedia-contributors-2022G" role="doc-biblioref">Wikipedia:
Barycentric coordinate system 2022</a>)</span>. Es decir, que para cada
punto <span class="math inline">\(P\)</span> del triángulo existen <span
class="math inline">\(\alpha, \beta\)</span> y <span
class="math inline">\(\gamma\)</span> tales que <span
class="math inline">\(\alpha + \beta + \gamma = 1\)</span> y</p>
<p><span class="math display">\[
P = \alpha A + \beta B + \gamma C
\]</span></p>
<blockquote>
<p>TODO: triángulo con coordenadas baricéntricas.</p>
</blockquote>
<p>Debemos destacar que existen dos grados de libertad debido a la
restricción de que las coordenadas sumen 1.</p>
<p>Una propiedad de estas coordenadas que nos puede resultar útil es que
un punto <span class="math inline">\(P\)</span> está contenido en el
triángulo si y solo si <span class="math inline">\(0 &lt; \alpha, \beta,
\gamma &lt; 1\)</span>.</p>
<p>Esta propiedad y la restricción de que sumen 1 nos da una cierta
intuición de cómo funcionan. Podemos ver las coordenadas baricéntricas
como la contribución de los vértices a un punto <span
class="math inline">\(P\)</span>. Por ejemplo, si <span
class="math inline">\(\alpha = 0\)</span>, eso significa que el punto
viene dado por <span class="math inline">\(\beta B + \gamma C\)</span>;
es decir, una combinación lineal de <span
class="math inline">\(B\)</span> y <span
class="math inline">\(C\)</span>. Se encuentra en la recta que
generan.</p>
<p>Por proponer otro ejemplo, si alguna de las coordenadas fuera mayor
que 1, eso significaría que el punto estaría más allá del triángulo.</p>
<blockquote>
<p>TODO: dibujo con explicación de cómo funciona (libreta Shinrin -
Yoku)</p>
</blockquote>
<h4 data-number="2.2.4.2" id="calculando-la-intersección"><span
class="header-section-number">2.2.4.2</span> Calculando la
intersección</h4>
<p>Podemos eliminar una de las varibales escribiendo <span
class="math inline">\(\alpha = 1 - \beta - \gamma\)</span>, lo que nos
dice</p>
<p><span class="math display">\[
\begin{aligned}
P &amp; = (1 - \beta - \gamma) A + \beta B + \gamma C \\
  &amp; = A + (B - A) \beta + (C - A) \gamma
\end{aligned}
\]</span></p>
<p>bajo la restricción</p>
<p><span id="eq:beta_gamma" class="eqnos"><span class="math display">\[
\begin{aligned}
\beta + \gamma &amp; &lt; 1 \\
0 &amp; &lt; \beta          \\
0 &amp; &lt; \gamma
\end{aligned}
\]</span><span class="eqnos-number">(1)</span></span> </p>
<p>Un rayo <span class="math inline">\(P(t) = O + tD\)</span> impactará
en un punto del triángulo si se cumple</p>
<p><span class="math display">\[
P(t) = O + tD = A + (B - A) \beta + (C - A) \gamma
\]</span></p>
<p>cumpliendo [<a href="#eq:beta_gamma">1</a>]. Podemos expandir la
ecuación anterior en sus coordenadas para obtener</p>
<p><span class="math display">\[
\begin{aligned}
O_x + tD_x &amp; = A_x + (B_x - A_x) \beta + (C_x - A_x) \gamma \\
O_y + tD_y &amp; = A_y + (B_y - A_y) \beta + (C_y - A_y) \gamma \\
O_z + tD_z &amp; = A_z + (B_z - A_z) \beta + (C_z - A_z) \gamma \\
\end{aligned}
\]</span></p>
<p>Reordenamos:</p>
<p><span class="math display">\[
\begin{aligned}
(A_x - B_x) \beta + (A_x - C_x) \gamma+ tD_x &amp; = A_x - O_x \\
(A_y - B_y) \beta + (A_y - C_y) \gamma+ tD_y &amp; = A_y - O_y \\
(A_z - B_z) \beta + (A_z - C_z) \gamma+ tD_z &amp; = A_z - O_z
\end{aligned}
\]</span></p>
<p>Lo que nos permite escribir el sistema en forma de ecuación:</p>
<p><span class="math display">\[
\begin{pmatrix}
        A_x - B_x &amp; A_x - C_x &amp; D_x \\
        A_y - B_y &amp; A_y - C_y &amp; D_y \\
        A_z - B_z &amp; A_z - C_z &amp; D_z
\end{pmatrix}
\begin{pmatrix}
        \beta \\ \gamma \\ t
\end{pmatrix}
=
\begin{pmatrix}
        A_x - O_x \\ A_y - O_y \\ A_z - O_z
\end{pmatrix}
\]</span></p>
<p>Calcular rápidamente la solución a un sistema de ecuaciones lineales
es un problema habitual. En <span class="citation"
data-cites="ShirleyRRT">(<a href="#ref-ShirleyRRT"
role="doc-biblioref">Shirley and Morley 2003</a>)</span> se utiliza la
regla de Cramer para hacerlo, esperando que el compilador optimice las
variables intermedias creadas. Nosotros no nos tendremos que preocupar
de esto en particular, ya que el punto de impacto lo calculará la GPU
gracias a las herramientras aportadas por KHR <span class="citation"
data-cites="vulkan">(<a href="#ref-vulkan" role="doc-biblioref">The
Khronos Vulkan Working Group 2022</a>, Ray Traversal)</span>.</p>
<p>Para obtener el vector normal, podemos hacer el producto vectorial de
dos vectores que se encuentren en el plano del triángulo. Como, por
convención, los vértices se guardan en sentido antihorario visto desde
fuera del objeto, entonces</p>
<p><span class="math display">\[
\mathbf{n} = (B - A) \times (C - A)
\]</span></p>
<h1 data-number="3" id="transporte-de-luz"><span
class="header-section-number">3</span> Transporte de luz</h1>
<p>En este capítulo estudiaremos las bases de la radiometría. Esta área
de la óptica nos proporcionará una serie de herramientas con las cuales
podremos responder a la pregunta <em>cuánta luz existe en un
punto</em>.</p>
<h2 data-number="3.1" id="introducción-a-la-radiometría"><span
class="header-section-number">3.1</span> Introducción a la
radiometría</h2>
<blockquote>
<p><strong>Nota</strong>: cuando usemos un paréntesis tras una ecuación,
dentro denotaremos sus unidades de medida.</p>
</blockquote>
<p>Antes de comenzar a trabajar, necesitamos conocer <em>qué
entendemos</em> por luz. Aunque hay muchas formas de trabajar con ella
(a fin de cuentas, todavía seguimos discutiendo sobre <em>qué es</em>
exactamente la luz <a href="#fn1" class="footnote-ref" id="fnref1"
role="doc-noteref"><sup>1</sup></a>), nosotros nos quedaremos con
algunas pinceladas de la cuántica. Nos será suficiente quedarnos con la
concepción de fotón. Una fuente de iluminación emite una serie de
fotones. Estos fotones tienen una posición, una dirección de propagación
y una longitud de onda <span class="math inline">\(\lambda\)</span>
<span class="citation" data-cites="ShirleyRRT">(<a
href="#ref-ShirleyRRT" role="doc-biblioref">Shirley and Morley
2003</a>)</span>. Un fotón también tiene asociado una velocidad <span
class="math inline">\(c\)</span> que depende del índice de refracción
del medio, <span class="math inline">\(n\)</span>.</p>
<p>La unidad de medida de <span class="math inline">\(\lambda\)</span>
es el nanómetro (<span class="math inline">\(\text{nm}\)</span>).
También nos vendrá bien definir una frecuencia, <span
class="math inline">\(f\)</span>. Su utilidad viene del hecho de que,
cuando la luz cambia de medio al propagarse, la frecuencia se mantiene
constante.</p>
<p><span class="math display">\[
f = \frac{c}{\lambda}
\]</span></p>
<p>Un fotón tiene asociada una carga de energía, denotada por <span
class="math inline">\(Q\)</span>:</p>
<p><span class="math display">\[
Q = hf = \frac{hc}{\lambda} (\text{J})
\]</span></p>
<p>donde <span class="math inline">\(h = 6.62607004 \times 10^{-34}
\text{J} \cdot \text{s}\)</span> es la constante de Plank y <span
class="math inline">\(c = 299 792 458 \text{m/s}\)</span> la velocidad
de la luz.</p>
<p>En realidad, <strong>todas estas cantidades deberían tener un
subíndice <span class="math inline">\(\lambda\)</span></strong>, puesto
que dependen de la longitud de onda. La energía de un fotón <span
class="math inline">\(Q\)</span>, por ejemplo, debería denotarse <span
class="math inline">\(Q_\lambda\)</span>. Sin embargo, en la literatura
de informática gráfica, <strong>se ha optado por omitirla</strong>.
¡Tenlo en cuenta a partir de aquí!</p>
<h3 data-number="3.1.1" id="potencia"><span
class="header-section-number">3.1.1</span> Potencia</h3>
<p>A partir de la energía anterior, podemos estimar <em>la tasa de
producción de energía</em>. A esta tasa la llamaremos
<strong>potencia</strong>, o <strong>flujo radiante</strong> <span
class="math inline">\(\Phi\)</span> <span class="citation"
data-cites="PBRT3e">(<a href="#ref-PBRT3e" role="doc-biblioref">Pharr,
Jakob, and Humphreys 2016</a>, Radiometry)</span>. Esta medida nos
resultará más útil que la energía total, puesto que nos permite estimar
la energía en un instante:</p>
<p><span class="math display">\[
\Phi = \lim_{\Delta t \to 0}{\frac{\Delta Q}{\Delta t}} = \frac{dQ}{dt}
(J/s)
\]</span></p>
<p>Su unidad es julios por segundo, comúnmente denotado vatio
(<em>watts</em>, <span class="math inline">\(\text{W}\)</span>). También
se utiliza el lumen. Podemos encontrar la energía total en un periodo de
tiempo <span class="math inline">\([t_0, t_1]\)</span> integrando el
flujo radiante:</p>
<p><span class="math display">\[
Q = \int_{t_0}^{t_1}{\Phi(t)dt}
\]</span></p>
<h3 data-number="3.1.2" id="irradiancia"><span
class="header-section-number">3.1.2</span> Irradiancia</h3>
<p>La <strong>irradiancia</strong> o <strong>radiancia emitida</strong>
es el flujo radiante que recibe una superficie. Dada un área <span
class="math inline">\(A\)</span>, se define como</p>
<p><span class="math display">\[
E = \frac{\Phi}{A} (\text{W/m}^2)
\]</span></p>
<div id="fig:irradiancia" class="fignos">
<figure>
<img loading="lazy" src="./img/02/Irradiancia.png"
alt="Figura 5: La irradiancia es la potencia por metro cuadrado incidente en una superficie. Es proporcional al coseno del ángulo entre la dirección de la luz y la normal a la superficie." />
<figcaption aria-hidden="true"><span>Figura 5:</span> La irradiancia es
la potencia por metro cuadrado incidente en una superficie. Es
proporcional al coseno del ángulo entre la dirección de la luz y la
normal a la superficie.</figcaption>
</figure>
</div>
<p>Ahora que tenemos la potencia emitida en una cierta área, nos surge
una pregunta: <em>¿y en un cierto punto <span
class="math inline">\(p\)</span>?</em>. Tomando límites en la expresión
anterior, encontramos la respuesta:</p>
<p><span class="math display">\[
E(p) = \lim_{\Delta A \to 0}{\frac{\Delta \Phi}{\Delta A}} =
\frac{d\Phi}{dA} (\text{W/m}^2)
\]</span></p>
<p>De la misma manera que con la potencia, integrando <span
class="math inline">\(E(p)\)</span> podemos obtener el flujo
radiante:</p>
<p><span class="math display">\[
\Phi = \int_{A}{E(p)dp}
\]</span></p>
<p>El principal problema de la irradiancia es que <em>no nos dice nada
sobre las direcciones</em> desde las que ha llegado la luz.</p>
<h3 data-number="3.1.3" id="ángulos-sólidos"><span
class="header-section-number">3.1.3</span> Ángulos sólidos</h3>
<p>Con estas tres unidades básicas, nos surge una pregunta muy natural:
<em>¿cómo mido cuánta luz llega a una superficie?</em></p>
<p>Para responder a esta pregunta, necesitaremos los <strong>ángulos
sólidos</strong>. Son la extensión de los <strong>ángulos
planares</strong>, en dos dimensiones.</p>
<p>Ilustremos el sentido de estos ángulos: imaginemos que tenemos un
cierto objeto en dos dimensiones delante de nosotros, a una distancia
desconocida. ¿Sabríamos cuál es su tamaño, solo con esta información? Es
más, si entrara otro objeto en la escena, ¿podríamos distinguir cuál de
ellos es más grande?</p>
<p>Parece difícil responder a estas preguntas. Sin embargo, sí que
podemos determinar <em>cómo de grandes nos parecen</em> desde nuestro
punto de vista. Para ello, describimos una circunferencia de radio <span
class="math inline">\(r\)</span> alrededor nuestra. Si trazamos un par
de líneas desde nuestra posición a las partes más alejadas de este
objeto, y las cortamos con nuestra circunferencia, obtendremos un par de
puntos inscritos en ella. Pues bien, al arco que encapsulan dichos
puntos le vamos a hacer corresponder un cierto ángulo: el ángulo
planar.</p>
<div id="fig:angulo_planar" class="fignos">
<figure>
<img loading="lazy" src="./img/02/Ángulo%20planar.png"
alt="Figura 6: La idea intuitiva de un ángulo planar" />
<figcaption aria-hidden="true"><span>Figura 6:</span> La idea intuitiva
de un ángulo planar</figcaption>
</figure>
</div>
<p>Llevando esta idea a las tres dimensiones es como conseguimos el
concepto de <strong>ángulo sólido</strong>. Si en dos dimensiones
teníamos una circunferencia, aquí tendremos una esfera. Cuando generemos
las rectas proyectantes hacia el volumen, a diferencia de los ángulos
planares, se inscribirá un área en la esfera. La razón entre dicha área
<span class="math inline">\(A\)</span> y el cuadrado del radio <span
class="math inline">\(r\)</span> nos dará un ángulo sólido:</p>
<p><span class="math display">\[
\omega = \frac{A}{r^2} \text{(sr)}
\]</span></p>
<div id="fig:angulo_solido" class="fignos">
<figure>
<img loading="lazy" src="./img/02/Ángulo%20sólido.png"
alt="Figura 7: Un ángulo sólido es la razón entre el área proyectada y el cuadrado del radio" />
<figcaption aria-hidden="true"><span>Figura 7:</span> Un ángulo sólido
es la razón entre el área proyectada y el cuadrado del
radio</figcaption>
</figure>
</div>
<p>Los denotaremos por <span class="math inline">\(\omega\)</span>. En
física se suele usar <span class="math inline">\(\Omega\)</span>, pero
aquí optaremos por la minúscula. Su unidad de medida es el
estereorradián (<span class="math inline">\(\text{sr}\)</span>). Se
tiene que <span class="math inline">\(\omega \in [0, 4\pi]\)</span>. Si
<span class="math inline">\(2 \pi\)</span> radianes corresponden a la
circunferencia completa, para la esfera se tiene que <span
class="math inline">\(4 \pi\)</span> esteorradianes cubren toda la
superficie de esta. Se tiene también que <span
class="math inline">\(2\pi \text{sr}\)</span> cubren un hemisferio.
Además, un esteorradián corresponde a una superficie con área <span
class="math inline">\(r^2\)</span>: <span class="math inline">\(1
\text{sr} = \frac{r^2}{r^2}\)</span>.</p>
<p>De vez en cuando, usaremos <span
class="math inline">\(\omega\)</span> <strong>un vector dirección
unitario en la esfera</strong>.</p>
<div id="fig:xkcd_1276" class="fignos">
<figure>
<img loading="lazy" src="./img/02/xkcd_1276.png" style="width:60.0%"
alt="Figura 8: Como de costumbre, hay un XKCD relevante (Randall Munroe n.d.)" />
<figcaption aria-hidden="true"><span>Figura 8:</span> Como de costumbre,
hay un XKCD relevante <span class="citation" data-cites="xkcd-size">(<a
href="#ref-xkcd-size" role="doc-biblioref">Randall Munroe
n.d.</a>)</span></figcaption>
</figure>
</div>
<p>Usualmente emplearemos coordenadas esféricas cuando trabajemos con
ellos, dado que resulta más cómodo.</p>
<p><span class="math display">\[
\begin{aligned}
    \begin{cases}
        x = \sin\theta\cos\theta \\
        y = \sin\theta\sin\theta \\
        z = \cos\theta
    \end{cases}
\end{aligned}
\]</span></p>
<p>A <span class="math inline">\(\theta\)</span> se le denomina ángulo
polar, mientras que a <span class="math inline">\(\phi\)</span> se le
llama acimut. Imaginémonos un punto en la esfera de radio <span
class="math inline">\(r\)</span> ubicado en una posición <span
class="math inline">\((r, \theta, \phi)\)</span>. Queremos calcular un
área chiquitita <span class="math inline">\(dA_h\)</span>, de forma que
el ángulo sólido asociado a dicha área debe ser <span
class="math inline">\(d\omega\)</span>. Así, <span
class="math inline">\(d\omega = \frac{dA_h}{r^2}\)</span>. Si
proyectamos el área, obtenemos <span
class="math inline">\(d\theta\)</span> y <span
class="math inline">\(d\phi\)</span>: pequeños cambios en los ángulos
que nos generan nuestra pequeña área <span class="citation"
data-cites="berkeley-cs184">(<a href="#ref-berkeley-cs184"
role="doc-biblioref">Berkeley cs184 2022</a>, Radiometry &amp;
Photometry)</span>.</p>
<p><span class="math inline">\(dA_h\)</span> debe tener dos lados <span
class="math inline">\(lado_1\)</span> y <span
class="math inline">\(lado_2\)</span>. Podemos hallar <span
class="math inline">\(lado_1\)</span> si lo trasladamos al eje <span
class="math inline">\(z\)</span> de nuevo. Así, <span
class="math inline">\(lado_1 = r \sin d\theta\)</span>. De la misma
manera, <span class="math inline">\(lado_2 = r d\theta\)</span>.</p>
<blockquote>
<p>TODO: foto que explique todo esto, porque si no, no hay quien se
entere. Quizás me sirva la de
https://cs184.eecs.berkeley.edu/public/sp22/lectures/lec-11-radiometry-and-photometry/lec-11-radiometry-and-photometry.pdf,
p.16 siempre que adapte <span class="math inline">\(\phi\)</span>.</p>
</blockquote>
<p>Poniendo estos valores en <span
class="math inline">\(d\omega\)</span>:</p>
<p><span id="eq:d_omega" class="eqnos"><span class="math display">\[
\begin{aligned}
d\omega &amp; = \frac{dA_h}{r^2} = \frac{lado_1 lado_2}{r^2} = \\
        &amp; = \frac{r \sin\theta\ d\phi\ r\ d\theta}{r^2} = \\
        &amp; = \sin\theta\ d\theta\ d\phi
\end{aligned}
\]</span><span class="eqnos-number">(2)</span></span></p>
<p>¡Genial! Acabamos de añadir un recurso muy potente a nuestro
inventario. Esta expresión nos permitirá convertir integrales sobre
ángulos sólidos en integrales sobre ángulos esféricos.</p>
<h3 data-number="3.1.4" id="intensidad-radiante"><span
class="header-section-number">3.1.4</span> Intensidad radiante</h3>
<p>Los ángulos sólidos nos proporcionan una variedad de herramientas
nuevas considerable. Gracias a ellos, podemos desarrollar algunos
conceptos nuevos. Uno de ellos es la <strong>intensidad
radiante</strong>.</p>
<p>Imaginémonos un pequeñito punto de luz encerrado en una esfera, el
cual emite fotones en todas direcciones. Nos gustaría medir cuánta
energía pasa por la esfera. Podríamos entonces definir</p>
<p><span class="math display">\[
I = \frac{\Phi}{4\pi} \text{(W/sr)}
\]</span></p>
<p>Otra unidad de medida es el lumen por esterorradián, <span
class="math inline">\(\text{(lm/sr)}\)</span>. La anterior definición
mide cuántos fotones pasan por toda la esfera. ¿Qué ocurre si
<em>cerramos</em> el ángulo, restringiéndonos así a un área muy pequeña
de la esfera?</p>
<p><span class="math display">\[
I = \lim_{\Delta\omega \to 0}{\frac{\Delta\Phi}{\Delta\omega}} =
\frac{d\Phi}{d\omega}
\]</span></p>
<p>De la misma manera que con los conceptos anteriores, podemos volver a
la potencia integrando sobre un conjunto de direcciones:</p>
<p><span class="math display">\[
\Phi = \int_{\Omega}{I(\omega)d\omega}
\]</span></p>
<h3 data-number="3.1.5" id="radiancia"><span
class="header-section-number">3.1.5</span> Radiancia</h3>
<p>Finalmente, llegamos al concepto más importante. La <strong>radiancia
espectral</strong> (o radiancia a secas<a href="#fn2"
class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>)
<span class="citation" data-cites="PBRT3e">(<a href="#ref-PBRT3e"
role="doc-biblioref">Pharr, Jakob, and Humphreys 2016</a>,
Radiometry)</span> es una extensión de la radiancia emitida teniendo en
cuenta la dirección:</p>
<p><span class="math display">\[
L(p, \omega) = \lim_{\Delta\omega \to 0}{\frac{\Delta
E_\omega(p)}{\Delta\omega}} = \frac{dE_\omega(p)}{d\omega}
\]</span></p>
<p>siendo <span class="math inline">\(E_\omega(p)\)</span> la radiancia
emitida a la superficie perpendicular a <span
class="math inline">\(\omega\)</span>.</p>
<blockquote>
<p>TODO: foto como la de
https://cs184.eecs.berkeley.edu/public/sp22/lectures/lec-11-radiometry-and-photometry/lec-11-radiometry-and-photometry.pdf,
página 10.</p>
</blockquote>
<p>Podemos dar otra expresión de la radiancia en términos del flujo:</p>
<p><span id="eq:radiancia_flujo" class="eqnos"><span
class="math display">\[
L(p, \omega) = \frac{d^2\Phi(p, \omega)}{d\omega\ dA^\bot} =
\frac{d^2\Phi(p, \omega)}{d\omega\ dA\ \cos\theta}
\]</span><span class="eqnos-number">(3)</span></span></p>
<p>donde <span class="math inline">\(dA^\bot\)</span> es el área
proyectada por <span class="math inline">\(dA\)</span> en una hipotética
superficie perpendicular a <span
class="math inline">\(\omega\)</span>:</p>
<blockquote>
<p>TODO: figura similar a pbr figura 5.10
https://www.pbr-book.org/3ed-2018/Color_and_Radiometry/Radiometry</p>
</blockquote>
<p>Cuando un rayo impacta en una superficie, <span
class="math inline">\(L\)</span> puede tomar valores muy diferentes en
un lado y otro de dicha superficie. Por ejemplo, si nos imaginamos un
espejo, el valor un poco por encima y un poco por debajo de un punto del
espejo es muy diferente. Para solucionarlo, podemos tomar límites para
distinguir a ambos lados:</p>
<p><span id="eq:L_limit" class="eqnos"><span class="math display">\[
\begin{aligned}
L^+(p, \omega) = \lim_{t \to 0^+}{L(p + t\mathbf{n_p}, \omega)} \\
L^-(p, \omega) = \lim_{t \to 0^-}{L(p + t\mathbf{n_p}, \omega)}
\end{aligned}
\]</span><span class="eqnos-number">(4)</span></span></p>
<p>donde <span class="math inline">\(\mathbf{n_p}\)</span> es la normal
en el punto <span class="math inline">\(p\)</span>.</p>
<p>Otra forma de solucionarlo (y preferible, puesto que simplifica
entender lo que ocurre) es distinguir entre la radiancia que llega a un
punto –la incidente–, y la saliente.</p>
<p>La primera se llamará <span class="math inline">\(L_i(p,
\omega)\)</span>, mientras que la segunda será <span
class="math inline">\(L_o(p, \omega)\)</span>. Es importante destacar
que <span class="math inline">\(\omega\)</span> apunta <em>hacia
fuera</em> de la superficie. Quizás es contraintuitivo en <span
class="math inline">\(L_i\)</span>, puesto que <span
class="math inline">\(-\omega\)</span> apunta <em>hacia</em> la
superficie. Depende del autor se utiliza una concepción u otra.</p>
<blockquote>
<p><strong>Nota</strong>(ción): a <span
class="math inline">\(L_o\)</span> también se le conoce como la
radiancia reflejada. Por eso, algunas veces aparece como <span
class="math inline">\(L_r\)</span> en algunas fuentes.</p>
</blockquote>
<p>Utilizando esta notación y usando [<a href="#eq:L_limit">4</a>],
podemos escribir <span class="math inline">\(L_i\)</span> y <span
class="math inline">\(L_o\)</span> como</p>
<p><span class="math display">\[
\begin{aligned}
    L_i(p, \omega) &amp; =
        \begin{cases}
            L^+(p, -\omega) &amp; \text{si } \omega \cdot \mathbf{n_p}
&gt; 0 \\
            L^-(p, -\omega) &amp; \text{si } \omega \cdot \mathbf{n_p}
&lt; 0
        \end{cases} \\
    L_o(p, \omega) &amp; =
        \begin{cases}
            L^+(p, \omega) &amp; \text{si } \omega \cdot \mathbf{n_p}
&gt; 0 \\
            L^-(p, \omega) &amp; \text{si } \omega \cdot \mathbf{n_p}
&lt; 0
        \end{cases}
\end{aligned}
\]</span></p>
<p>Hacemos esta distinción porque, a fin de cuentas, necesitamos
distinguir entre los fotones que llegan a la superficie y los que
salen.</p>
<blockquote>
<p>TODO:
https://cs184.eecs.berkeley.edu/public/sp22/lectures/lec-11-radiometry-and-photometry/lec-11-radiometry-and-photometry.pdf,
p.36</p>
</blockquote>
<p>Una propiedad a tener en cuenta es que, si cogemos un punto <span
class="math inline">\(p\)</span> del espacio donde no existe ninguna
superifcie, <span class="math inline">\(L_o(p, \omega) = L_i(p, -\omega)
= L(p, \omega)\)</span></p>
<p>La importancia de la radiancia se debe a un par de propiedades:</p>
<p>La primera de ellas es que, dado <span
class="math inline">\(L\)</span>, podemos calcular cualquier otra unidad
básica mediante integración. Además, <strong>su valor se mantiene
constante en rayos que viajan en el vacío en línea recta</strong> <span
class="citation" data-cites="pellacini-marschner-2017">(<a
href="#ref-pellacini-marschner-2017" role="doc-biblioref">Fabio
Pellacini 2022</a>)</span>. Esto último hace que resulte muy natural
usarla en un ray tracer.</p>
<p>Veamos por qué ocurre esto:</p>
<blockquote>
<p>TODO:
https://pellacini.di.uniroma1.it/teaching/graphics17b/lectures/12_pathtracing.pdf,
página 18.</p>
</blockquote>
<p>Consideremos dos superficies ortogonales entre sí, <span
class="math inline">\(S_1\)</span> y <span
class="math inline">\(S_2\)</span> separadas una distancia <span
class="math inline">\(r\)</span>. Debido a la conservación de la
energía, cualquier fotón que salga de una superficie y se encuentre bajo
el ángulo sólido de la otra debe llegar impactar en dicha superficie
opuesta.</p>
<p>Por tanto:</p>
<p><span class="math display">\[
d^2\Phi_1 = d^2\Phi_2
\]</span></p>
<p>Sustituyendo en la expresión de la radiancia [<a
href="#eq:radiancia_flujo">3</a>], y teniendo en cuenta que son
ortogonales (lo que nos dice que <span class="math inline">\(\cos\theta
= 1\)</span>):</p>
<p><span class="math display">\[
L_1 d\omega_1 dA_1 = L_2 d\omega_2 dA_2
\]</span></p>
<p>Por construcción, podemos cambiar los ángulos sólidos:</p>
<p><span class="math display">\[
L_1 \frac{dA_2}{r^2} dA_1 = L_2 \frac{dA_1}{r^2} dA_2
\]</span></p>
<p>Lo que finalmente nos dice que <span class="math inline">\(L_1 =
L_2\)</span>, como queríamos ver.</p>
<h3 data-number="3.1.6" id="integrales-radiométricas"><span
class="header-section-number">3.1.6</span> Integrales radiométricas</h3>
<p>En esta sección, vamos a explorar las nuevas herramientas que nos
proporciona la radiancia. Veremos también cómo integrar ángulos sólidos,
y cómo simplificar dichas integrales.</p>
<h4 data-number="3.1.6.1"
id="una-nueva-expresión-de-la-irradiancia-y-el-flujo"><span
class="header-section-number">3.1.6.1</span> Una nueva expresión de la
irradiancia y el flujo</h4>
<p>Como dijimos al final de <a href="#irradiancia">la sección de la
irradiancia</a>, esta medida no tiene en cuenta las direcciones desde
las que llegaba la luz. A diferencia de esta, la radiancia sí que las
utiliza. Dado que una de las ventajas de la radiancia es que nos permite
obtener el resto de medidas radiométricas, ¿por qué no desarrollamos una
nueva expresión de la irradiancia?</p>
<p>Para obtener cuánta luz llega a un punto, debemos acumular la
radiancia incidente que nos llega desde cualquier dirección.</p>
<blockquote>
<p>TODO: dibujo como el de la libreta roja. Me lo mandé por Telegram,
por si no lo encuentro</p>
</blockquote>
<p>Dado un punto <span class="math inline">\(p\)</span> que se encuentra
en una superficie con normal <span
class="math inline">\(\mathbf{n}\)</span> en dicho punto, la irradiancia
se puede expresar como <span class="citation" data-cites="PBRT3e">(<a
href="#ref-PBRT3e" role="doc-biblioref">Pharr, Jakob, and Humphreys
2016</a>, Working with Radiometric Integrals)</span></p>
<p><span id="eq:E_abs_cos" class="eqnos"><span class="math display">\[
E(p, \mathbf{n}) = \int_{\Omega}{L_i(p, \omega) \left\lvert cos\theta
\right\rvert d\omega}
\]</span><span class="eqnos-number">(5)</span></span></p>
<p>El término <span class="math inline">\(\cos\theta\)</span> aparece en
la integral debido a la derivada del área proyectada, <span
class="math inline">\(dA^\bot\)</span>. <span
class="math inline">\(\theta\)</span> es el ángulo entre la dirección
<span class="math inline">\(\omega\)</span> y la normal <span
class="math inline">\(\mathbf{n}\)</span>.</p>
<p>Generalmente, la irradiancia se calcula únicamente en el hemisferio
de direcciones asociado a la normal en el punto, <span
class="math inline">\(H^2(\mathbf{n})\)</span>.</p>
<p>Podemos eliminar el <span class="math inline">\(\cos\theta\)</span>
de la integral mediante una pequeña transformación: proyectando el
ángulo sólido sobre el disco alrededor del punto <span
class="math inline">\(p\)</span> con normal <span
class="math inline">\(\mathbf{n}\)</span>, obtenemos una expresión más
sencilla: como <span class="math inline">\(d\omega^\bot = \left\lvert
\cos\theta \right\rvert d\omega\)</span>, entonces</p>
<p><span class="math display">\[
\begin{aligned}
    E(p, \mathbf{n}) = \int_{H^2(\mathbf{n})}{L_i(p, \omega)
d\omega^\bot}
\end{aligned}
\]</span></p>
<p>Usando lo que aprendimos sobre la derivada de los ángulos sólidos [<a
href="#eq:d_omega">2</a>], se puede reescribir la ecuación anterior
como</p>
<p><span class="math display">\[
E(p, \mathbf{n}) = \int_{0}^{2\pi}\int_{0}^{\pi/2}{L_i(p, \theta, \phi)
\cos\theta\ \sin\theta\ d\theta\ d\phi}
\]</span></p>
<p>Haciendo el mismo juego con el flujo emitido de un cierto objeto al
hemisferio que encapsula la normal, conseguimos:</p>
<p><span class="math display">\[
\begin{aligned}
    \Phi &amp; = \int_{A}\int_{H^2(\mathbf{n})}{L_o(p, \omega)
\cos\theta\ d\omega dA} = \\
         &amp; = \int_{A}\int_{H^2(\mathbf{n})}{L_o(p, \omega)
d\omega^\bot dA}
\end{aligned}
\]</span></p>
<blockquote>
<p>TODO: a lo mejor merece la pena hacer un ejemplo sobre los diferentes
tipos de luz, como en
https://cs184.eecs.berkeley.edu/public/sp22/lectures/lec-11-radiometry-and-photometry/lec-11-radiometry-and-photometry.pdf
p.41? O a lo mejor un capítulo para hablar de luces en general.</p>
</blockquote>
<h4 data-number="3.1.6.2" id="integrando-sobre-área"><span
class="header-section-number">3.1.6.2</span> Integrando sobre área</h4>
<p>Una herramienta más que nos vendrá bien será la capacidad de
convertir integrales sobre direcciones en integrales sobre área. Hemos
hecho algo similar en las secciones anteriores, así que no perdemos nada
por generalizarlo.</p>
<p>Considera un punto <span class="math inline">\(p\)</span> sobre una
superficie con normal en dicho punto <span
class="math inline">\(\mathbf{n}\)</span>. Supongamos que tenemos una
pequeña área <span class="math inline">\(dA\)</span> con normal <span
class="math inline">\(\mathbf{n_{dA}}\)</span>. Sea <span
class="math inline">\(\theta\)</span> el ángulo entre <span
class="math inline">\(\mathbf{n}\)</span> y <span
class="math inline">\(\mathbf{n_{dA}}\)</span>, y <span
class="math inline">\(r\)</span> la distancia entre <span
class="math inline">\(p\)</span> y <span
class="math inline">\(dA\)</span>.</p>
<p>Entonces, la relación entre la diferencial de un ángulo sólido y la
de un área es</p>
<p><span class="math display">\[
d\omega = \frac{dA\cos\theta}{r^2}
\]</span></p>
<blockquote>
<p>TODO: figura como la de pbr book 5.16.</p>
</blockquote>
<p>Esto nos permite, por ejemplo, expandir algunas expresiones como la
de la irradiancia [<a href="#eq:E_abs_cos">5</a>] si partimos de un
cuadrilátero <span class="math inline">\(dA\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
    E(p, \mathbf{n}) &amp; = \int_{\Omega}{L_i(p, \omega) \left\lvert
\cos\theta \right\rvert d\omega} = \\
                     &amp; = \int_{A}{L\cos\theta\
\frac{\cos\theta_o}{r^2}dA}
\end{aligned}
\]</span></p>
<p>siendo <span class="math inline">\(\theta_o\)</span> el ángulo de la
radiancia de salida de la superficie del cuadrilátero.</p>
<h3 data-number="3.1.7" id="fotometría-y-radiometría"><span
class="header-section-number">3.1.7</span> Fotometría y radiometría</h3>
<blockquote>
<p>TODO: hablar sobre las diferencias. Hay información útil en
01_lights.pdf, p.43</p>
</blockquote>
<h2 data-number="3.2" id="dispersión-de-luz"><span
class="header-section-number">3.2</span> Dispersión de luz</h2>
<p>Cuando una luz impacta en una superficie, ocurren un par de sucesos:
parte de los fotones se reflejan saliendo disparados hacia alguna
dirección, mientras que otros se absorben.</p>
<p>La forma en la que se comportan depende de cómo sea la superficie.
Específicamente, del material del que esté hecha.</p>
<p>En informática gráfica se consideran tres tipos principales de
dispersión de luz: <strong>dispersión en superficie</strong>
(<em>surface scattering</em>), <strong>dispersión volumétrica</strong>
(<em>volumetric scattering</em>) y <strong>dispersión bajo
superficie</strong> (<em>subsurface scattering</em>)</p>
<p>En este capítulo vamos a modelar la primera. Estudiaremos qué es lo
que ocurre cuando los fotones alcanzan una superficie, en qué dirección
se reflejan, y cómo cambia el comportamiento dependiendo de las
propiedades del material.</p>
<h3 data-number="3.2.1"
id="la-función-de-distribución-de-reflectancia-bidireccional-brdf"><span
class="header-section-number">3.2.1</span> La función de distribución de
reflectancia bidireccional (BRDF)</h3>
<p>La <strong>función de distribución de reflectancia
bidireccional</strong> (en inglés, <em>bidirectional reflectance
distribution function</em>, BRDF) <span class="citation"
data-cites="PBRT3e">(<a href="#ref-PBRT3e" role="doc-biblioref">Pharr,
Jakob, and Humphreys 2016</a>, Surface Reflection)</span> describe cómo
la luz se refleja en una superficie opaca. Se encarga de informarnos
sobre cuánta radiancia sale en dirección <span
class="math inline">\(\omega_o\)</span> debido a la radiancia incidente
desde la dirección <span class="math inline">\(\omega_i\)</span>,
partiendo de un punto <span class="math inline">\(p\)</span> en una
superficie con normal <span class="math inline">\(\mathbf{n}\)</span>.
Depende de la longitud de onda <span
class="math inline">\(\lambda\)</span>, pero, como de costumbre, la
omitiremos.</p>
<blockquote>
<p><strong>Intuición</strong>: <em>¿cuál es la probabilidad de que,
habiéndome llegado un fotón desde <span
class="math inline">\(\omega_i\)</span>, me salga disparado hacia <span
class="math inline">\(\omega_o\)</span>?</em></p>
</blockquote>
<blockquote>
<p>TODO: esquema como el de pbr fig. 5.18, o como
https://pellacini.di.uniroma1.it/teaching/graphics17b/lectures/12_pathtracing.pdf
p.20</p>
</blockquote>
<p>Si consideramos <span class="math inline">\(\omega_i\)</span> como un
cono diferencial de direcciones, la irradiancia diferencial en <span
class="math inline">\(p\)</span> viene dada por</p>
<p><span class="math display">\[
dE(p, \omega_i) = L_i(p, \omega_i) \cos\theta_i\ d\omega_i
\]</span></p>
<p>Debido a esta irradiancia, una pequeña parte de radiancia saldrá en
dirección <span class="math inline">\(\omega_o\)</span>, proporcional a
la irradiancia:</p>
<p><span class="math display">\[
dL_o(p, \omega_o) \propto dE(p, \omega_i)
\]</span></p>
<p>Si lo ponemos en forma de cociente, sabremos exactamente cuál es la
proporción de luz. A este cociente lo llamaremos <span
class="math inline">\(f_r(p, \omega_o \leftarrow \omega_i)\)</span>; la
función de distribución de reflectancia bidireccional:</p>
<p><span class="math display">\[
f_r(p, \omega_o \leftarrow \omega_i) = \frac{dL_o(p, \omega_o)}{dE(p,
\omega_i)} = \frac{dL_o(p, \omega_o)}{L_i(p, \omega_i) \cos\theta_i\
d\omega_i} \text{(1/sr)}
\]</span></p>
<blockquote>
<p><strong>Nota</strong>(ción): dependiendo de la fuente que estés
leyendo, es posible que te encuentres una integral algo diferente. Por
ejemplo, en tanto en Wikipedia como en <span class="citation"
data-cites="ShirleyRRT">(<a href="#ref-ShirleyRRT"
role="doc-biblioref">Shirley and Morley 2003</a>)</span> se integra con
respecto a los ángulos de salida <span
class="math inline">\(\omega_o\)</span>, en vez de los incidentes.</p>
<p>Aquí, usaremos la notación de integrar con respecto a los incidentes,
como se hace en <span class="citation" data-cites="PBRT3e">(<a
href="#ref-PBRT3e" role="doc-biblioref">Pharr, Jakob, and Humphreys
2016</a>)</span>.</p>
</blockquote>
<p>Las BRDF físicamente realistas tienen un par de propiedades
importantes:</p>
<ol type="1">
<li><strong>Reciprocidad</strong>: para cualquier par de direcciones
<span class="math inline">\(\omega_i\)</span>, <span
class="math inline">\(\omega_o\)</span>, se tiene que <span
class="math inline">\(f_r(p, \omega_i, \omega_o)=\ \)</span> <span
class="math inline">\(f_r(p, \omega_o \leftarrow
\omega_i)\)</span>.</li>
<li><strong>Conservación de la energía</strong>: La energía reflejada
tiene que ser menor o igual que la incidente:</li>
</ol>
<p><span class="math display">\[
\int_{H^2(\mathbf{n})}{f_r(p, \omega_o \leftarrow \omega_i)
\cos\theta_i\ d\omega_i} \leq 1
\]</span></p>
<h3 data-number="3.2.2"
id="la-función-de-distribución-de-transmitancia-bidireccional-btdf"><span
class="header-section-number">3.2.2</span> La función de distribución de
transmitancia bidireccional (BTDF)</h3>
<p>Si la BRDF describe cómo se refleja la luz, la <em>bidirectional
transmittance distribution function</em> (abreviada BTDF) nos informará
sobre la transmitancia; es decir, cómo se comporta la luz cuando
<em>entra</em> en un medio. Generalmente serán dos caras de la misma
moneda: cuando la luz impacta en una superficie, parte de ella, se
reflejará, y otra parte se transmitirá.</p>
<p>Puedes imaginarte la BTDF como una función de reflectancia del
hemisferio opuesto a donde se encuentra la normal de la superficie.</p>
<p>Denotaremos a la BTDF por</p>
<p><span class="math display">\[
f_t(p, \omega_o \leftarrow \omega_i)
\]</span></p>
<p>Al contrario que en la BRDF, <span
class="math inline">\(\omega_o\)</span> y <span
class="math inline">\(\omega_i\)</span> se encuentran en hemisferios
diferentes.</p>
<h3 data-number="3.2.3"
id="la-función-de-distribución-de-dispersión-bidireccional-bsdf"><span
class="header-section-number">3.2.3</span> La función de distribución de
dispersión bidireccional (BSDF)</h3>
<p>Convenientemente, podemos unir la BRDF y la BTDF en una sola
expresión, llamada <strong>la función de distribución de dispersión
bidireccional</strong> (<em>bidirectional scattering distribution
function</em>, BSDF). A la BSDF la denotaremos por</p>
<p><span class="math display">\[
f(p, \omega_o \leftarrow \omega_i)
\]</span></p>
<blockquote>
<p><strong>Nota</strong>(ción): también se suele utilizar BxDF en vez de
BSDF.</p>
</blockquote>
<p>Usando esta definición, podemos obtener</p>
<p><span class="math display">\[
dL_o(p, \omega_o) = f(p, \omega_o \leftarrow \omega_i) L_i(p, \omega_i)
\left\lvert \cos\theta_i \right\rvert d\omega_i
\]</span></p>
<p>Esto nos deja a punto de caramelo una nueva expresión de la
randiancia en términos de la randiancia incidente en un punto <span
class="math inline">\(p\)</span>. Integrando la expresión anterior,
obtenemos</p>
<p><span id="eq:scattering_equation" class="eqnos"><span
class="math display">\[
L_o(p, \omega_o) = \int_{\mathbb{S}^2}{f(p, \omega_o \leftarrow
\omega_i)L_i(p, \omega_i)\left\lvert \cos\theta_i \right\rvert
d\omega_i}
\]</span><span class="eqnos-number">(6)</span></span></p>
<p>siendo <span class="math inline">\(\mathbb{S}^2\)</span> la
esfera.</p>
<blockquote>
<p><strong>Intuición:</strong> <em>la BSDF son todas las posibles
direcciones en las que puede salir disparada la luz.</em></p>
</blockquote>
<p>Esta forma de expresar la radiancia es muy importante. Generalmente
se le suele llamar la <em>ecuación de dispersión</em> (<em>scattering
equation</em>, en inglés). Dado que es una integral muy importante,
seguramente tengamos que evaluarla repetidamente. ¡Los métodos de Monte
Carlo nos vendrán de perlas! Más adelante hablaremos de ella.</p>
<p>Las BSDFs tienen unas propiedades interesantes:</p>
<ul>
<li><strong>Positividad</strong>: como los fotones no se pueden reflejar
“negativamente”, <span class="math inline">\(f(p, \omega_o \leftarrow
\omega_i) \ge 0\)</span>.</li>
<li><strong>Reciprocidad de Helmotz:</strong> se puede invertir la
dirección de un rayo: <span class="math inline">\(f(p, \omega_o
\leftarrow \omega_i) = f(p, \omega_i \leftarrow \omega_o)\)</span>.</li>
<li><strong>White furnace test</strong>: Toda la luz incidente debe ser
reflejada cuando la reflectividad de la superficie es 1.</li>
<li><strong>Conservación de la energía</strong>: todos los fotones que
llegan a la superficie deben ser reflejados o absorbidos. Es decir, no
se emite ningún fotón nuevo:</li>
</ul>
<p><span class="math display">\[
\int_{H^2(\mathbf{n})}{f(p, \omega_o \leftarrow \omega_i) \cos\theta_i\
d\omega_i} \le 1\ \forall \omega_o
\]</span></p>
<h3 data-number="3.2.4" id="reflectancia-hemisférica"><span
class="header-section-number">3.2.4</span> Reflectancia hemisférica</h3>
<p>Puede ser útil tomar el comportamiento agregado de las BRDFs y las
BTDFs y reducirlo un cierto valor que describa su comportamiento general
de dispersión. Sería Algo así como un resumen de su distribución. Para
conseguirlo, vamos a introducir dos nuevas funciones:</p>
<p>El albedo <span class="citation"
data-cites="Szirmay-Kalos00monte-carlomethods">(<a
href="#ref-Szirmay-Kalos00monte-carlomethods"
role="doc-biblioref">Szirmay-Kalos 2000</a>)</span>, o también conocido
como la <strong>reflectancia hemisférica-direccional</strong>
(<em>hemispherical-directional reflectance</em>) <span class="citation"
data-cites="PBRT3e">(<a href="#ref-PBRT3e" role="doc-biblioref">Pharr,
Jakob, and Humphreys 2016</a>, Reflection Models, Basic
Interface)</span> describe la reflexión total sobre un hemisferio debida
a una fuente de luz que proviene desde la dirección <span
class="math inline">\(\omega_o\)</span> :</p>
<p><span class="math display">\[
\rho_{hd}(\omega_o) = \int_{H^2(n)}{f_r(p, \omega_o \leftarrow \omega_i)
\left\lvert \cos\theta_i \right\rvert\ d\omega_i}
\]</span></p>
<p>Por otra parte, la <strong>reflectancia
hemisférica-hemisférica</strong> (<em>hemispherical-hemispherical
reflectance</em>) es un valor espectral que nos proporciona el ratio de
luz incidente reflejada por una superficie, suponiendo que llega la
misma luz desde todas direcciones:</p>
<p><span class="math display">\[
\rho_{hh} = \frac{1}{\pi} \int_{H^2(n)} \int_{H^2(n)}{f_r(p, \omega_o
\leftarrow \omega_i) \left\lvert \cos\theta_o\ \cos\theta_i
\right\rvert\ d\omega_o\ d\omega_i}
\]</span></p>
<h2 data-number="3.3" id="modelos-ópticos-de-materiales"><span
class="header-section-number">3.3</span> Modelos ópticos de
materiales</h2>
<p>En la práctica, cada superficie tendrá una BSDF característica. Esto
hace que la luz adquiera una dirección particular al incidir en cada
punto de esta. En esta sección, vamos a tratar algunas BSDFs
particulares e introduciremos las fórmulas fundamentales que se usan en
los modelos de materiales (también conocidos como modelos de
<em>shading</em>).</p>
<p>Los tipos de materiales que vamos a tratar son las básicos. Entre
ellos, se encuentran la difusa lambertiana, materiales dieléctricos,
espejos y algunas BSDFs compuestas. Un repertorio de implementaciones se
encuentra en el repositorio de BRDFs de <span class="citation"
data-cites="disney-brdfs">(<a href="#ref-disney-brdfs"
role="doc-biblioref">Walt Disney Animation Studios 2019</a>)</span>.</p>
<h3 data-number="3.3.1" id="tipos-de-dispersión"><span
class="header-section-number">3.3.1</span> Tipos de dispersión</h3>
<p>Prácticamente toda superficie, en mayor o menor medida, refleja parte
de la luz incidente. Otros tipos de materiales reflejan y refractan a la
vez, como puede ser un espejo o el agua.</p>
<div id="fig:refraccion" class="fignos">
<figure>
<img loading="lazy" src="./img/02/Reflexión%20y%20refracción.png" style="width:50.0%"
alt="Figura 9: Reflexión y refracción de luz (Adam Marrs and Wald 2021, 106)." />
<figcaption aria-hidden="true"><span>Figura 9:</span> Reflexión y
refracción de luz <span class="citation" data-cites="Marrs2021">(<a
href="#ref-Marrs2021" role="doc-biblioref">Adam Marrs and Wald 2021,
106</a>)</span>.</figcaption>
</figure>
</div>
<blockquote>
<p>TODO: cambiar por foto propia</p>
<p>En esencia, los reflejos se pueden clasificar en cuatro grandes tipos
<span class="citation" data-cites="McGuire2018GraphicsCodex">(<a
href="#ref-McGuire2018GraphicsCodex" role="doc-biblioref">McGuire
2021</a>, Materials)</span>:</p>
</blockquote>
<ul>
<li><strong>Difusos</strong> (<em>Diffuse</em>): esparcen la luz en
todas direcciones casi equiprobablemente. Por ejemplo, la tela y el
papel son materiales difusos.</li>
<li><strong>Especulares brillantes</strong> (<em>Glossy specular</em>):
la distribución de luz se asemeja a un cono. La chapa de un coche es un
material especular brillante.</li>
<li><strong>Especulares perfectos</strong> (<em>Perfect specular</em>):
en esencia, son espejos. El ángulo de salida de la luz es muy pequeño,
por lo que reflejan casi a la perfección lo que les llega.</li>
<li><strong>Retrorreflectores</strong> (<em>Retro reflective</em>): la
luz se refleja en dirección contraria a la de llegada. Esto es lo que
sucede a la luna.</li>
</ul>
<p>Ten en cuenta que es muy difícil encontrar objetos físicos que imiten
a la perfección un cierto modelo. Suelen recaer en un híbrido entre dos
o más modelos.</p>
<p>Fijado un cierto modelo, la función de distribución de reflectancia,
BRDF, puede ser <strong>isotrópica</strong> o
<strong>anisotrópica</strong>. Los materiales isotrópicos mantienen las
propiedades de reflectancia invariantes ante rotaciones; es decir, la
distribución de luz es la misma en todas direcciones. Por el contrario,
los anisotrópicos reflejan diferentes cantidades de luz dependiendo
desde dónde los miremos. Los ejemplos más habituales de materiales
anisotrópicos son las rocas y la madera.</p>
<h3 data-number="3.3.2" id="reflexión"><span
class="header-section-number">3.3.2</span> Reflexión</h3>
<p>Primero, tratemos con materiales que únicamente reflejan luz; es
decir, su BSDF es una BRDF.</p>
<h4 data-number="3.3.2.1" id="reflexión-especular-perfecta"><span
class="header-section-number">3.3.2.1</span> Reflexión especular
perfecta</h4>
<p>Para un material especular perfecto (es decir, espejos), la dirección
reflejada <span class="math inline">\(\mathbf{r}\)</span> dado un rayo
incidente <span class="math inline">\(\mathbf{i}\)</span> es <span
class="citation" data-cites="Marrs2021">(<a href="#ref-Marrs2021"
role="doc-biblioref">Adam Marrs and Wald 2021</a>, Reflection and
refraction formulas, p. 105)</span>:</p>
<p><span class="math display">\[
\mathbf{r} = \mathbf{i} - 2 (\mathbf{i} \cdot \mathbf{n}) \mathbf{n}
\]</span></p>
<p>siendo <span class="math inline">\(\mathbf{n}\)</span> la normal en
el punto incidente. Con esta expresión, se necesita que <span
class="math inline">\(\mathbf{n}\)</span> esté normalizado. Para los
otros dos vectores no es necesario; la dirección de salida tendrá la
misma norma que la de entrada.</p>
<p>Su BRDF se define mediante una delta de Dirac <span class="citation"
data-cites="Szirmay-Kalos00monte-carlomethods">(<a
href="#ref-Szirmay-Kalos00monte-carlomethods"
role="doc-biblioref">Szirmay-Kalos 2000, 3.2</a>)</span>, <span
class="citation" data-cites="McGuire2018GraphicsCodex">(<a
href="#ref-McGuire2018GraphicsCodex" role="doc-biblioref">McGuire
2021</a>, Materials)</span>:</p>
<p><span class="math display">\[
f_r(\mathbf{r} \leftarrow \mathbf{i} ) = \frac{\delta(\mathbf{i},
\mathbf{r}) k_r(\left\lvert \mathbf{i} \cdot \mathbf{n}
\right\rvert)}{\left\lvert \mathbf{i} \cdot \mathbf{n} \right\rvert}
\]</span></p>
<p>siendo <span class="math inline">\(\rho_{hd} = k_r(\left\lvert
\mathbf{i} \cdot \mathbf{n} \right\rvert)\)</span> el albedo, con <span
class="math inline">\(k_r\)</span> el coeficiente de reflectividad, cuyo
valor se encuentra entre 0 y 1, dependiendo de la energía que se
pierda.</p>
<h4 data-number="3.3.2.2" id="reflexión-difusa-o-lamberiana"><span
class="header-section-number">3.3.2.2</span> Reflexión difusa o
lamberiana</h4>
<p>Este es uno de los modelos más sencillos. Es conocido también como el
modelo lambertiano. Se asume que la superficie es completamente difusa,
lo cual implica que la luz se refleja en todas direcciones
equiprobablemente, independientemente del punto de vista del observador.
Esto significa que</p>
<p><span class="math display">\[
f_r(\omega_o \leftarrow \omega_i) = k_d
\]</span></p>
<p>con <span class="math inline">\(k_d\)</span> el coeficiente de
difusión.</p>
<p>El albedo viene dado por</p>
<p><span class="math display">\[
\begin{aligned}
\rho_{hd}(\omega_o) &amp; = \int_{H^2(n)}{k_d \cos\theta_i\ d\omega_i} =
\\
                    &amp; = \int_{\phi = 0}^{2\pi} \int_{\theta =
0}^{\pi/2}{k_d \cos\theta\ d\theta\ d\phi} = \\
                    &amp; = k_d \pi
\end{aligned}
\]</span></p>
<p>Para que se cumpla la condición de conservación de energía,
necesariamente <span class="math inline">\(k_d \le 1/\pi\)</span>.</p>
<p>En la práctica no se utiliza mucho, pues está muy limitado.</p>
<h4 data-number="3.3.2.3" id="reflexión-especular-no-perfecta"><span
class="header-section-number">3.3.2.3</span> Reflexión especular no
perfecta</h4>
<h5 data-number="3.3.2.3.1" id="phong"><span
class="header-section-number">3.3.2.3.1</span> Phong</h5>
<p>El modelo de Phong se basa en la observación de que, cuando el punto
de vista se alinea con la dirección del vector de luz reflejado <span
class="math inline">\(r = 1 - 2(\mathbf{n} \cdot
\mathbf{l})\mathbf{n}\)</span>, aparecen puntos muy iluminados, lo que
se conoce como resaltado especular.</p>
<p>Esta idea se <em>refleja</em> considerando la componente especular
como</p>
<p><span class="math display">\[
L_o^s(p, \omega_o \leftarrow \omega_i) =
      k_\alpha
    + k_d L_o^d(p, \omega_o \leftarrow \omega_i)
    + k_s \max\{0, \omega \cdot \mathbf{r}\}^\alpha
\]</span></p>
<p>donde <span class="math inline">\(k_\alpha\)</span> es el coeficiente
de luz ambiental (con <span class="math inline">\(\alpha\)</span> el
índice de brillo) <span class="math inline">\(k_s\)</span> es la
constante de reflectancia especular (<em>specular-reflection</em>) que
define el ratio de luz reflejada, <span
class="math inline">\(k_d\)</span> el de radiancia difusa <span
class="math inline">\(L_o^d\)</span>. Usualmente, <span
class="math inline">\(k_s \vert k_d &lt; 1\)</span>.</p>
<p>Evidentemente, este modelo no es más que una aproximación físicamente
poco realista de la realidad; pero funciona lo suficientemente bien como
para usarlo en ciertas partes.</p>
<pre><code class="language-glsl">float Phong_specular(vec3 normal, vec3 light_dir, vec3 view_dir, float shininess) {
    return pow(
        max(
            0.0,
            dot(
                reflejar(normal, light_dir),
                view_dir
            )
        ),
        shininess
    );
}</code></pre>
<h5 data-number="3.3.2.3.2" id="blinn---phong"><span
class="header-section-number">3.3.2.3.2</span> Blinn - Phong</h5>
<p>Este es una pequeña modificación al de Phong. En vez de usar el
vector reflejado de luz, se define un vector unitario entre el
observador y la luz, <span class="math inline">\(\mathbf{h} =
\frac{\omega + \mathbf{l}}{\left\lVert \omega + \mathbf{l}
\right\rVert}\)</span>. Resulta más fácil calcularlo. Además, este
modelo es más realista.</p>
<p><span class="math display">\[
L_o^s(p, \omega_o \leftarrow \omega_i) =
      k_\alpha
    + k_d L_o^d(p, \omega_o \leftarrow \omega_i)
    + k_s \max\{0, \mathbf{h} \cdot \mathbf{n}\}^\alpha
\]</span></p>
<pre><code class="language-glsl">float BlingPhong_specular(vec3 normal, vec3 light_dir, vec3 view_dir, float shininess) {
    vec3 h = normalize(view_dir + light_dir);
    return pow(
        max(
            0.0,
            dot(h, normal)
        ),
        shininess
    );
}</code></pre>
<h3 data-number="3.3.3" id="refracción"><span
class="header-section-number">3.3.3</span> Refracción</h3>
<p>Algunos materiales permiten que la luz los atraviese –conocido como
transmisión–. En estos casos, decimos que se produce un cambio en el
medio. Para conocer cómo de rápido viajan los fotones a través de ellos,
se utiliza un valor denominado <strong>índice de refracción</strong>,
usualmente denotado por <span class="math inline">\(\eta\)</span>:</p>
<p><span class="math display">\[
\eta = \frac{c}{\nu}
\]</span></p>
<p>siendo <span class="math inline">\(c\)</span> la velocidad de la luz
en el vacío y <span class="math inline">\(\nu\)</span> la velocidad de
fase del medio, la cual depende de la longitud de onda. Sin embargo,
como hemos comentado varias veces, no tendremos en cuenta la longitud de
onda en nuestro ray tracer, por lo que no nos tenemos que preocupar de
esto.</p>
<p>Algunos materiales como el aire tienen un índice de refracción <span
class="math inline">\(\eta_{\text{aire}} = 1.0003\)</span>, mientras que
el del agua vale <span class="math inline">\(\eta_{\text{agua}} =
1.333\)</span>, y el del cristal vale <span
class="math inline">\(\eta_{\text{cristal}} = 1.52\)</span>.</p>
<h4 data-number="3.3.3.1" id="ley-de-snell"><span
class="header-section-number">3.3.3.1</span> Ley de Snell</h4>
<p>La <strong>ley de Snell</strong> nos proporciona una ecuación muy
sencilla que relaciona el cambio de un medio con índice de refracción
<span class="math inline">\(\eta_1\)</span> a otro con índice de
refracción <span class="math inline">\(\eta_2\)</span>:</p>
<p><span id="eq:ley_snell" class="eqnos"><span class="math display">\[
\eta_1 \sin\theta_1 = \eta_2 \sin\theta_2
\]</span><span class="eqnos-number">(7)</span></span></p>
<p>siendo <span class="math inline">\(\theta_1\)</span> y <span
class="math inline">\(\theta_2\)</span> los ángulos de entrada y salida
respectivamente.</p>
<p>Usualmente, los índices de refración son conocidos, así como el
ángulo de incidencia <span class="math inline">\(\theta_1\)</span>, por
lo que podremos calcular el ángulo del vector refractado con
facilidad:</p>
<p><span class="math display">\[
\theta_2 = \arcsin{\left(\frac{\eta_1}{\eta_2}\sin\theta_2\right)}
\]</span></p>
<p>Cuando cambiamos de un medio con índice de refracción <span
class="math inline">\(\eta_1\)</span> a otro con <span
class="math inline">\(\eta_2 &lt; \eta_1\)</span>, podemos encontrarnos
ante un caso de <strong>reflexión interna total</strong>.
Analíticamente, lo que ocurre es que</p>
<p><span class="math display">\[
\sin\theta_2 = \frac{\eta_1}{\eta_2}\sin\theta_1 &gt; 1
\]</span></p>
<p>lo cual no puede ocurrir. Se denomina el ángulo crítico a aquel <span
class="math inline">\(\theta_1\)</span> para la cual <span
class="math inline">\(\frac{\eta_1}{\eta_2}\sin\theta_1 &gt;
1\)</span>:</p>
<p><span class="math display">\[
\theta_1 = \arcsin{\left(\frac{\eta_2}{\eta_1}\right)}
\]</span></p>
<p>Por ejemplo, si un haz de luz viaja desde un cristal hacia un cuerpo
de agua, entonces <span class="math inline">\(\theta_1 =
\arcsin{(1.333/1.52)} \approx 1.06\)</span> radianes <span
class="math inline">\(= 61.04\textdegree\)</span>.</p>
<p>Lo que ocurre en estos casos es que, en vez de pasar al segundo
medio, los fotones vuelven al primero; creando un reflejo como si de un
espejo se tratara.</p>
<div id="fig:reflexion_interna_total" class="fignos">
<figure>
<img loading="lazy" src="./img/02/Reflexión%20interna%20total.jpg" style="width:80.0%"
alt="Figura 10: Como el ángulo de incidencia es considerablemente alto, por la parte de arriba la luz no puede atravesar el agua. Esto hace que podamos ver el edificio de enfrente. En el centro vemos refractado el suelo. Y, sin embargo, en la parte inferior, ¡observamos luz solar y el edificio de nuevo!" />
<figcaption aria-hidden="true"><span>Figura 10:</span> Como el ángulo de
incidencia es considerablemente alto, por la parte de arriba la luz no
puede atravesar el agua. Esto hace que podamos ver el edificio de
enfrente. En el centro vemos refractado el suelo. Y, sin embargo, en la
parte inferior, ¡observamos luz solar y el edificio de
nuevo!</figcaption>
</figure>
</div>
<h4 data-number="3.3.3.2" id="ecuaciones-de-fresnel"><span
class="header-section-number">3.3.3.2</span> Ecuaciones de Fresnel</h4>
<p>Aquellos materiales que refractan y reflejan luz (como el agua de la
foto anterior) no pueden generar energía de la nada; por lo que la
combinación de ambos efectos debe ser proporcional a la luz incidente.
Es decir, una fracción de luz es reflejada, y otra es refractada. Las
<strong>ecuaciones de Fresnel</strong> nos permiten conocer esta
cantidad.</p>
<p>La proporción de luz reflejada desde un rayo que viaja por un medio
con índice de refracción <span class="math inline">\(\eta_1\)</span> y
ángulo de incidencia <span class="math inline">\(\theta_1\)</span> a
otro medio con índice de refracción <span
class="math inline">\(\eta_2\)</span> es <span class="citation"
data-cites="Marrs2021">(<a href="#ref-Marrs2021"
role="doc-biblioref">Adam Marrs and Wald 2021</a>, The Schlick Fresnel
approximation, p. 109)</span>:</p>
<p><span id="eq:fresnel_equations" class="eqnos"><span
class="math display">\[
\begin{aligned}
    R_s &amp; = \left\lvert \frac
        {\eta_1 \cos\theta_1 - \eta_2 \sqrt{1 - \left(
\frac{\eta_1}{\eta_2}\sin\theta_1\right) ^2}}
        {\eta_1 \cos\theta_1 + \eta_2 \sqrt{1 - \left(
\frac{\eta_1}{\eta_2}\sin\theta_1\right) ^2}}
     \right\rvert^2 \\
    R_p &amp; = \left\lvert \frac
        {\eta_1 \sqrt{1 - \left(
\frac{\eta_1}{\eta_2}\sin\theta_1\right)^2} - \eta_2\cos\theta_1}
        {\eta_1 \sqrt{1 - \left(
\frac{\eta_1}{\eta_2}\sin\theta_1\right)^2} + \eta_2\cos\theta_1}
     \right\rvert^2
\end{aligned}
\]</span><span class="eqnos-number">(8)</span></span></p>
<p>donde los subíndices <span class="math inline">\(s\)</span> y <span
class="math inline">\(p\)</span> denotan la polarización de la luz:
<span class="math inline">\(s\)</span> es perpendicular a la dirección
de propagación, mientras que <span class="math inline">\(p\)</span> es
paralela.</p>
<p>Generalmente en los ray tracers la polarización se ignora,
promediando ambas expresiones, resultando en una ecuación más
simple:</p>
<p><span id="eq:fresnel_equation" class="eqnos"><span
class="math display">\[
R = \frac{R_s + R_p}{2}
\]</span><span class="eqnos-number">(9)</span></span></p>
<h4 data-number="3.3.3.3" id="la-aproximación-de-schlick"><span
class="header-section-number">3.3.3.3</span> La aproximación de
Schlick</h4>
<p>Como podemos imaginarnos, calcular las expresiones de Fresnel [<a
href="#eq:fresnel_equations">8</a>] no es precisamente barato. En la
práctica, todo el mundo utiliza una aproximación creada por Schlick, la
cual funciona sorprendentemente bien. Viene dada por</p>
<p><span id="eq:schlick_aprox" class="eqnos"><span
class="math display">\[
R(\theta_1) = R_0 + (1 - R_0)(1 - \cos\theta_1)^5
\]</span><span class="eqnos-number">(10)</span></span></p>
<p>siendo <span class="math inline">\(R_0 = R(0)\)</span>; es decir, el
valor que toma <span class="math inline">\(R\)</span> cuando el rayo
incidente es paralelo al medio. Su valor es</p>
<p><span class="math display">\[
R_0 = \left(\frac{\eta_1 - \eta_2}{\eta_1 + \eta_2}\right)^2
\]</span></p>
<p>Esta aproximación es 32 veces más rápida de calcular que las
ecuaciones de Fresnel, generando un error medio inferior al 1% <span
class="citation"
data-cites="https://doi.org/10.1111/1467-8659.1330233">(<a
href="#ref-https://doi.org/10.1111/1467-8659.1330233"
role="doc-biblioref">Schlick 1994</a>)</span></p>
<h3 data-number="3.3.4" id="materiales-híbridos"><span
class="header-section-number">3.3.4</span> Materiales híbridos</h3>
<h3 data-number="3.3.5" id="otros-modelos"><span
class="header-section-number">3.3.5</span> Otros modelos</h3>
<h4 data-number="3.3.5.1" id="oren---nayar"><span
class="header-section-number">3.3.5.1</span> Oren - Nayar</h4>
<p>Este modelo intenta aproximar superficies difusas utilizando un ratio
de lambertiano, lo cual mejora el rendimiento el <em>white furnace
test</em>:</p>
<pre><code>float OrenNayar_diffuse(vec3 normal, vec3 light_dir, vec3 view_dir, material m) {
    float L_dot_V = dot(light_dir, view_dir);
    float N_dot_L = dot(light_dir, noral);
    float N_dot_V = dot(normal, view_dir);

    float s = L_dot_V - N_dot_L * N_dot_V;
    float t = mix(
        1.0,
        ma(N_dot_L, N_dot_V),
        step(0.0, s)
    );

    float sigma2 = m.roughness * m.roughness;
    float A = 1.0 + sigma2 * (m.albedo / (sigma2 + 0.13) + 0.5 / (sigma2 + 0.33));
    float B = 0.45 * sigma2 / (sigma2 + 0.09);

    return m.albedo * max(0.0, N_dot_L) * (A + B * s / t) / PI;
}</code></pre>
<h4 data-number="3.3.5.2" id="ggx"><span
class="header-section-number">3.3.5.2</span> GGX</h4>
<p>El modelo Ground Glass Unknown es una BSDF analítica que se basa en
la distribución de microfacetas del material subyacente. Es una de las
técnicas más avanzadas y exploradas recientemente. Los motores modernos
como Unreal Engine 4 y Unity lo utilizan en sus pipelines físicamente
realistas.</p>
<p>A diferencia de los otros modelos, no entraremos en detalles de la
implementación.</p>
<h2 data-number="3.4" id="la-rendering-equation"><span
class="header-section-number">3.4</span> La rendering equation</h2>
<p>Y, finalmente, tras esta introducción de los principales conceptos
radiométricos, llegamos a la ecuación más importante de todo este
trabajo: la <strong>rendering equation</strong>; también llamada la
<strong>ecuación del transporte de luz</strong>.</p>
<blockquote>
<p><strong>Nota</strong>(ción): esta vez no traduciré el concepto. Es
cierto que afea un poco la escritura teniendo en cuenta que esto es un
texto en castellano. Sin embargo, la otra opción es inventarme una
traducción que nadie usa.</p>
</blockquote>
<p>Antes de comenzar, volvamos a plantear de nuevo la situación: nos
encontramos observando desde nuestra pantalla una escena virtual
mediante la cámara. Queremos saber qué color tomará un pixel específico.
Para conseguirlo, dispararemos rayos desde nuestro punto de vista hacia
el entorno, haciendo que reboten en los objetos. Cuando un rayo impacte
en una superficie, adquirirá parte de las propiedades del material del
objeto. Además, de este rayo surgirán otros nuevos (un rayo dispersado y
otro refractado), que a su vez repetirán el proceso. La información que
se obtiene a partir de estos caminos de rayos nos permitirá darle color
al píxel. Con dicha ecuación, describiremos analíticamente cómo ocurre
esto.</p>
<p>Un último concepto más: denotemos por <span
class="math inline">\(L_e(p, \omega_o)\)</span> a <strong>la radiancia
producida por los materiales emisivos</strong>. En esencia, estos
materiales son fuentes de luz, pues emiten radiancia por sí mismos.</p>
<p>La <em>rendering equation</em> viene dada por la siguiente
expresión:</p>
<p><span id="eq:rendering_equation" class="eqnos"><span
class="math display">\[
L_o(p, \omega_o) = L_e(p, \omega_o) + \int_{H^2(\mathbf{n})}{f(p,
\omega_o \leftarrow \omega_i) L_i(p, \omega_i) \cos\theta_i\ d\omega_i}
\]</span><span class="eqnos-number">(11)</span></span></p>
<p>Para hacerla operativa en términos computacionales podemos
transformarla un poco. Bien, partamos de la ecuación de para la
radiancia reflejada:</p>
<p><span class="math display">\[
L_o(p, \omega_o) = \int_{H^2(\mathbf{n})}{f(p, \omega_o \leftarrow
\omega_i) L_i(p, \omega_i) \cos\theta_i\ d\omega_i}
\]</span></p>
<p>Vamos a buscar expresar la radiancia incidente en términos de la
radiancia reflejada. Para ello, usamos la propiedad de que la radiancia
a lo largo de un rayo no cambia.</p>
<p>Si a una superficie le llega un fotón desde alguna parte, debe ser
porque <em>“alguien”</em> ha tenido que emitirlo. El fotón
necesariamente ha llegado a partir de un rayo. La propiedad nos dice que
la radiancia no ha podido cambiar en el camino.</p>
<p>Pues bien, consideremos una función <span class="math inline">\(r:
\mathbb{R}^3 \times \Omega \to \mathbb{R}^3\)</span> tal que, dado un
punto <span class="math inline">\(p\)</span> y una dirección <span
class="math inline">\(\omega\)</span>, devuelve el siguiente punto de
impacto en una superficie. En esencia, es una función de <em>ray
casting</em> <span class="citation"
data-cites="pellacini-marschner-2017">(<a
href="#ref-pellacini-marschner-2017" role="doc-biblioref">Fabio
Pellacini 2022</a>, Path Tracing)</span>.</p>
<p>Esta función nos permite expresar el punto anterior de la siguiente
forma:</p>
<p><span class="math display">\[
L_i(p, \omega) = L_o(r(p, \omega), -\omega)
\]</span></p>
<p>Esto nos permite cambiar la expresión de <span
class="math inline">\(L_i\)</span> en la integral anterior:</p>
<p><span class="math display">\[
L_o(p, \omega_o) = \int_{H^2(\mathbf{n})}{f(p, \omega_o \leftarrow
\omega_i) L_o(r(p, \omega_i), -\omega_i) \cos\theta_i\ d\omega_i}
\]</span></p>
<p>Finalmente, la radiancia total vendrá dada por la suma de la
radiancia emitida y la reflejada:</p>
<p><span class="math display">\[
L(p, \omega_o) = L_e(p, \omega_o) + \int_{H^2(\mathbf{n})}{f(p, \omega_o
\leftarrow \omega_i) L_o(r(p, \omega_i), -\omega_i) \cos\theta_i\
d\omega_i}
\]</span></p>
<p>Y con esto, ¡hemos obtenido la <em>rendering equation</em>!</p>
<p>Si quieres ver gráficamente cómo funciona, te recomiendo pasarte por
<span class="citation" data-cites="arneback-2019">(<a
href="#ref-arneback-2019" role="doc-biblioref">Arnebäck
2019</a>)</span>. Es un vídeo muy intuitivo.</p>
<iframe width="784" height="441" src="https://www.youtube-nocookie.com/embed/eo_MTI-d28s" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
<p>Si nos paramos a pensar, la ecuación de reflexión es muy similar a la
de renderizado. Sin embargo, hay un par de matices que las hacen muy
diferentes:</p>
<ul>
<li>La ecuación de reflexión describe cómo se comporta la luz reflejada
en un cierto punto. Es decir, tiene un ámbito local. Además, para
calcular la radiancia reflejada, se necesita conocer la radiancia
incidente.</li>
<li>La <em>rendering equation</em> calcula las condiciones globales de
la luz. Además, no se conocen las radiancias de salida.</li>
</ul>
<p>Este último matiz es importante. Para renderizar una imagen, se
necesita calcular la radiancia de salida para aquellos puntos visibles
desde nuestra cámara.</p>
<h1 data-number="4" id="métodos-de-monte-carlo"><span
class="header-section-number">4</span> Métodos de Monte Carlo</h1>
<p>Como vimos en el capítulo anterior, la clave para conseguir una
imagen en nuestro ray tracer es calcular la cantidad de luz en un punto
de la escena. Para ello, necesitamos hallar la radiancia en dicha
posición mediante la <em>rendering equation</em>. Sin embargo, es
<em>muy</em> difícil resolverla; tanto computacional como
analíticamente. Por ello, debemos atacar el problema desde otro punto de
vista.</p>
<p>Las técnicas de Monte Carlo nos permitirán aproximar el valor que
toman las integrales mediante una estimación. Utilizando muestreo
aleatorio para evaluar puntos de una función, seremos capaces de obtener
un resultado suficientemente bueno.</p>
<p>Una de las propiedades que hacen interesantes a este tipo de métodos
es la <strong>independencia del ratio de convergencia y la
dimensionalidad del integrando</strong>. Sin embargo, conseguir un mejor
rendimiento tiene un precio a pagar. Dadas <span
class="math inline">\(n\)</span> muestras, la convergencia a la solución
correcta tiene un orden de <span
class="math inline">\(\mathcal{O}\left(n^{-1/2}\right) =
\mathcal{O}\left(\frac{1}{\sqrt{n}}\right)\)</span>. Es decir, para
reducir el error a la mitad, necesitaríamos 4 veces más muestras.</p>
<p>En este capítulo veremos los fundamentos de la integración de Monte
Carlo, cómo muestrear distribuciones específicas y métodos para afinar
el resultado final.</p>
<h2 data-number="4.1" id="repaso-de-probabilidad"><span
class="header-section-number">4.1</span> Repaso de probabilidad</h2>
<p>Antes de comenzar a fondo, necesitaremos unas nociones de variable
aleatoria para poder entender la integración de Monte Carlo, por lo que
vamos a hacer un breve repaso.</p>
<p>Una <strong>variable aleatoria</strong> <span
class="math inline">\(X\)</span> (v.a.) es, esencialmente, una regla que
asigna un valor numérico a cada posibilidad de un proceso de azar.
Formalmente, es una función definida en un espacio de probabilidad <span
class="math inline">\((\Omega, \mathcal{A}, P)\)</span> asociado a un
experimento aleatorio:</p>
<p><span class="math display">\[
X: \Omega \rightarrow \mathbb{R}
\]</span></p>
<p>A <span class="math inline">\(\Omega\)</span> lo conocemos como
espacio muestral (conjunto de todas las posibilidades), <span
class="math inline">\(\mathcal{A}\)</span> es una <span
class="math inline">\(\sigma\)</span>-álgebra de subconjuntos de <span
class="math inline">\(\Omega\)</span> que refleja todas las
posibilidades de eventos aleatorios, y <span
class="math inline">\(P\)</span> es una función probabilidad, que asigna
a cada evento una probabilidad.</p>
<p>Una variable aleatoria <span class="math inline">\(X\)</span> puede
clasificarse atendiendo a cómo sea su rango <span
class="math inline">\(R_X = \left\{x \in \mathbb{R} \,\middle|\, \exists
\omega \in \Omega \text{ tal que } X(\omega) = x \right\}\)</span>: en
discreta o continua.</p>
<h3 data-number="4.1.1" id="variables-aleatorias-discretas"><span
class="header-section-number">4.1.1</span> Variables aleatorias
discretas</h3>
<p>Las v.a. discretas son aquellas cuyo rango es un conjunto
discreto.</p>
<p>Para comprender mejor cómo funcionan, pongamos un ejemplo:
Consideremos un experimento en el que lanzamos dos dados, anotando lo
que sale en cada uno. Los posibles valores que toman serán <span
class="citation" data-cites="galvin-no-date">(<a
href="#ref-galvin-no-date" role="doc-biblioref">Galvin
n.d.</a>)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
\Omega = \{ &amp; (1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6),  \\
   &amp; (2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (2, 6),  \\
   &amp; (3, 1), (3, 2), (3, 3), (3, 4), (3, 5), (3, 6),  \\
   &amp; (4, 1), (4, 2), (4, 3), (4, 4), (4, 5), (4, 6),  \\
   &amp; (5, 1), (5, 2), (5, 3), (5, 4), (5, 5), (5, 6),  \\
   &amp; (6, 1), (6, 2), (6, 3), (6, 4), (6, 5), (6, 6)   \}
\end{aligned}
\]</span></p>
<p>Cada resultado tiene la misma probabilidad de ocurrir (claro está, si
el dado no está trucado). Como hay <span
class="math inline">\(36\)</span> posibilidades, la probabilidad de
obtener un cierto valor es de <span
class="math inline">\(\frac{1}{36}\)</span>.</p>
<p>La v.a. <span class="math inline">\(X\)</span> denotará la suma de
los valores obtenidos en cada uno. Así, por ejemplo, si al lanzar los
dados hemos obtenido <span class="math inline">\((1, 3)\)</span>, <span
class="math inline">\(X\)</span> tomará el valor <span
class="math inline">\(4\)</span>. En total, <span
class="math inline">\(X\)</span> puede tomar todos los valores
comprendidos entre <span class="math inline">\(2\)</span> y <span
class="math inline">\(12\)</span>. Cada pareja no está asociada a un
único valor de <span class="math inline">\(X\)</span>. Por ejemplo,
<span class="math inline">\((1, 2)\)</span> suma lo mismo que <span
class="math inline">\((2, 1)\)</span>. Esto nos lleva a preguntarnos…
¿Cuál es la probabilidad de que <span class="math inline">\(X\)</span>
adquiera un cierto valor?</p>
<p>La <strong>función masa de probabilidad</strong> nos permite conocer
la probabilidad de que <span class="math inline">\(X\)</span> tome un
cierto valor <span class="math inline">\(x\)</span>. Se denota por <span
class="math inline">\(P(X = x)\)</span>.</p>
<p>También se suele usar <span class="math inline">\(p_X(x)\)</span> o,
directamente <span class="math inline">\(p(x)\)</span>, cuando no haya
lugar a dudas. Sin embargo, en este trabajo reservaremos este nombre a
otro tipo de funciones.</p>
<blockquote>
<p><strong>Nota</strong>(ción): Cuando <span
class="math inline">\(X\)</span> tenga una cierta función masa de
probabilidad, escribiremos <span class="math inline">\(X \sim
p_X\)</span></p>
</blockquote>
<p>En este ejemplo, la probabilidad de que <span
class="math inline">\(X\)</span> tome el valor <span
class="math inline">\(4\)</span> es</p>
<p><span class="math display">\[
\begin{aligned}
P(X = 4) &amp; = \sum{\small{\text{nº parejas que suman 4}} \cdot
\small{\text{probabilidad de que salga la pareja}}} \\
         &amp; = 3 \cdot \frac{1}{36} = \frac{1}{12}
\end{aligned}
\]</span></p>
<p>Las parejas serían <span class="math inline">\((1, 3), (2,
2)\)</span> y <span class="math inline">\((3, 1)\)</span>.</p>
<p>Por definición, si el conjunto de valores que puede tomar <span
class="math inline">\(X\)</span> es <span class="math inline">\(\{x_1,
\dots, x_n\}\)</span>, la función masa de probabilidad debe cumplir
que</p>
<p><span class="math display">\[
\sum_{i = 1}^{n}{P(X = x_i)} = 1
\]</span></p>
<p>Muchas veces nos interesará conocer la probabilidad de que <span
class="math inline">\(X\)</span> se quede por debajo de un cierto valor
<span class="math inline">\(x\)</span> (de hecho, podemos caracterizar
distribuciones aleatorias gracias a esto). Para ello, usamos la
<strong>función de distribución</strong>:</p>
<p><span class="math display">\[
F_X(x) = P(X \le x) = \sum_{\substack{k \in \mathbb{R} \\ k \le x}}{P(X
= k)}
\]</span></p>
<p>Es una función continua por la derecha y monótona no decreciente.
Además, se cumple que <span class="math inline">\(0 \le F_X \le
1(x)\)</span> y <span class="math inline">\(\lim_{x \to -\infty}{F_X} =
0\)</span>, <span class="math inline">\(\lim_{x \to \infty}{F_X} =
1\)</span>.</p>
<p>En nuestro ejemplo, si consideramos <span class="math inline">\(x =
3\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
F_X(x) &amp; = \sum_{i = 1}^{3}{P(X = i)} = P(X = 1) + P(X = 2) + P(X =
3) \\
       &amp; = \frac{1}{36} + \frac{2}{36} + \frac{3}{36} = \frac{1}{12}
\end{aligned}
\]</span></p>
<h3 data-number="4.1.2" id="variables-aleatorias-continuas"><span
class="header-section-number">4.1.2</span> Variables aleatorias
continuas</h3>
<p>Este tipo de variables aleatorias tienen un rango no numerable; es
decir, el conjunto de valores que puede tomar abarca un intervalo de
números.</p>
<p>Un ejemplo podría ser la altura de una persona.</p>
<p>Si en las variables aleatorias discretas teníamos funciones masa de
probabilidad, aquí definiremos las <strong>funciones de densidad de
probabilidad</strong> (o simplemente, funciones de densidad). La idea es
la misma: nos permite conocer la probabilidad de que nuestra variable
aleatoria tome un cierto valor del espacio muestral.</p>
<p>Es importante mencionar que, aunque <em>la probabilidad de que la
variable aleatoria tome un valor específico</em> es <span
class="math inline">\(0\)</span>, ya que nos encontramos en un conjunto
no numerable, sí que podemos calcular la probabilidad de que se
encuentre entre dos valores. Por tanto, si la función de densidad es
<span class="math inline">\(f_X\)</span>, entonces</p>
<p><span class="math display">\[
P(a \le X \le b) = \int_{a}^{b}{f_X(x)dx}
\]</span></p>
<p>La función de densidad tiene dos características importantes:</p>
<ol type="1">
<li><span class="math inline">\(f_X\)</span> es no negativa; esto es,
<span class="math inline">\(f_X(x) \ge 0\ \forall x \in
\mathbb{R}\)</span></li>
<li><span class="math inline">\(f_X\)</span> integra uno en todo <span
class="math inline">\(\mathbb{R}\)</span>:</li>
</ol>
<p><span class="math display">\[
\int_{-\infty}^{\infty}{f_X(x)} = 1
\]</span></p>
<p>Estas dos propiedades caracterizan a una función de densidad; es
decir, toda función <span class="math inline">\(f: \mathbb{R}
\rightarrow \mathbb{R}\)</span> no negativa e integrable tal que <span
class="math inline">\(\int_{\infty}^{\infty}{f_X(x)} = 1\)</span> es la
función de densidad de alguna variable continua.</p>
<p>Intuitivamente, podemos ver esta última propiedad como <em>si
acumulamos todos los valores que puede tomar la variable aleatoria, la
probabilidad de que te encuentres en el conjunto debe ser 1</em>. Si
tratamos con un conjunto de números reales, podemos escribir la integral
como <span class="math inline">\(\int_{-\infty}^{\infty}{f_X(x)} =
1\)</span>.</p>
<p>Una de las variables aleatorias que más juego nos darán en el futuro
será la <strong>v.a. con distribución uniforme en <span
class="math inline">\([0, 1)\)</span></strong>. La denotaremos <span
class="math inline">\(\Xi \sim U\left([0, 1)\right)\)</span>. La
probabilidad de que <span class="math inline">\(\xi\)</span> tome un
valor es constante, por lo que podemos definir su función de densidad
como</p>
<p><span class="math display">\[
f_\Xi(\xi) = \left\{  \begin{array}{llc}
                  1 &amp; \text{si } \xi \in [0, 1) \\
                  0 &amp; \text{en otro caso.}
                  \end{array}
         \right.
\]</span></p>
<p>La probabilidad de <span class="math inline">\(\Xi\)</span> tome un
valor entre dos elementos <span class="math inline">\(a, b \in [0,
1)\)</span> es</p>
<p><span class="math display">\[
P(\Xi \in [a, b]) = \int_{a}^{b}{1dx} = b - a
\]</span></p>
<p>Como veremos más adelante, definiendo correctamente una función de
densidad conseguiremos mejorar el rendimiento del path tracer.</p>
<p>La función de distribución <span
class="math inline">\(F_X(x)\)</span> podemos definirla como:</p>
<p><span class="math display">\[
F_X(x) = P(X \le x) = \int_{-\infty}^{x}{f_X(t)dt}
\]</span></p>
<p>Es decir, dado un <span class="math inline">\(x\)</span>, ¿cuál sería
la probabilidad de que <span class="math inline">\(X\)</span> se quede
por debajo de <span class="math inline">\(x\)</span>?</p>
<p>El Teorema Fundamental del Cálculo nos permite relacionar función de
distribución y función de densidad directamente:</p>
<p><span class="math display">\[
f_X(x) = \frac{dF_X(x)}{dx}
\]</span></p>
<h3 data-number="4.1.3"
id="esperanza-y-varianza-de-una-variable-aleatoria"><span
class="header-section-number">4.1.3</span> Esperanza y varianza de una
variable aleatoria</h3>
<p>La <strong>esperanza de una variable aleatoria</strong>, denotada
<span class="math inline">\(E\left[ X \right]\)</span>, es una
generalización de la media ponderada. Nos informa del <em>valor
esperado</em> de dicha variable aleatoria.</p>
<p>En el caso de las variables discretas, se define como</p>
<p><span class="math display">\[
E\left[ X \right] = \sum_{i}{x_i p_i}
\]</span></p>
<p>donde <span class="math inline">\(x_i\)</span> son los posibles
valores que puede tomar la v.a., y <span
class="math inline">\(p_i\)</span> la probabilidad asociada a cada uno
de ellos; es decir, <span class="math inline">\(p_i = P[X =
x_i]\)</span></p>
<p>Para una variable aleatoria continua real, la esperanza viene dada
por</p>
<p><span class="math display">\[
E\left[ X \right] = \int_{-\infty}^{\infty}{x f_X(x) dx}
\]</span></p>
<p>Pongamos un par de ejemplos del cálculo de la esperanza. En el <a
href="#variables-aleatorias-discretas">ejemplo de las variables
discretas</a>, la esperanza venía dada por</p>
<p><span class="math display">\[
E\left[ X \right] = \sum_{i = 2}^{12}{i P[X = i]} = 2\frac{1}{36} + 3
\frac{2}{36} + \dots + 12 \frac{1}{36} = 7
\]</span></p>
<p>Para variables aleatorias uniformes en <span
class="math inline">\((a, b)\)</span> (es decir, <span
class="math inline">\(X \sim U(a, b)\)</span>), la esperanza es</p>
<p><span class="math display">\[
E\left[ X \right] = \int_{a}^{b}{x \frac{1}{b - a}dx} = \frac{a + b}{2}
\]</span></p>
<p>La esperanza tiene unas cuantas propiedades que nos resultarán muy
útiles. Estas son:</p>
<ul>
<li><strong>Linealidad</strong>:
<ul>
<li>Si <span class="math inline">\(X, Y\)</span> son dos v.a., <span
class="math inline">\(E\left[ X + Y \right] = E\left[ X \right] +
E\left[ Y \right]\)</span></li>
<li>Si <span class="math inline">\(a\)</span> es una constante, <span
class="math inline">\(X\)</span> una v.a., entonces <span
class="math inline">\(E\left[ aX \right] = aE\left[ X
\right]\)</span></li>
<li>Análogamente, para ciertas <span class="math inline">\(X_1, \dots,
X_k\)</span>, <span class="math inline">\(E\left[ \sum_{i = 1}^{k}{X_i}
\right] = \sum_{i = 1}^{k}{E\left[ X_i \right]}\)</span></li>
<li>Estas propiedades no necesitan que las variables aleatorias sean
independientes. Este hecho será clave para las técnicas de Monte
Carlo.</li>
</ul></li>
</ul>
<p>Será habitual encontrarnos con el problema de que no conocemos la
distribución de una variable aleatoria <span
class="math inline">\(Y\)</span>. Sin embargo, si encontramos una
transformación medible de una variable aleatoria <span
class="math inline">\(X\)</span> de forma que obtengamos <span
class="math inline">\(Y\)</span> (esto es, <span
class="math inline">\(\exists g\)</span> función medible tal que <span
class="math inline">\(g(X) = Y\)</span>), entonces podemos calcular la
esperanza de <span class="math inline">\(Y\)</span> fácilmente. Esta
propiedad hará que las variables aleatorias con distribución uniforme
adquieran muchísima importancia. Generar números aleatorios en <span
class="math inline">\([0, 1)\)</span> es muy fácil, así <a
href="#método-de-la-transformada-inversa">que obtendremos otras vv.aa a
partir de <span class="math inline">\(\xi\)</span></a>.</p>
<p>Otra medida muy útil de una variable aleatoria es <strong>la
varianza</strong>. Nos permitirá medir cómo de dispersa es la
distribución con respecto a su media. La denotaremos como <span
class="math inline">\(Var[X]\)</span>, y se define como</p>
<p><span class="math display">\[
Var[X] = E\left[ (X - E\left[ X \right])^2 \right]
\]</span></p>
<p>Si desarrollamos esta definición, podemos conseguir una expresión
algo más agradable:</p>
<p><span class="math display">\[
\begin{aligned}
   Var[X] &amp; = E\left[ (X - E\left[ X \right])^2 \right] = \\
          &amp; = E\left[ X^2 + E\left[ X \right]^2 - 2XE\left[ X
\right] \right] = \\
          &amp; = E\left[ X^2 \right] + E\left[ X \right]^2 - 2E\left[ X
\right]E\left[ X \right] = \\
          &amp; = E\left[ X^2\right] - E\left[X \right]^2
\end{aligned}
\]</span></p>
<p>Hemos usado que <span class="math inline">\(E\left[ E\left[ X \right]
\right] = E\left[ X \right]\)</span> y la linealidad de la
esperanza.</p>
<p>Enunciemos un par de propiedades que tiene, similares a la de la
esperanza:</p>
<ul>
<li>La varianza saca constantes al cuadrado: <span
class="math inline">\(Var[aX] = a^2Var[X]\)</span></li>
<li><span class="math inline">\(Var[X + Y] =\)</span> <span
class="math inline">\(Var[X] + Var[Y] + 2Cov[X, Y]\)</span>, donde <span
class="math inline">\(Cov[X, Y]\)</span> es la covarianza de <span
class="math inline">\(X\)</span> y <span
class="math inline">\(Y\)</span>.
<ul>
<li>En el caso en el que <span class="math inline">\(X\)</span> e <span
class="math inline">\(Y\)</span> sean incorreladas (es decir, la
covarianza es <span class="math inline">\(0\)</span>), <span
class="math inline">\(Var[X + Y] =\)</span> <span
class="math inline">\(Var[X] + Var[Y]\)</span>.</li>
</ul></li>
</ul>
<p>La varianza nos será útil a la hora de medir el error cometido por
una estimación de Monte Carlo.</p>
<h3 data-number="4.1.4" id="teoremas-importantes"><span
class="header-section-number">4.1.4</span> Teoremas importantes</h3>
<p>Además de las anteriores propiedades, existen una serie de teoremas
esenciales que necesitaremos más adelante:</p>
<p><strong>Ley del estadístico insconciente</strong> (<em>Law of the
unconscious statistician</em>, o LOTUS): dada una variable aleatoria
<span class="math inline">\(X\)</span> y una función medible <span
class="math inline">\(g\)</span>, la esperanza de <span
class="math inline">\(g(X)\)</span> se puede calcular como</p>
<p><span id="eq:LOTUS" class="eqnos"><span class="math display">\[
E\left[ g(X) \right] = \int_{-\infty}^{\infty}{g(x) f_X(x) dx}
\]</span><span class="eqnos-number">(12)</span></span></p>
<p><strong>Ley (fuerte) de los grandes números</strong>: dada una
muestra de <span class="math inline">\(N\)</span> valores <span
class="math inline">\(X_1, \dots, X_N\)</span> de una variable aleatoria
<span class="math inline">\(X\)</span> con esperanza <span
class="math inline">\(E\left[ X \right] = \mu\)</span>,</p>
<p><span class="math display">\[
P\left[ \lim_{N \to \infty}{\frac{1}{N} \sum_{i = 1}^{N}{X_i}} =
\mu  \right] = 1
\]</span></p>
<p>Usando que <span class="math inline">\(\bar{X}_N = \frac{1}{N}
\sum_{i = 1}^{N}{X_i}\)</span>, esta ley se suele escribir como</p>
<p><span id="eq:ley_numeros_grandes" class="eqnos"><span
class="math display">\[
P\left[ \lim_{N \to \infty}{\bar{X}_N} = \mu  \right] = 1
\]</span><span class="eqnos-number">(13)</span></span></p>
<p>Este teorema es especialmente importante. En esencia, nos dice que
cuando repetimos muchas veces un experimento, al promediar los
resultados obtendremos una esperanza muy cercana a la esperanza
real.</p>
<p><strong>Teorema Central del Límite (CLT) para variables idénticamente
distribuidas</strong> <span class="citation" data-cites="mcbook">(<a
href="#ref-mcbook" role="doc-biblioref">Owen 2013</a>, capítulo
2)</span>: Sean <span class="math inline">\(X_1, \dots, X_N\)</span>
muestras aleatorias simples de una variable aleatoria <span
class="math inline">\(X\)</span> con esperanza <span
class="math inline">\(E\left[ X \right] = \mu\)</span> y varianza <span
class="math inline">\(Var[X] = \sigma^2\)</span>. Sea</p>
<p><span class="math display">\[
Z_N = \frac{\sum_{i = 1}^{N}{X_i - N\mu}}{\sigma \sqrt{N}}
\]</span></p>
<p>Entonces, la variable aleatoria <span
class="math inline">\(Z_N\)</span> converge hacia una función de
distribución normal estándar cuando <span
class="math inline">\(N\)</span> es suficientemente grande:</p>
<p><span id="eq:CLT" class="eqnos"><span class="math display">\[
\lim_{N \to \infty}{P[Z_N \le z]} = \int_{-\infty}^{z}{\frac{1}{\sqrt{2
\pi}} e^{- \frac{x^2}{2}}dx}
\]</span><span class="eqnos-number">(14)</span></span></p>
<h3 data-number="4.1.5" id="estimadores"><span
class="header-section-number">4.1.5</span> Estimadores</h3>
<p>A veces, no podremos conocer de antemano el valor que toma un cierto
parámetro de una distribución. Sin embargo, conocemos el tipo de
distribución que nuestra variable aleatoria <span
class="math inline">\(X\)</span> sigue. Los estimadores nos
proporcionarán una forma de calcular el posible valor de esos parámetros
a partir de una muestra de <span class="math inline">\(X\)</span>.</p>
<p>Sea <span class="math inline">\(X\)</span> una variablea aleatoria
con distribución perteneciente a una familia de distribuciones
paramétricas <span class="math inline">\(X \sim F \in \left\{F(\theta)
\,\middle|\, \theta \in \Theta \right\}\)</span>. <span
class="math inline">\(\Theta\)</span> es el conjunto de valores que
puede tomar el parámetro. Buscamos una forma de determinar el valor de
<span class="math inline">\(\theta\)</span>.</p>
<p>Diremos que <span class="math inline">\(T(X_1, \dots, X_N)\)</span>
es <strong>un estimador de <span
class="math inline">\(\theta\)</span></strong> si <span
class="math inline">\(T\)</span> toma valores en <span
class="math inline">\(\Theta\)</span>.</p>
<p>A los estimadores de un parámetro los solemos denotar con <span
class="math inline">\(\hat{\theta}\)</span>.</p>
<p>Como vemos, la definición no es muy restrictiva. Únicamente le
estamos pidiendo a la función de la muestra que pueda tomar valores
viables para la distribución.</p>
<p>Se dice que un estimador <span class="math inline">\(T(X_1, \dots,
X_N)\)</span> es <strong>insesgado</strong> (o centrado en el parámetro
<span class="math inline">\(\theta\)</span>) si</p>
<p><span class="math display">\[
E\left[ T(X_1, \dots, X_n) \right] = \theta\quad \forall \theta \in
\Theta
\]</span></p>
<p>Naturalmente, decimos que un estimador <span
class="math inline">\(T(X_1, \dots, X_N)\)</span> está
<strong>sesgado</strong> si <span class="math inline">\(E\left[ T(X_1,
\dots, X_N) \right] \not = \theta\)</span>.</p>
<h2 data-number="4.2" id="el-estimador-de-monte-carlo"><span
class="header-section-number">4.2</span> El estimador de Monte
Carlo</h2>
<p>Tras este breve repaso de probabilidad, estamos en condiciones de
definir el estimador de Monte Carlo. Primero, vamos con su versión más
sencilla.</p>
<h3 data-number="4.2.1" id="monte-carlo-básico"><span
class="header-section-number">4.2.1</span> Monte Carlo básico</h3>
<p>Los estimadores de Monte Carlo nos permiten hallar la esperanza de
una variable aleatoria, digamos, <span class="math inline">\(Y\)</span>,
sin necesidad de calcular explícitamente su valor. Para ello, tomamos
<span class="math inline">\(N\)</span> muestras <span
class="math inline">\(Y_1, \dots, Y_N\)</span> de <span
class="math inline">\(Y\)</span>, cuya media vale <span
class="math inline">\(\mu\)</span>. Entonces, el estimador de <span
class="math inline">\(\mu\)</span> <span class="citation"
data-cites="mcbook">(<a href="#ref-mcbook" role="doc-biblioref">Owen
2013</a>, capítulo 2)</span> es:</p>
<p><span id="eq:mc_simple" class="eqnos"><span class="math display">\[
\hat\mu_N = \frac{1}{N} \sum_{i = 1}^{N}{Y_i}
\]</span><span class="eqnos-number">(15)</span></span></p>
<p>La intuición del estimador es, esencialmente, la misma que la del
teorema central del límite. Lo que buscamos es una forma de calcular el
valor promedio de un cierto suceso aleatorio, pero lo único que podemos
usar son muestras de su variable aleatoria. Promediando esas muestras,
sacamos información de la distribución. En este caso, la media.</p>
<p>En cualquier caso, la existencia de este estimador viene dada por la
ley de los grandes números (tanto débil como fuerte [<a
href="#eq:ley_numeros_grandes">13</a>]). Si <span
class="math inline">\(\mu = E\left[ Y \right]\)</span>, se tiene que</p>
<p><span class="math display">\[
\lim_{N \to \infty}P\left[ \left\lvert \hat\mu_N - \mu \right\rvert \le
\varepsilon \right] = 1 \quad \forall\ \varepsilon &gt; 0
\]</span></p>
<p>o utilizando la ley de los números grandes,</p>
<p><span class="math display">\[
\lim_{N \to \infty}P\left[ \left\lvert \hat\mu_N - \mu \right\rvert = 0
\right] = 1
\]</span></p>
<p>Haciendo la esperanza de este estimador, vemos que</p>
<p><span class="math display">\[
\begin{aligned}
E\left[ \hat\mu_N \right] &amp; = E\left[ \frac{1}{N} \sum_{i =
1}^{N}{Y_i}\right] = \frac{1}{N} E\left[\sum_{i = 1}^{N}{Y_i} \right] \\
             &amp; = \frac{1}{N} \sum_{i = 1}^{N}{E\left[ Y_i \right]} =
\frac{1}{N} \sum_{i = 1}^{N}{\mu} = \\
             &amp; = \mu
\end{aligned}
\]</span></p>
<p>Por lo que el estimador es insesgado. Además, se tiene que la
varianza es</p>
<p><span class="math display">\[
E\left[ (\hat\mu_N - \mu)^2 \right] = \frac{\sigma^2}{N}
\]</span></p>
<p>Un ejemplo clásico de estimador de Monte Carlo es calcular el valor
de <span class="math inline">\(\pi\)</span>. Se puede hallar integrando
una función que valga <span class="math inline">\(1\)</span> en el
interior de la circunferencia de radio unidad y <span
class="math inline">\(0\)</span> en el exterior:</p>
<p><span class="math display">\[
\begin{aligned}
f = \begin{cases}
      1 &amp; \text{si } x^2 + y^2 \le 1 \\
      0 &amp; \text{en otro caso}
    \end{cases} \Longrightarrow \pi = \int_{-1}^{1} \int_{-1}^{1}{f(x,
y)}\ dxdy
\end{aligned}
\]</span></p>
<p>Para usar el estimador de [<a href="#eq:mc_integral">16</a>],
necesitamos saber la probabilidad de obtener un punto dentro de la
circunferencia.</p>
<p>Bien, consideremos que una circunferencia de radio <span
class="math inline">\(r\)</span> se encuentra inscrita en un cuadrado.
El área de la circunferencia es <span class="math inline">\(\pi
r^2\)</span>, mientras que la del cuadrado es <span
class="math inline">\((2r)^2 = 4r^2\)</span>. Por tanto, la probabilidad
de obtener un punto dentro de la circunferencia es <span
class="math inline">\(\frac{\pi r^2}{4r^2} = \frac{\pi}{4}\)</span>.
Podemos tomar <span class="math inline">\(p(x, y) =
\frac{1}{4}\)</span>, de forma que</p>
<p><span class="math display">\[
\pi \approx \frac{4}{N} \sum_{i = 1}^{N}{f(x_i, y_i)}, \text{  con }
(x_i, y_i) \sim U(\small{[-1, 1] \times [-1, 1]})
\]</span></p>
<h3 data-number="4.2.2" id="integración-de-monte-carlo"><span
class="header-section-number">4.2.2</span> Integración de Monte
Carlo</h3>
<p>Generalmente nos encontraremos en la situación en la que <span
class="math inline">\(Y = f(X)\)</span>, donde <span
class="math inline">\(X \in S \subset \mathbb{R}^d\)</span> sigue una
distribución con función de densidad <span
class="math inline">\(p_X(x)\)</span> con media <span
class="math inline">\(\mu = E\left[ X \right]\)</span>, y <span
class="math inline">\(f: S \rightarrow \mathbb{R}\)</span>.</p>
<p>Consideremos el promedio de <span class="math inline">\(N\)</span>
muestras de <span class="math inline">\(f(X)\)</span>:</p>
<p><span class="math display">\[
\frac{1}{N} \sum_{i = 1}^{N}{f(X_i)}, \quad X_i \text{ idénticamente
distribuidas}
\]</span></p>
<p>En ese caso, la esperanza es</p>
<p><span class="math display">\[
\begin{aligned}
E\left[ \frac{1}{N} \sum_{i = 1}^{N}{f(X_i)} \right] &amp; =
E\left[\frac{1}{N} \sum_{i = 1}^{N}{f(X)}  \right] = \\
                                                    &amp; = \frac{1}{N}
N E\left[ f(X) \right] = \\
                                                    &amp; =  E\left[
f(X) \right] = \\
                                                    &amp; = \int_S f(x)
p_X(x) dx
\end{aligned}
\]</span></p>
<p>¡Genial! Esto nos da una forma de <strong>calcular la integral de una
función</strong> usando las imágenes de <span
class="math inline">\(N\)</span> muestras <span
class="math inline">\(f(X_1), \dots, f(X_N)\)</span> de una variable
aleatoria <span class="math inline">\(X \sim p_X\)</span>. A este
estimador de Monte Carlo lo llamaremos <span
class="math inline">\(\hat{I}_N\)</span>:</p>
<p><span id="eq:mc_integral" class="eqnos"><span class="math display">\[
\begin{aligned}
\hat{I}_N &amp; = \frac{1}{N} \sum_{i = 1}^{N}{f(X_i)} \\
          \Rightarrow E\left[ \hat{I}_N \right] &amp; = \int_S f(x)
p_X(x) dx
\end{aligned}
\]</span><span class="eqnos-number">(16)</span></span></p>
<blockquote>
<p><strong>Nota</strong>(ción): si te preguntas por qué lo llamamos
<span class="math inline">\(\hat{I}_N\)</span>, piensa que queremos
calcular la intergal <span class="math inline">\(I =
\int_{S}{f(x)p_X(x)dx}\)</span>. Para ello, usamos el estimador <span
class="math inline">\(\hat{I}\)</span>, y marcamos explícitamente que
usamos <span class="math inline">\(N\)</span> muestras.</p>
</blockquote>
<p>La varianza del estimador se puede calcular fácilmente utilizando las
propiedades que vimos en la <a
href="#esperanza-y-varianza-de-una-variable-aleatoria">sección de la
varianza</a>:</p>
<p><span id="eq:mc_varianza" class="eqnos"><span class="math display">\[
\begin{aligned}
  Var[\hat{I}_N]
    &amp; = Var\left[ \frac{1}{N} \sum_{i = 1}^{N}{f(X_i)} \right] = \\
    &amp; = \frac{1}{N^2} Var\left[  \sum_{i = 1}^{N}{f(X_i)} \right] =
\\
    &amp; = \frac{1}{N^2} N Var\left[ f(X) \right] = \\
    &amp; = \frac{1}{N} Var\left[ f(X) \right]
\end{aligned}
\]</span><span class="eqnos-number">(17)</span></span></p>
<p>Como es natural, el número de muestras que usemos será clave para la
proximidad de la estimación. ¿Cómo <em>de lejos</em> se queda del valor
real de la integral <span class="math inline">\(E\left[ f(X)
\right]\)</span>? Es decir; ¿cómo modifica <span
class="math inline">\(N\)</span> la varianza del estimador <span
class="math inline">\(Var\left[ \hat{I}_N \right]\)</span>?</p>
<p>Para comprobarlo, debemos introducir dos nuevos teoremas: la
desigualdad de Markov y la desigualdad de Chebyshsev <span
class="citation" data-cites="metodos-monte-carlo">(<a
href="#ref-metodos-monte-carlo" role="doc-biblioref">Illana 2013</a>,
Introducción)</span>.</p>
<p><strong>Desigualdad de Markov</strong>: Sea <span
class="math inline">\(X\)</span> una variable aleatoria que toma valores
no negativos, y sea <span class="math inline">\(p_X\)</span> su función
de densidad. Entonces, <span class="math inline">\(\forall x &gt;
0\)</span>,</p>
<p><span id="eq:desigualdad_markov" class="eqnos"><span
class="math display">\[
\begin{aligned}
E\left[ X \right]  &amp;   =  \int_0^x t p_X(t) dt + \int_x^\infty t
p_X(t) dt \ge  \int_x^\infty t p_X(t) \\
      &amp; \ge  \int_x^\infty x p_X(t) = x P\left[ X \ge x \right] \\
      &amp; \Rightarrow P\left[ X \ge x \right] \le \frac{E\left[ X
\right]}{x}
\end{aligned}
\]</span><span class="eqnos-number">(18)</span></span></p>
<p><strong>Desigualdad de Chebyshev</strong>: Sea <span
class="math inline">\(X\)</span> una variable aleatoria con esperanza
<span class="math inline">\(\mu = E\left[ X \right]\)</span> y varianza
<span class="math inline">\(\sigma^2 = E\left[ (X - \mu)^2
\right]\)</span>. Entonces, aplicando la desigualdad de Markov [<a
href="#eq:desigualdad_markov">18</a>] a <span class="math inline">\(D^2
= (X - \mu)^2\)</span> se tiene que</p>
<p><span id="eq:desigualdad_chebyshev" class="eqnos"><span
class="math display">\[
\begin{aligned}
       P\left[ D^2 \ge x^2 \right]         &amp; \le
\frac{\sigma^2}{x^2} \\
  \iff P\left[ \left\lvert X - \mu \right\rvert \ge x \right] &amp; \le
\frac{\sigma^2}{x^2}
\end{aligned}
\]</span><span class="eqnos-number">(19)</span></span></p>
<p>Ahora que tenemos estas dos desigualdades, apliquemos la de Chebyshev
a [<a href="#eq:mc_integral">16</a>] con <span
class="math inline">\(\sigma^2 = Var\left[ \hat{I}_N \right]\)</span>,
<span class="math inline">\(x^2 = \sigma^2/\varepsilon, \varepsilon &gt;
0\)</span>:</p>
<p><span class="math display">\[
P\left[ \left\lvert \hat{I}_N - E\left[ \hat{I}_N \right] \right\rvert
\ge \left(\frac{Var[\hat{I}_N]}{\varepsilon}\right)^{1/2} \right] \le
\varepsilon
\]</span></p>
<p>Esto nos dice que, usando un número de muestras relativamente grande
(<span class="math inline">\(N &gt;&gt; \frac{1}{\varepsilon}\)</span>),
es prácticamente imposible que el estimador se aleje de <span
class="math inline">\(E\left[ f(X) \right]\)</span>.</p>
<p>La desviación estándar puede calcularse fácilmente a partir de la
varianza:</p>
<p><span id="eq:desviacion_estandar" class="eqnos"><span
class="math display">\[
\sqrt{Var[\hat{I}_N]} = \frac{\sqrt{Var\left[ f(X) \right]}}{\sqrt{N}}
\]</span><span class="eqnos-number">(20)</span></span></p>
<p>así que, como adelantamos al inicio del capítulo, la estimación tiene
un error del orden <span
class="math inline">\(\mathcal{O}(N^{-1/2})\)</span>. Esto nos dice que,
para reducir el error a la mitad, debemos tomar 4 veces más
muestras.</p>
<p>Es importante destacar la <strong>ausencia del parámetro de la
dimensión</strong>. Sabemos que <span class="math inline">\(X \in S
\subset \mathbb{R}^d\)</span>, pero en ningún momento aparece <span
class="math inline">\(d\)</span> en la expresión de la desviación
estándar [<a href="#eq:desviacion_estandar">20</a>]. Este hecho es una
de las ventajas de la integración de Monte Carlo.</p>
<h4 data-number="4.2.2.1" id="un-ejemplo-práctico-en-r"><span
class="header-section-number">4.2.2.1</span> Un ejemplo práctico en
R</h4>
<p>Hagamos un ejemplo práctico para visualizar lo que hemos aprendido en
el software estadístico <strong>R</strong>.</p>
<p>Supongamos que queremos integrar la función <span
class="math inline">\(f: [0, 1] \rightarrow \mathbb{R}\)</span>, <span
class="math inline">\(f(x) = 2x^4\)</span>. Es decir, queremos
calcular</p>
<p><span class="math display">\[
\int_0^1{2x^4\ dx}
\]</span></p>
<p>El valor de esta integral es <span class="math inline">\(2
\left[\frac{x^5}{5}\right]_0^1 = 2/5 = 0.4\)</span>.</p>
<p>Primero, definimos la función <span
class="math inline">\(f\)</span>:</p>
<pre><code class="language-r">f &lt;- function(x) {
  2 * x^4 * (x &gt; 0 &amp; x &lt; 1)
}</code></pre>
<p>Tomamos N muestras en el intervalo <span class="math inline">\([0,
1]\)</span> de forma uniforme:</p>
<pre><code class="language-r">N &lt;- 1000
x &lt;- runif(N)       # x1, ...., xn
f_x &lt;- sapply(x, f) # f(x1), ..., f(xn)
mean(f_x)           # -&gt; 0.3891845</code></pre>
<p>Observamos que el valor se queda muy cerca de <span
class="math inline">\(0.4\)</span>. El error en este caso es <span
class="math inline">\(0.4 - 0.3891845 = 0.01081546\)</span>.</p>
<p>Es interesante estudiar cómo de rápido converge el estimador al valor
de la integral. Con el siguiente código, podemos caclular el error en
función del número de muestras <span
class="math inline">\(N\)</span>:</p>
<pre><code class="language-r"># Calcular la media y su error
estimacion &lt;- cumsum(f_x) / (1:N)
error &lt;- sqrt(cumsum((f_x - estimacion)^2)) / (1:N)

# Gráfico
plot(1:N, estimacion,
    type = &quot;l&quot;,
    ylab = &quot;Aproximación y límites del error (1 - alpha = 0.975)&quot;,
    xlab = &quot;Número de simulaciones&quot;,
)
z &lt;- qnorm(0.025, lower.tail = FALSE)
lines(estimacion - z * error, col = &quot;blue&quot;, lwd = 2, lty = 3)
lines(estimacion + z * error, col = &quot;blue&quot;, lwd = 2, lty = 3)
abline(h = 0.4, col = 2)</code></pre>
<p>Este código produce la siguiente gráfica:</p>
<div id="fig:error_simulacion" class="fignos">
<figure>
<img loading="lazy" src="./img/03/Error%20simulación.png"
alt="Figura 11: Error de la simulación para el estimador de la integral \int_0^1{2x^4\ dx}" />
<figcaption aria-hidden="true"><span>Figura 11:</span> Error de la
simulación para el estimador de la integral <span
class="math inline">\(\int_0^1{2x^4\ dx}\)</span></figcaption>
</figure>
</div>
<p>Se puede ver cómo debemos usar un número considerable de muestras,
alrededor de 200, para que el error se mantenga bajo control. Aún así,
aumentar el tamaño de <span class="math inline">\(N\)</span> no
disminuye necesariamente el error; nos encontramos en una situación de
retornos reducidos.</p>
<h2 data-number="4.3" id="técnicas-de-reducción-de-varianza"><span
class="header-section-number">4.3</span> Técnicas de reducción de
varianza</h2>
<h3 data-number="4.3.1" id="muestreo-por-importancia"><span
class="header-section-number">4.3.1</span> Muestreo por importancia</h3>
<p>Como hemos visto, <span class="math inline">\(Var\left[ \hat{I}_N
\right]\)</span> depende del número de muestras <span
class="math inline">\(N\)</span> y de <span
class="math inline">\(Var\left[ f(X) \right]\)</span>. Aumentar el
tamaño de <span class="math inline">\(N\)</span> es una forma fácil de
reducir la varianza, pero rápidamente llegaríamos a una situación de
retornos reducidos <span class="citation" data-cites="PBRT3e">(<a
href="#ref-PBRT3e" role="doc-biblioref">Pharr, Jakob, and Humphreys
2016</a>, The Monte Carlo Estimator)</span>. ¿Podemos hacer algo con el
término <span class="math inline">\(Var\left[ f(X) \right]\)</span>?</p>
<p>Vamos a jugar con él.</p>
<p>La integral que estamos evaluando ahora mismo es <span
class="math inline">\(\int_S{f(x)p_X(x)}dx\)</span>, con <span
class="math inline">\(p_X\)</span> una función de densidad sobre <span
class="math inline">\(S \subset \mathbb{R}^d\)</span> <span
class="math inline">\(\Rightarrow p_X = 0\ \forall x \notin S\)</span>.
Ahora bien, si <span class="math inline">\(q_X\)</span> es otra función
de densidad en <span class="math inline">\(\mathbb{R}^d\)</span>,
entonces <span class="citation" data-cites="mcbook">(<a
href="#ref-mcbook" role="doc-biblioref">Owen 2013</a>, Importance
Sampling)</span>:</p>
<p><span class="math display">\[
I = \int_S{f(x)p_X(x)dx} = \int_S{\frac{f(x)p_X(x)}{q_X(x)}q_X(x)dx} =
E\left[ \frac{f(X)p_X(X)}{q_X(X)} \right]
\]</span></p>
<p>Esta última esperanza depende de <span
class="math inline">\(q_X\)</span>. Nuestro objetivo era encontrar <span
class="math inline">\(E\left[ f(X) \right]\)</span>, pero podemos
hacerlo tomando un término nuevo para muestrear desde <span
class="math inline">\(q_X\)</span> en vez de <span
class="math inline">\(p_X\)</span>. Al factor <span
class="math inline">\(\frac{p_X}{q_X}\)</span> lo llamamos
<strong>cociente de probabilidad</strong>, con <span
class="math inline">\(q_X\)</span> la <strong>distribución de
importancia</strong> y <span class="math inline">\(p_X\)</span> la
<strong>distribución nominal</strong>.</p>
<p>No es necesario que <span class="math inline">\(q_X\)</span> sea
positiva en todo punto. Con que se cumpla que <span
class="math inline">\(q_X(x) &gt; 0\)</span> cuando <span
class="math inline">\(f(x)p_X(x) \not = 0\)</span> es suficiente. Es
decir, para <span class="math inline">\(Q = \left\{x \,\middle|\, q_X(x)
&gt; 0  \right\}\)</span>, tenemos que <span class="math inline">\(x \in
Q\)</span> cuando <span class="math inline">\(f(x)p_X(x) \not =
0\)</span>. Así, si <span class="math inline">\(x \in S \cap Q^c
\Rightarrow f(X) = 0\)</span>, mientras que si <span
class="math inline">\(x \in S^c \cap Q \Rightarrow p_X(X) \neq
0\)</span>. Entonces,</p>
<p><span class="math display">\[
\begin{aligned}
E\left[ \frac{f(X)p_X(X)}{q_X(X)} \right] &amp; =
\int_Q{\frac{f(x)p_X(x)}{q_X(x)}q_X(x)dx} = \int_Q{f(x)p_X(x)dx} = \\
                              &amp; = \int_Q{f(x)p_X(x)dx} + \int_{S^c
\cap Q}{f(x)p_X(x)dx} - \int_{S \cap Q^c}{f(x)p_X(x)dx} =  \\
                              &amp; = \int_S{f(x)p_X(x)dx} = \\
                              &amp; = E\left[ f(X) \right]
\end{aligned}
\]</span></p>
<p>De esta forma, hemos llegado al <strong>estimador de Monte Carlo por
importancia</strong>:</p>
<p><span id="eq:mc_integral_importancia" class="eqnos"><span
class="math display">\[
\tilde{I}_N = \frac{1}{N} \sum_{i=1}^N{\frac{f(X_i)p_X(X_i)}{q_X(X_i)}},
\quad X_i \sim q_X
\]</span><span class="eqnos-number">(21)</span></span></p>
<blockquote>
<p><strong>Nota</strong>(ción): ¡fíjate en el gusanito! <span
class="math inline">\(\hat{I}_N\)</span> [<a
href="#eq:mc_integral">16</a>] y <span
class="math inline">\(\tilde{I}_N\)</span> tienen la misma esperanza,
pero son estimadores diferentes.</p>
</blockquote>
<p>Vamos a calcular ahora la varianza de este estimador. Sea <span
class="math inline">\(\mu = E\left[ f(X) \right]\)</span></p>
<p><span class="math display">\[
\begin{aligned}
Var\left[ \tilde{I}_N \right] &amp; = \frac{1}{N} \left(
\int_Q{\left(\frac{f(x)p_X(x)}{q_X(x)}\right)^2q_X(x) dx} -
\mu^2  \right) = \\
                  &amp; = \frac{1}{N}
\textcolor{verde-oscurisimo}{\left(
\int_Q{\frac{\left(f(x)p_X(x)\right)^2}{q_X(x)} dx} - \mu^2  \right)} =
\\
                  &amp; =
\frac{\textcolor{verde-oscurisimo}{\sigma^2_q}}{N}
\end{aligned}
\]</span></p>
<p>La clave de este método reside en escoger una buena distribución de
importancia. Puede probarse que la función de densidad que minimiza
<span class="math inline">\(\sigma^2_q\)</span> es proporcional a <span
class="math inline">\(\left\lvert f(x) \right\rvert p_X(x)\)</span>
<span class="citation" data-cites="mcbook">(<a href="#ref-mcbook"
role="doc-biblioref">Owen 2013, 6</a>)</span>.</p>
<h4 data-number="4.3.1.1"
id="muestreo-por-importancia-en-transporte-de-luz"><span
class="header-section-number">4.3.1.1</span> Muestreo por importancia en
transporte de luz</h4>
<p>Esta técnica es especialmente importante en nuestra área de estudio.
En transporte de luz, buscamos calcular el valor de la rendering
equation [<a href="#eq:rendering_equation">11</a>]. Específicamente, de
la integral</p>
<p><span class="math display">\[
\int_{H^2(\mathbf{n})}{BSDF(p, \omega_o \leftarrow \omega_i) L_i(p,
\omega_i) \cos\theta_i\ d\omega_i}
\]</span></p>
<p>que se suele representar como una simple integral sobre un cierto
conjunto <span class="math inline">\(\int_S{f(x)dx}\)</span>. En la
literatura se usa una versión modificada de muestreo por
importancia:</p>
<p><span id="eq:mc_integral_tl" class="eqnos"><span
class="math display">\[
\tilde{I}_N = \frac{1}{N} \sum_{i=1}^N{\frac{f(X_i)}{p_X(X_i)}}
\]</span><span class="eqnos-number">(22)</span></span></p>
<p>para que, utilizando muestras <span class="math inline">\(X_i \sim
p_X\)</span>, <span class="math inline">\(E\left[ \frac{f}{p_X} \right]
= \int_S{\frac{f}{p_X}p_X}\)</span> y así se evalúe directamente la
integral de <span class="math inline">\(f\)</span>. En cualquiera de los
casos, el fundamento teórico es el mismo <span class="citation"
data-cites="berkeley-mc-lecture">(<a href="#ref-berkeley-mc-lecture"
role="doc-biblioref">Eric C. Anderson 1999</a>)</span>.</p>
<p>Esta forma de escribir el estimador nos permite amenizar algunos
casos particulares. Por ejemplo, si usamos muestras <span
class="math inline">\(X_i\)</span> que sigan una distribución uniforme
en <span class="math inline">\([a, b]\)</span>, entonces, su función de
densidad es <span class="math inline">\(p_X(x) = \frac{1}{b -
a}\)</span>. Esto da lugar a</p>
<p><span class="math display">\[
\tilde{I}_N = \frac{b - a}{N} \sum_{i = 1}^{N}{g(X_i)}
\]</span></p>
<p><strong>En lo que resta de capítulo, se utilizará indistintamente
<span class="math inline">\(\frac{1}{N}
\sum_{i=1}^N{\frac{f(X_i)p_X(X_i)}{q_X(X_i)}}\)</span> o <span
class="math inline">\(\frac{1}{N}
\sum_{i=1}^N{\frac{f(X_i)}{p_X(X_i)}}\)</span> según convenga</strong>.
¡Tenlo en cuenta!</p>
<p>Usando esta expresión, la distribución de importancia <span
class="math inline">\(p_X\)</span> que hace decrecer la varianza es
aquella proporcional a <span class="math inline">\(f\)</span>. Es decir,
supongamos que <span class="math inline">\(f \propto p_X\)</span>. Esto
es, existe un <span class="math inline">\(s\)</span> tal que <span
class="math inline">\(f(x) = s p_X(x)\)</span>. Como <span
class="math inline">\(p_X\)</span> debe integrar uno, podemos calcular
el valor de <span class="math inline">\(s\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
  \int_{S}{p_X(x)dx} &amp; = \int_{S}{sf(x)dx} = 1 \quad \iff \\
  s &amp; = \frac{1}{\int_{S}{f(x)dx}}
\end{aligned}
\]</span></p>
<p>Y entonces, se tendría que</p>
<p><span class="math display">\[
\begin{aligned}
  Var\left[ \frac{f(X)}{p_X(X)}\right] &amp; =
Var\left[\frac{f(X)}{sf(X)} \right] = \\
  &amp; = Var\left[ \frac{1}{s} \right] = \\
  &amp; = 0
\end{aligned}
\]</span></p>
<p>En la práctica, esto es inviable. El problema que queremos resolver
es calcular la integral de <span class="math inline">\(f\)</span>. Y
para sacar <span class="math inline">\(s\)</span>, necesitaríamos el
valor de la integral de <span class="math inline">\(f\)</span>. ¡Estamos
dando vueltas!</p>
<p>Por fortuna, hay algoritmos que son capaces de proporcionar la
constante <span class="math inline">\(s\)</span> sin necesidad de
calcular la integral. Uno de los más conocidos es
<strong>Metropolis-Hastings</strong>, el cual se basa en cadenas de
Markov de Monte Carlo.</p>
<p>En este trabajo nos centraremos en buscar funciones de densidad <span
class="math inline">\(p_X\)</span> que se aproximen a <span
class="math inline">\(f\)</span> lo más fielmente posible, dentro del
contexto del transporte de luz.</p>
<p>Pongamos un ejemplo de estimador de Monte Carlo para una caja de
dimensiones <span class="math inline">\(\small{[x_0, x_1] \times [y_0,
y_1] \times [z_0, z_1]}\)</span>. Si queremos estimar la integral de la
función <span class="math inline">\(f: \mathbb{R}^3 \rightarrow
\mathbb{R}\)</span></p>
<p><span class="math display">\[
\int_{x_0}^{x_1} \int_{y_0}^{y_1} \int_{z_0}^{z_1}{f(x, y, z)dx dy dz}
\]</span></p>
<p>mediante una variable aleatoria <span class="math inline">\(X \sim
U(\small{[x_0, x_1] \times [y_0, y_1] \times [z_0, z_1]})\)</span> con
función de densidad <span class="math inline">\(p(x, y, z) =
\frac{1}{x_1 - x_0} \frac{1}{y_1 - y_0} \frac{1}{z_1 - z_0}\)</span>,
tomamos el estimador</p>
<p><span class="math display">\[
\tilde{I}_N = \frac{1}{(x_1 - x_0) \cdot (y_1 - y_0) \cdot (z_1 - z_0)}
\sum_{i = 1}^{N}{f(X_i)}
\]</span></p>
<h3 data-number="4.3.2" id="muestreo-por-importancia-múltiple"><span
class="header-section-number">4.3.2</span> Muestreo por importancia
múltiple</h3>
<p>Las técnicas de <a href="#muestreo-por-importancia">muestreo por
importancia</a> nos proporcionan estimadores para una integral de la
forma <span class="math inline">\(\int{f(x)dx}\)</span>. Sin embargo, es
frecuente encontrarse un producto de dos funciones, <span
class="math inline">\(\int{f(x)g(x)dx}\)</span>. Si tuviéramos una forma
de coger muestras para <span class="math inline">\(f\)</span>, y otra
para <span class="math inline">\(g\)</span>, ¿cuál deberíamos usar?</p>
<p>Se puede utilizar un nuevo estimador de Monte Carlo, que viene dado
por <span class="citation" data-cites="PBRT3e">(<a href="#ref-PBRT3e"
role="doc-biblioref">Pharr, Jakob, and Humphreys 2016</a>)</span></p>
<p><span class="math display">\[
\frac{1}{N_f} \sum_{i = 1}^{N_f}{\frac{f(X_i)g(X_i)w_f(X_i)}{p_f(X_i)}}
+ \frac{1}{N_g} \sum_{j =
1}^{N_g}{\frac{f(Y_j)g(Y_j)w_g(Y_j)}{p_g(Y_j)}}
\]</span></p>
<p>donde <span class="math inline">\(N_f\)</span> y <span
class="math inline">\(N_g\)</span> son el número de muestras tomadas
para <span class="math inline">\(f\)</span> y <span
class="math inline">\(g\)</span> respectivamente, <span
class="math inline">\(p_f, p_g\)</span> las funciones de densidad
respectivas y <span class="math inline">\(w_f, w_g\)</span> funciones de
peso escogidas tales que la esperanza del estimador sea <span
class="math inline">\(\int{f(x)g(x)dx}\)</span>.</p>
<p>Estas funciones peso suelen tener en cuenta todas las formas
diferentes que hay de generar muestras para <span
class="math inline">\(X_i\)</span> e <span
class="math inline">\(Y_j\)</span>. Por ejemplo, una de las que podemos
usar es la heurística de balanceo:</p>
<p><span class="math display">\[
w_s(x) = \frac{N_s p_s(x)}{\sum_{i}{N_i p_i(x)}}
\]</span></p>
<p>Una modificación de esta es la heurística potencial (<em>power
heuristic</em>):</p>
<p><span class="math display">\[
w_s(x, \beta) = \frac{\left(N_s p_s(x)\right)^\beta}{\sum_{i}{\left(N_i
p_i(x)\right)^\beta}}
\]</span></p>
<p>la cual reduce la varianza con respecto a la heurística de balanceo.
Un valor para <span class="math inline">\(\beta\)</span> habitual es
<span class="math inline">\(\beta = 2\)</span>.</p>
<h4 data-number="4.3.2.1"
id="muestreo-por-importancia-múltiple-en-transporte-de-luz"><span
class="header-section-number">4.3.2.1</span> Muestreo por importancia
múltiple en transporte de luz</h4>
<p>Si queremos evaluar la contribución de luz en un punto teniendo en
cuenta la luz directa, la expresión utilizada es</p>
<p><span class="math display">\[
L_o(p, \omega_o) = \int_{S^2}{f(p, \omega_o \leftarrow \omega_i)
L_{directa}(p, \omega_i) \cos\theta_i\ d\omega_i}
\]</span></p>
<p>Si utilizáramos muestreo por importancia basándonos en las
distribuciones de <span class="math inline">\(L_{directa}\)</span> o
<span class="math inline">\(f\)</span> por separado, algunas de las dos
no rendiría especialmente bien. Combinando ambas mediante muestreo por
importancia múltiple se conseguiría un mejor resultado.</p>
<div id="fig:multiple_importance_sampling" class="fignos">
<figure>
<img loading="lazy" src="./img/03/Multiple%20importance%20sampling.png"
style="width:67.0%"
alt="Figura 12: Muestreo por importancia múltiple en transporte de luz ilustrado. Fuente: (Eric Veach December 1997, Multiple Importance Sampling)" />
<figcaption aria-hidden="true"><span>Figura 12:</span> Muestreo por
importancia múltiple en transporte de luz ilustrado. Fuente: <span
class="citation" data-cites="robust-monte-carlo">(<a
href="#ref-robust-monte-carlo" role="doc-biblioref">Eric Veach December
1997</a>, Multiple Importance Sampling)</span></figcaption>
</figure>
</div>
<h3 data-number="4.3.3"
id="otras-técnicas-de-reducción-de-varianza-en-transporte-de-luz"><span
class="header-section-number">4.3.3</span> Otras técnicas de reducción
de varianza en transporte de luz</h3>
<p>Hasta ahora, la principal técnica estudiada ha sido muestreo por
importancia (sea o no múltiple). Esto no quiere decir que sea la única.
Al contrario; esas dos son de las más sencillas que se pueden usar.</p>
<p>En esta sección vamos a ver de forma breve otras formas de reducir la
varianza de un estimador, centrádonos específicamente en el contexto de
transporte de luz.</p>
<h4 data-number="4.3.3.1" id="ruleta-rusa"><span
class="header-section-number">4.3.3.1</span> Ruleta rusa</h4>
<p>Un problema habitual en la práctica es saber cuándo terminar la
propagación de un rayo. Una solución simple es utilizar un parámetro de
profundidad –lo cual hemos implementado en el motor–. Otra opción más
eficiente es utilizar el método de <strong>ruleta rusa</strong>.</p>
<p>En esencia, la idea es que se genere un número aleatorio <span
class="math inline">\(\xi \in [0, 1)\)</span>. Si <span
class="math inline">\(\xi &lt; p_i\)</span>, el camino del rayo se
continúa, pero multiplicando la radiancia acumulada por <span
class="math inline">\(L_i(p, \omega_o \leftarrow \omega_i)/p_i\)</span>.
En otro caso (i.e., si <span class="math inline">\(\xi \ge
p_i\)</span>), el rayo se descarta. Esto hace que se acepten caminos más
fuertes, rechazando aquellas rutas con excesivo ruido.</p>
<p>Más información puede encontrarse en <span class="citation"
data-cites="PBRT3e">(<a href="#ref-PBRT3e" role="doc-biblioref">Pharr,
Jakob, and Humphreys 2016</a>, Russian Roulette and
Splitting)</span>.</p>
<h4 data-number="4.3.3.2"
id="next-event-estimation-o-muestreo-directo-de-fuentes-de-luz"><span
class="header-section-number">4.3.3.2</span> Next event estimation, o
muestreo directo de fuentes de luz</h4>
<blockquote>
<p><strong>Idea</strong>: Tracing shadow rays to the light source on
each bounce to see if you can terminate the current path. This involves
shooting a shadow ray towards light sources, if it’s occluded, terminate
the ray.</p>
</blockquote>
<p>Esta técnica recibe dos nombres. Tradicionalmente, se la conocía como
muestreo directo de fuentes de luz, pero en los últimos años ha adoptado
el nombre de next event estimation. Esencialmente, se trata de utilizar
las luces de la escena para calcular la radiancia de un punto.</p>
<p>Podemos dividir la rendering equation [<a
href="#eq:rendering_equation">11</a>] en dos sumandos <span
class="citation" data-cites="carlos-path-tracing">(<a
href="#ref-carlos-path-tracing" role="doc-biblioref">Carlos Ureña
2021</a>)</span>:</p>
<p><span class="math display">\[
L(p, \omega_o) = L_e(p, \omega_o) + \underbrace{L_{directa}(p, \omega_o
\leftarrow \omega_i) + L_{indirecta}(p, \omega_o \leftarrow
\omega_i)}_{\text{La parte integral de la rendering equation}}
\]</span></p>
<p>siendo <span class="math inline">\(L_e\)</span> la radiancia emitida
por la superficie, <span class="math inline">\(L_{directa}\)</span> la
radiancia proporcionada por las fuentes de luz y <span
class="math inline">\(L_{indirecta}\)</span> la radiancia indirecta.</p>
<p><span class="math display">\[
\begin{aligned}
  L_{directa}   &amp; = \int_{S^2}{f(p, \omega_o \leftarrow \omega_i)
L_{e}(y, -\omega_i) \cos\theta_i\ d\omega_i} \\
  L_{indirecta} &amp; = \int_{S^2}{f(p, \omega_o \leftarrow \omega_i)
L_{i}(y \omega_o \leftarrow \omega_i) \cos\theta_i\ d\omega_i}
\end{aligned}
\]</span></p>
<p>siendo <span class="math inline">\(y\)</span> el primer punto visible
desde <span class="math inline">\(p\)</span> en la dirección <span
class="math inline">\(\omega_i\)</span> situado en una fuente de
luz.</p>
<p>En cada punto de intersección <span class="math inline">\(p\)</span>,
escogeremos aleatoriamente un punto <span
class="math inline">\(y\)</span> en la fuente de luz, y calcularemos
<span class="math inline">\(L_{directa}\)</span>. Esta integral es fácil
de conseguir con las técnicas que ya conocemos. Sin embargo, <span
class="math inline">\(L_{indirecta}\)</span> cuesta más trabajo. Al
aparecer la radiancia incidente en el punto <span
class="math inline">\(p, L_i(p, \omega_o \leftarrow \omega_i)\)</span>,
necesitaremos evaluarla de forma recursiva trazando rayos en la
escena.</p>
<p>Aunque estamos haciendo más cálculos en cada punto de la cadena de
ray trace, al evaluar por separado <span
class="math inline">\(L_{directa}\)</span> y <span
class="math inline">\(L_{indirecta}\)</span> conseguimos reducir
considerablemente la varianza. Por tanto, suponiendo fija la varianza,
el coste computacional de un camino es mayor, pero el coste total es más
bajo.</p>
<p>Esta técnica requiere conocer si desde el punto <span
class="math inline">\(p\)</span> se puede ver <span
class="math inline">\(y\)</span> en la fuente de luz. Es decir, ¿hay
algún objeto en medio de <span class="math inline">\(p\)</span> e <span
class="math inline">\(y\)</span>? Para ello, se suele utilizar lo que se
conocen como <strong><em>shadow rays</em></strong>. Dispara uno de estos
rayos para conocer si está ocluido.</p>
<div id="fig:next_event_estimation" class="fignos">
<figure>
<img loading="lazy" src="./img/03/Next%20event%20estimation.png" style="width:67.0%"
alt="Figura 13: El muestreo directo de fuentes de luz cambia la forma de calcular la radiancia en un punto, pero mejora considerablemente el ruido de una imagen. Fuente: (Carlos Ureña 2021)" />
<figcaption aria-hidden="true"><span>Figura 13:</span> El muestreo
directo de fuentes de luz cambia la forma de calcular la radiancia en un
punto, pero mejora considerablemente el ruido de una imagen. Fuente:
<span class="citation" data-cites="carlos-path-tracing">(<a
href="#ref-carlos-path-tracing" role="doc-biblioref">Carlos Ureña
2021</a>)</span></figcaption>
</figure>
</div>
<p>Si quieres informarte más sobre esta técnica, puedes leer <span
class="citation" data-cites="Marrs2021">(<a href="#ref-Marrs2021"
role="doc-biblioref">Adam Marrs and Wald 2021</a>, Importance Sampling
of Many Lights on the GPU)</span>.</p>
<h4 data-number="4.3.3.3" id="quasi-monte-carlo"><span
class="header-section-number">4.3.3.3</span> Quasi-Monte Carlo</h4>
<p>Generalmente, en los estimadores de Monte Carlo se utilizan variables
aleatorias distribuidas uniformemente a las que se le aplican
transformaciones, pues resulta más sencillo generar un número aleatorio
de la primera manera que de la segunda. La idea de los quasi-Monte Carlo
es muestrear puntos que, de la manera posible, se extiendan
uniformemente en <span class="math inline">\([0, 1]^d\)</span>; evitando
así clústeres y zonas vacías <span class="citation"
data-cites="mcbook">(<a href="#ref-mcbook" role="doc-biblioref">Owen
2013</a>, Quasi-Monte Carlo)</span>.</p>
<p>Existen varias formas de conseguir esto. Algunas de las más famosas
son las secuencias de Sobol, que son computacionalmente caras pero
presentan menores discrepancias; o las series de Halton, que son más
fáciles de conseguir.</p>
<p>Se puede estudiar el tema en profundidad en <span class="citation"
data-cites="quasi-monte-carlo">(<a href="#ref-quasi-monte-carlo"
role="doc-biblioref">Martin Roberts 2018</a>)</span></p>
<div id="fig:quasimontecarlo" class="fignos">
<figure>
<img loading="lazy" src="./img/03/Quasi-Monte%20Carlo.png"
alt="Figura 14: Comparativa entre diferentes métodos de quasi-aleatoriedad. Fuente: (Martin Roberts 2018)" />
<figcaption aria-hidden="true"><span>Figura 14:</span> Comparativa entre
diferentes métodos de quasi-aleatoriedad. Fuente: <span class="citation"
data-cites="quasi-monte-carlo">(<a href="#ref-quasi-monte-carlo"
role="doc-biblioref">Martin Roberts 2018</a>)</span></figcaption>
</figure>
</div>
<h2 data-number="4.4" id="escogiendo-puntos-aleatorios"><span
class="header-section-number">4.4</span> Escogiendo puntos
aleatorios</h2>
<p>Una de las partes clave del estimador de Monte Carlo [<a
href="#eq:mc_integral">16</a>] es saber escoger la función de densidad
<span class="math inline">\(p_X\)</span> correctamente. En esta sección,
veremos algunos métodos para conseguir distribuciones específicas
partiendo de funciones de densidad sencillas, así como formas de elegir
funciones de densidad próximas a <span
class="math inline">\(f\)</span>.</p>
<h3 data-number="4.4.1" id="método-de-la-transformada-inversa"><span
class="header-section-number">4.4.1</span> Método de la transformada
inversa</h3>
<blockquote>
<p><strong>En resumen</strong>: Para conseguir una muestra de una
distribución específica <span class="math inline">\(F_X\)</span>:</p>
<ol type="1">
<li>Generar un número aleatorio <span class="math inline">\(\xi \sim
U(0, 1)\)</span>.</li>
<li>Hallar la inversa de la función de distribución deseada <span
class="math inline">\(F_X\)</span>, denotada <span
class="math inline">\(F_X^{-1}(x)\)</span>.</li>
<li>Calcular <span class="math inline">\(F_X^{-1}(\xi) =
X\)</span>.</li>
</ol>
</blockquote>
<p>Este método nos permite conseguir muestras de cualquier distribución
continua a partir de variables aleatorias uniformes, siempre que se
conozca la inversa de la función de distribución.</p>
<p>Sea <span class="math inline">\(X\)</span> una variable aleatoria con
función de distribución <span class="math inline">\(F_X\)</span><a
href="#fn3" class="footnote-ref" id="fnref3"
role="doc-noteref"><sup>3</sup></a>. Queremos buscar una transformación
<span class="math inline">\(T: [0, 1] \rightarrow \mathbb{R}\)</span>
tal que <span class="math inline">\(T(\xi) \stackrel{\text{\small d}}{=}
X\)</span>, siendo <span class="math inline">\(\xi\)</span> una v.a.
uniformemente distribuida. Para que esto se cumpla, se debe dar</p>
<p><span class="math display">\[
\begin{aligned}
F_X(x) &amp; = P[X &lt; x] = \\
       &amp; = P[T(\xi) &lt; x] = \\
       &amp; = P(\xi &lt; T^{-1}(x)) = \\
       &amp; = T^{-1}(x)
\end{aligned}
\]</span></p>
<p>Este último paso se debe a que, como <span
class="math inline">\(\xi\)</span> es uniforme en <span
class="math inline">\((0, 1)\)</span>, <span class="math inline">\(P[\xi
&lt; x] = x\)</span>. Es decir, hemos obtenido que <span
class="math inline">\(F_X\)</span> es la inversa de <span
class="math inline">\(T\)</span>.</p>
<blockquote>
<p>TODO: dibujo similar a <a
href="https://cs184.eecs.berkeley.edu/public/sp22/lectures/lec-12-monte-carlo-integration/lec-12-monte-carlo-integration.pdf">este:
p.52</a></p>
</blockquote>
<p>Como ejemplo, vamos a muestrear la función <span
class="math inline">\(f(x) = x^2,\ x \in [0, 2]\)</span> <span
class="citation" data-cites="berkeley-cs184">(<a
href="#ref-berkeley-cs184" role="doc-biblioref">Berkeley cs184 2022</a>,
Monte Carlo Integration)</span>.</p>
<p>Primero, normalizamos esta función para obtener una función de
densidad <span class="math inline">\(p_X(x)\)</span>. Es decir, buscamos
<span class="math inline">\(p_X(x) = c f(x)\)</span> tal que</p>
<p><span class="math display">\[
\begin{aligned}
1 &amp; = \int_{0}^{2}{p_X(x)dx} = \int_{0}^{2}{c f(x)dx} = c
\int_{0}^{2}{f(x)dx} = \\
  &amp; = \left.\frac{cx^3}{3}\right\rvert_{2}^{3} = \frac{8c}{3} \\
  &amp; \Rightarrow c = \frac{3}{8} \\
  &amp; \Rightarrow p_X(x) = \frac{3x^2}{8}
\end{aligned}
\]</span></p>
<p>A continuación, integramos la función de densidad para obtener la de
distribución <span class="math inline">\(F_X\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
F_X(x) = \int_{0}^{x}{p_X(x)dx} = \int_{0}^{x}{\frac{3x^2}{8}} =
\frac{x^3}{8}
\end{aligned}
\]</span></p>
<p>Solo nos queda conseguir la muestra. Para ello,</p>
<p><span class="math display">\[
\begin{aligned}
\xi &amp; = F_X(x)  = \frac{x^3}{8} \quad \iff \\
x &amp; = \sqrt[3]{8 \xi}
\end{aligned}
\]</span></p>
<p>Sacando un número aleatorio <span class="math inline">\(\xi\)</span>,
y pasándolo por la función obtenida, conseguimos un elemento con
distribución <span class="math inline">\(F(x)\)</span>.</p>
<h3 data-number="4.4.2" id="método-del-rechazo"><span
class="header-section-number">4.4.2</span> Método del rechazo</h3>
<blockquote>
<p><strong>En resumen</strong>: Para conseguir una muestra de una
variable aleatoria <span class="math inline">\(X\)</span> con función de
densidad <span class="math inline">\(p_X\)</span>:</p>
<ol type="1">
<li>Obtener una muestra <span class="math inline">\(y\)</span> de <span
class="math inline">\(Y\)</span> , y otra <span
class="math inline">\(\xi\)</span> de <span class="math inline">\(U(0,
1)\)</span>.</li>
<li>Comprobar si <span class="math inline">\(\xi &lt;
\frac{p_X(y)}{Mp_Y(y)}\)</span>. Si es así, aceptarla. Si no, sacar otra
muestra.</li>
</ol>
</blockquote>
<p>El método anterior presenta principalmente dos problemas:</p>
<ol type="1">
<li>No siempre es posible integrar una función para hallar su función de
densidad.</li>
<li>La inversa de la función de distribución, <span
class="math inline">\(F_X^{-1}\)</span> no tiene por qué existir.</li>
</ol>
<p>Como alternativa, podemos usar este método (en inglés, <em>rejection
method</em>). Para ello, necesitamos una variable aleatoria <span
class="math inline">\(Y\)</span> con función de densidad <span
class="math inline">\(p_Y(y)\)</span>. El objetivo es conseguir una
muestra de <span class="math inline">\(X\)</span> con función de
densidad <span class="math inline">\(p_X(x)\)</span>.</p>
<p>La idea principal es aceptar una muestra de <span
class="math inline">\(Y\)</span> con probabilidad <span
class="math inline">\(p_X/Mp_Y\)</span>, con <span
class="math inline">\(1 &lt; M &lt; \infty\)</span>. En esencia, estamos
jugando a los dardos: si la muestra de <span
class="math inline">\(y\)</span> que hemos obtenido se queda por debajo
de la gráfica de la función <span class="math inline">\(Mp_Y &lt;
p_X\)</span>, estaremos obteniendo una de <span
class="math inline">\(p_X\)</span>.</p>
<blockquote>
<p>TODO dibujo de la gráfica <span
class="math inline">\(\frac{p_X(y)}{Mp_Y(y)}\)</span>.</p>
<p>¿Quizás haga falta una demostración también? No estoy satisfecho con
este apartado ahora mismo. Necesita trabajo.</p>
</blockquote>
<p>El algoritmo consiste en:</p>
<ol type="1">
<li>Obtener una muestra de <span class="math inline">\(Y\)</span>,
denotada <span class="math inline">\(y\)</span>, y otra de <span
class="math inline">\(U(0, 1)\)</span>, llamada <span
class="math inline">\(\xi\)</span>.</li>
<li>Comprobar si <span class="math inline">\(\xi &lt;
\frac{p_X(y)}{Mp_Y(y)}\)</span>.
<ol type="1">
<li>Si se cumple, se acepta <span class="math inline">\(y\)</span> como
muestra de <span class="math inline">\(p_X\)</span></li>
<li>En caso contrario, se rechaza <span class="math inline">\(y\)</span>
y se vuelve al paso 1.</li>
</ol></li>
</ol>
<h1 data-number="5" id="construyamos-un-path-tracer"><span
class="header-section-number">5</span> ¡Construyamos un path
tracer!</h1>
<p>Ahora que hemos introducido toda la teoría necesaria, es hora de
ponernos manos a la obra. En este capítulo escogeremos una serie de
herramientas y con ellas implementaremos un pequeño motor de path
tracing en tiempo real.</p>
<p>La implementación estará basada en Vulkan, junto al pequeño framework
de nvpro-samples. El motor mantendrá el mismo espíritu que la serie de
<span class="citation" data-cites="Shirley2020RTW1">(<a
href="#ref-Shirley2020RTW1" role="doc-biblioref">Shirley
2020a</a>)</span>, Ray Tracing In One Weekend.</p>
<p>El resultado final puede verse en el siguiente vídeo <span
class="citation" data-cites="video">(<a href="#ref-video"
role="doc-biblioref">Andrés Millán 2022c</a>)</span></p>
<iframe width="784" height="441" src="https://www.youtube.com/embed/pXrD3K69MqE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
<h2 data-number="5.1" id="el-algoritmo-de-path-tracing"><span
class="header-section-number">5.1</span> El algoritmo de path
tracing</h2>
<p>Hemos llegado a una de las partes más importantes de este trabajo. Es
el momento de poner en concordancia todo lo que hemos visto a lo largo
de los capítulos anteriores. Vamos a aplicar las <a
href="#métodos-de-monte-carlo">técnicas de Monte Carlo</a> a las
ecuaciones vistas en <a href="#transporte-de-luz">radiometría</a>,
teniendo en cuenta las propiedades de los diferentes materiales.</p>
<p>El código ilustrado en las siguientes secciones está basado en el de
<span class="citation" data-cites="nvpro-samples-tutorial">(<a
href="#ref-nvpro-samples-tutorial" role="doc-biblioref">Nvidia
2022a</a>)</span>, aunque se pueden encontrar numerosísimas
modificaciones en la literatura del sector.</p>
<h3 data-number="5.1.1"
id="estimando-la-rendering-equation-con-monte-carlo"><span
class="header-section-number">5.1.1</span> Estimando la rendering
equation con Monte Carlo</h3>
<p>Lo que buscamos en esta sección es aproximar el valor de la radiancia
en un cierto punto, que dependerá de cada píxel dela pantalla.
¿Recuerdas la ecuación de dispersión [<a
href="#eq:scattering_equation">6</a>]?</p>
<p><span class="math display">\[
L_o(p, \omega_o \leftarrow \omega_i) = \int_{\mathbb{S}^2}{f(p, \omega_o
\leftarrow \omega_i)L_i(p, \omega_i)\cos\theta_i} d\omega_i
\]</span></p>
<p>Recordemos que <span class="math inline">\(L_o(p, \omega_o \leftarrow
\omega_i)\)</span> es la radiancia emitida en un punto <span
class="math inline">\(p\)</span> hacia la dirección <span
class="math inline">\(\omega_o\)</span> desde <span
class="math inline">\(\omega_i\)</span>, <span
class="math inline">\(f(p, \omega_o \leftarrow \omega_i)\)</span> es la
función de distribución de dispersión bidireccional (i.e., cómo refleja
la luz el punto) y <span class="math inline">\(\cos\theta_i\)</span> el
ángulo que forman el ángulo sólido de entrada <span
class="math inline">\(\omega_i\)</span> y la normal en el punto <span
class="math inline">\(p\)</span>, <span
class="math inline">\(\mathbf{n}\)</span>: <span
class="math inline">\(\cos\theta_i = \omega_i \cdot
\mathbf{n}\)</span>.</p>
<p>Añadamos el término de radiancia emitida <span
class="math inline">\(L_e(p, \omega_o)\)</span>, la cantidad de
randiancia emitida por el material del punto <span
class="math inline">\(p\)</span>:</p>
<p><span class="math display">\[
L_o(p, \omega_o \leftarrow \omega_i) = L_e(p, \omega_o) +
\int_{\mathbb{S}^2}{f(p, \omega_o \leftarrow \omega_i)L_i(p,
\omega_i)\cos\theta_i} d\omega_i
\]</span></p>
<p>Podemos aproximar el valor de la integral utilizando el estimador de
Monte Carlo comúnmente considerado en la <a
href="#muestreo-por-importancia-múltiple-en-transporte-de-luz">industria</a>,
[<a href="#eq:mc_integral_importancia">21</a>]:</p>
<p><span id="eq:rendering_equation_mc" class="eqnos"><span
class="math display">\[
\begin{aligned}
L_o(p, \omega_o \leftarrow \omega_i) &amp; = \int_{\mathbb{S}^2}{f(p,
\omega_o \leftarrow \omega_i)L_i(p, \omega_i)\cos\theta_i} d\omega_i \\
                 &amp; \approx \frac{1}{N} \sum_{j = 1}^{N}{\frac{f(p,
\omega_o \leftarrow \omega_j) L_i(p, \omega_j)
\cos\theta_j}{p(\omega_j)}}
\end{aligned}
\]</span><span class="eqnos-number">(23)</span></span></p>
<p>Con <span class="math inline">\(N \in \mathbb{Z}^+\)</span>. Con
<span class="math inline">\(N\)</span> suficientemente grande, se
conseguiría un valor de radiancia relativamente acertado. Sin embargo,
en algunos casos, podemos simplificar más el sumando.</p>
<p>Fijémonos en el denominador. Lo que estamos haciendo es tomar una
muestra de un vector en la esfera. Si trabajamos con una BRDF en vez de
una BSDF, usaríamos un hemisferio en vez de la esfera.</p>
<p>En el caso de la componente difusa, sabemos que la BRDF es <span
class="math inline">\(f_r(p, \omega_o \leftarrow \omega_i) =
\frac{\rho}{\pi}\)</span> aplicando reflectancia lambertiana, así
que</p>
<p><span class="math display">\[
\frac{1}{N} \sum_{j = 1}^{N}{\frac{(\rho / \pi) L_i(p, \omega_j)
\cos\theta_j}{p(\omega_j)}}
\]</span></p>
<p>En la sección <a href="#muestreo-por-importancia">muestreo por
importancia</a>, introducimos la idea de buscar una función proporcional
a <span class="math inline">\(f\)</span> para con el fin de reducir el
error. Podemos usar <span class="math inline">\(p(\omega) =
\frac{\cos\theta}{\pi}\)</span>, de forma que</p>
<p><span id="eq:rendering_eq_lambertian" class="eqnos"><span
class="math display">\[
\frac{1}{N} \sum_{j = 1}^{N}{\frac{(\rho / \pi) L_i(p, \omega_j)
\cos\theta_j}{(\cos\theta_j / \pi)}} = \frac{1}{N} \sum_{j =
1}^{N}{L_i(p, \omega_j) \rho}
\]</span><span class="eqnos-number">(24)</span></span></p>
<p>Lo cual nos proporciona una expresión muy agradable para los
materiales difusos.</p>
<p>Por lo general, no será necesario simplificar hasta tal punto la
expresión.</p>
<h3 data-number="5.1.2" id="pseudocódigo-de-un-path-tracer"><span
class="header-section-number">5.1.2</span> Pseudocódigo de un path
tracer</h3>
<p>Con lo que conocemos hasta ahora, podemos empezar a programar los
shaders. Una primera implementación inspirada en la rendering equation
[<a href="#eq:rendering_equation_mc">23</a>] sería similar a lo
siguiente:</p>
<pre><code class="language-cpp">pathtrace(Rayo r, profundidad) {
    if (profundidad == profundidad_maxima) {
        return contribucion_ambiental;
    }

    r.closest_hit()     // -&gt; Guardar información del impacto

    if (!r.ha_impactado()) {
        // Si no se golpea nada, añadir una pequeña contribución del entorno.
        return contribucion_ambiental
    }

    // Sacar información del punto de impacto
    hit_info = r.hit_info
    material = hit_info.material
    emision  = material.emision

    // Calcular los parámetros de la ecuación
    cos_theta = dot(r.direccion, hit_info.normal)
    BRDF, pdf = extraer_info(material)

    nuevo_rayo = Rayo(
        origen    = hit_info.punto_impacto,
        direccion = siguiente_direccion(hit_info.normal)
    )

    // Devolver la radiancia del punto de impacto.
    // L_i se calcula a partir del pathtrace del nuevo rayo.
    return emision
        + (BRDF * pathtrace(nuevo_rayo, profundidad + 1) * cos_theta) / pdf;
}</code></pre>
<p>El término <code>emision</code> corresponde a <span
class="math inline">\(L_e(p, \omega_o)\)</span>. Siempre lo añadimos,
pues en caso de que el objeto no emita luz, la contribución de este
término sería 0.</p>
<p>La principal desventaja de esta implementación es que utiliza
recursividad. Como bien es conocido, abusar de recursividad provoca que
el tiempo de ejecución aumente significativamente. Además, con la
implementación anterior, se generan rayos desde el closest hit shader,
lo cual no es ideal.</p>
<h3 data-number="5.1.3" id="evitando-la-recursividad"><span
class="header-section-number">5.1.3</span> Evitando la recursividad</h3>
<p>Podemos evitar los problemas de la implementación anterior con una
pequeña modificación. En vez de calcular la radiancia desde el closest
hit, nos traemos la información necesaria al raygen shader, y calculamos
la radiancia total desde allí.</p>
<p>Para conseguirlo, debemos hacer que el <code>HitPayload</code>
almacene dos nuevos parámetros: <code>weight</code> y
<code>hit_value</code>, así como el nuevo origen y la dirección del
rayo.</p>
<p>El pseudocódigo sería el siguiente: por una parte, una función se
encarga de generar los rayos:</p>
<pre><code class="language-cpp">pathtrace() {
    // Inicializar parámetros del primer rayo
    HitPayload prd {
        hit_value,
        weight,
        ray_origin,
        ray_direction
    }

    current_weight = vec3(1);
    hit_value      = vec3(0);

    for (profundidad in [0, profunidad_maxima]) {
        closest_hit(prd.ray_origin, prd.ray_dir);
        // prd actualiza sus parámetros

        hit_value = hit_value + prd.hit_value * current_weight;
        current_weight = current_weight * prd.weight;
    }

    return hit_value;
}</code></pre>
<p>Y por otro lado, otra función debe almacenar correctamente la
información del punto de impacto, así como la radiancia de ese punto.
Corresponde al closest hit:</p>
<pre><code class="language-cpp">closest_hit() {
    // Sacar información sobre el punto de impacto: material, normal...

    // Preparar información para el raygen
    prd.ray_origin = punto_impacto
    prd.ray_dir = siguiente_direccion(material)

    // Calcular la radiancia
    float cos_theta = dot(prd.ray_dir, normal);
    BRDF, pdf = extraer_info(material)

    prd.hit_value = material.emision

    prd.weight = (BRDF * cos_theta) / pdf

    return prd
}</code></pre>
<p>Esta versión no es tan intuitiva. ¿Por qué este último genera el
mismo resultado que el de <a href="#pseudocódigo-de-un-path-tracer">la
versión recursiva</a>?</p>
<p>Analicemos lo está ocurriendo.</p>
<p>Sea <span class="math inline">\(h\)</span> el <em>hit value</em> (que
simboliza la radiancia), <span class="math inline">\(w\)</span> el peso,
<span class="math inline">\(f_i\)</span> la BRDF (o en su defecto,
BTDF/BSDF), <span class="math inline">\(i\)</span>, <span
class="math inline">\(e_i\)</span> la emisión, <span
class="math inline">\(\cos\theta_i\)</span> el coseno del ángulo que
forman la nueva dirección del rayo y la normal, y <span
class="math inline">\(p_i\)</span> la función de densidad que, dada una
dirección, proporciona la probabilidad de que se escoja. El subíndice
denota el <span class="math inline">\(i\)</span>-ésimo punto de
impacto.</p>
<p>En esencia, este algoritmo está descomponiendo lo que recogemos en
<code>weight</code>, que es <span class="math inline">\(f_i \cos\theta_i
/ p_i\)</span>. Inicialmente, para el primer envío del rayo, <span
class="math inline">\(h = (0, 0, 0)\)</span>, <span
class="math inline">\(w = (1, 1, 1)\)</span>. Tras trazar el primer
rayo, se tiene que</p>
<p><span class="math display">\[
\begin{aligned}
    h &amp; = 0 + e_1 w = e_1 \\
    w &amp; = \frac{f_1 \cos\theta_1}{p_1}
\end{aligned}
\]</span></p>
<p>Tras el segundo rayo, obtenemos</p>
<p><span class="math display">\[
\begin{aligned}
    h &amp; = e_1 + e_2 w = \\
      &amp; = e_1 + e_2 \frac{f_1 \cos\theta_1}{p_1} \\
    w &amp; = \frac{f_1 \cos\theta_1}{p_1} \frac{f_2 \cos\theta_2}{p_2}
\end{aligned}
\]</span></p>
<p>Y para el tercero</p>
<p><span class="math display">\[
\begin{aligned}
    h &amp; = e_1 + e_2 \frac{f_1 \cos\theta_1}{p_1} + e_3 w = \\
      &amp; = e_1 + e_2 \frac{f_1 \cos\theta_1}{p_1} + e_3 \frac{f_1
\cos\theta_1}{p_1} \frac{f_2 \cos\theta_2}{p_2} = \\
      &amp; = e_1 + \frac{f_1
\cos\theta_1}{p_1}\textcolor{verde-oscurisimo}{\left(e_2 + e_3 \frac{f_2
\cos\theta_2}{p_2}\right)} \\
    w &amp; = \frac{f_1 \cos\theta_1}{p_1} \frac{f_2 \cos\theta_2}{p_2}
\frac{f_3 \cos\theta_3}{p_3}
\end{aligned}
\]</span></p>
<p>El <span
class="math inline">\(\textcolor{verde-oscurisimo}{\text{término que
acompaña}}\)</span> a <span class="math inline">\(\frac{f_1
\cos\theta_1}{p_1}\)</span> es la radiancia del tercer punto de impacto.
Por tanto, a la larga, se tendrá que <span
class="math inline">\(h\)</span> estima correctamente la radiancia de un
punto. Con esto, podemos afirmar que</p>
<p><span class="math display">\[
h \approx \frac{1}{N} \sum_{j = 1}^{N}{\frac{f(p, \omega_o \leftarrow
\omega_j) L_i(p, \omega_j) \cos\theta_j}{p(\omega_j)}}
\]</span></p>
<p>Este algoritmo supone una mejora de hasta 3 veces mayor rendimiento
que el recursivo <span class="citation"
data-cites="nvpro-samples-tutorial">(<a
href="#ref-nvpro-samples-tutorial" role="doc-biblioref">Nvidia
2022a</a>, glTF Scene)</span>.</p>
<h2 data-number="5.2"
id="requisitos-de-ray-tracing-en-tiempo-real"><span
class="header-section-number">5.2</span> Requisitos de ray tracing en
tiempo real</h2>
<p>Como es natural, el tiempo es una limitación enorme para cualquier
programa en tiempo real. Mientras que en un <em>offline renderer</em>
disponemos de un tiempo muy considerable por frame (desde varios
segundos hasta horas), en un programa en tiempo real necesitamos que un
frame salga en 16 milisegundos o menos. Este concepto se suele denominar
<em>frame budget</em>: la cantidad de tiempo que disponemos para un
frame.</p>
<blockquote>
<p><strong>Nota</strong>: cuando hablamos del tiempo disponible para un
frame, solemos utilizarmilisegundos (ms) o frames por segundo (FPS).
Para que un programa en tiempo real vaya suficientemente fluido,
necesitaremos que el motor corra a un mínimo de 30 FPS (que equivalen a
33 ms por frame). Hoy en día, debido al avance del área en campos como
los videosjuegos, el estándar se está convirtiendo en 60 FPS (16
ms/frame).</p>
</blockquote>
<p>Las nociones de los capítulos anteriores no distinguen entre un motor
en tiempo real y <em>offline</em>. Como es natural, necesitaremos
introducir unos pocos conceptos más para llevarlo a tiempo real. Además,
existen una serie de requisitos hardware que debemos cumplir para que un
motor en tiempo real con ray tracing funcione.</p>
<h3 data-number="5.2.1" id="arquitecturas-de-gráficas"><span
class="header-section-number">5.2.1</span> Arquitecturas de
gráficas</h3>
<p>El requisito más importante de todos es la gráfica. Para ser capaces
de realizar cálculos de ray tracing en tiempo real, necesitaremos una
arquitectura moderna con núcleos dedicados a este tipo de cáclulos <a
href="#fn4" class="footnote-ref" id="fnref4"
role="doc-noteref"><sup>4</sup></a>.</p>
<p>A día 17 de abril de 2022, para correr ray tracing en tiempo real, se
necesita alguna de las siguientes tarjetas gráficas:</p>
<table>
<colgroup>
<col style="width: 20%" />
<col style="width: 10%" />
<col style="width: 68%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Arquitectura</strong></th>
<th style="text-align: left;"><strong>Fabricante</strong></th>
<th style="text-align: center;"><strong>Modelos de
gráficas</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Turing</strong></td>
<td style="text-align: left;">Nvidia</td>
<td style="text-align: center;">RTX 2060, RTX 2060 Super, RTX 2070, RTX
2070 Super, RTX 2080, RTX 2080 Super, RTX 2080 Ti, RTX Titan</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Ampere</strong></td>
<td style="text-align: left;">Nvidia</td>
<td style="text-align: center;">RTX 3050, RTX 3060, RTX 3060 Ti, RTX
3070, RTX 3070 Ti, RTX 3080, RTX 3080 Ti, RTX 3090, RTX 3090 Ti</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>RDNA2</strong> (Navi 2X, Big
Navi)</td>
<td style="text-align: left;">AMD</td>
<td style="text-align: center;">RX 6400, RX 6500 XT, RX 6600, RX 6600
XT, RX 6700 XT, RX 6800, RX 6800 XT, RX 6900 XT</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Arc Alchemist</strong></td>
<td style="text-align: left;">Intel</td>
<td style="text-align: center;"><em>No reveleado aún</em></td>
</tr>
</tbody>
</table>
<p>Se puede encontrar más información sobre las diferentes arquitecturas
y gráficas en el siguiente artículo de AMD Radeon <span class="citation"
data-cites="wikipedia-radeon">(<a href="#ref-wikipedia-radeon"
role="doc-biblioref">Wikipedia 2022d</a>)</span>, Nvidia <span
class="citation" data-cites="wikipedia-nvidia">(<a
href="#ref-wikipedia-nvidia" role="doc-biblioref">Wikipedia
2022b</a>)</span>, e <span class="citation" data-cites="intel-arc">(<a
href="#ref-intel-arc" role="doc-biblioref">Intel 2022</a>)</span>. Solo
se han incluido las gráficas de escritorio de consumidor.</p>
<p>Para este trabajo se ha utilizado una <strong>RTX 2070
Super</strong>. En el capítulo de análisis del rendimiento se hablará
con mayor profundidad de este apartado.</p>
<h3 data-number="5.2.2"
id="frameworks-y-api-de-ray-tracing-en-tiempo-real"><span
class="header-section-number">5.2.2</span> Frameworks y API de ray
tracing en tiempo real</h3>
<p>Una vez hemos cumplido los requisitos de hardware, es hora de escoger
los frameworks de trabajo.</p>
<p>Las API de gráficos están empezando a adaptarse a los requisitos del
tiempo real, por lo que cambian frecuentemente. La mayoría adquirieron
las directivas necesarias muy recientemente. Aun así, son lo
suficientemente sólidas para que se pueda usar en aplicaciones
empresariales de gran embergadura.</p>
<p>Esta es una lista de las API disponibles con capacidades de Ray
Tracing disponibles para, al menos, la arquitectura Turing:</p>
<ul>
<li>Vulkan, junto a los <em>bindings</em> de ray tracing, denominados
KHR.</li>
<li>Microsoft DirectX Ray Tracing (DXR), una extensión de DirectX 12
<span class="citation" data-cites="directx-12">(<a
href="#ref-directx-12" role="doc-biblioref">Wikipedia
2022a</a>)</span>.</li>
<li>Nvidia OptiX <span class="citation" data-cites="optix">(<a
href="#ref-optix" role="doc-biblioref">Wikipedia 2022c</a>)</span>.</li>
</ul>
<p>De momento, no hay mucho donde elegir.</p>
<p>OptiX es la API más vieja de todas. Su primera versión salió en 2009,
mientras que la última estable es de 2021. Tradicionalmente se ha usado
para offline renderers, y no tiene un especial interés para este trabajo
estando las otras dos disponibles.</p>
<p>Tanto DXR como Vulkan son los candidatos más sólidos. DXR salió en
2018, con la llegada de Turing. Es un par de años más reciente que
Vulkan KHR. Cualquiera de las dos cumpliría su cometido de forma
exitosa. Sin embargo, para este trabajo, <strong>hemos escogido
Vulkan</strong> por los siguientes motivos:</p>
<ul>
<li>DirectX 12 está destinado principalmente a plataformas de Microsoft.
Es decir, está pensado para sistemas operativos Windows 10 o mayor <a
href="#fn5" class="footnote-ref" id="fnref5"
role="doc-noteref"><sup>5</sup></a>.</li>
<li>Vulkan, al estar apoyado principalmente por AMD y desarrollado por
Khonos, es un proyecto de código. Su principal aliciente es la capacidad
de correr en múltiples sistemas operativos, como Windows, distribuciones
de Linux o Android.</li>
</ul>
<p>Ambas API se comportan de manera muy similar, y no existe una gran
diferencia entre ellas; tanto en rendimiento como en complejidad de
desarrollo. Actualmente el proyecto solo compila en Windows 10 o mayor,
por lo que estos dos puntos no resultan especialmente relevantes para el
trabajo.</p>
<p>Si se desea, se puede encontrar una comparación más a fondo de las
API en el blog de <span class="citation" data-cites="alain-API">(<a
href="#ref-alain-API" role="doc-biblioref">Alain Galvan
2022</a>)</span>. Además, el manual de Vulkan con las extensiones de KHR
se puede encontrar en <span class="citation" data-cites="vulkan">(<a
href="#ref-vulkan" role="doc-biblioref">The Khronos Vulkan Working Group
2022</a>)</span>.</p>
<h2 data-number="5.3" id="setup-del-proyecto"><span
class="header-section-number">5.3</span> Setup del proyecto</h2>
<p>Un proyecto de Vulkan necesita una cantidad de código inicial
considerable. Para acelerar este trámite y partir de una base más
sólida, se ha decidido usar un pequeño framework de trabajo de Nvidia
llamado [nvpro-samples] <span class="citation"
data-cites="nvpro-samples">(<a href="#ref-nvpro-samples"
role="doc-biblioref">Nvidia 2022b</a>)</span>.</p>
<p>Esta serie de repositorios de Nvidia DesignWorks contienen proyectos
de ray tracing de Nvidia con fines didácticos. Nosotros usaremos
<strong>vk_raytracing_tutorial_KHR</strong> <span class="citation"
data-cites="nvpro-samples-tutorial">(<a
href="#ref-nvpro-samples-tutorial" role="doc-biblioref">Nvidia
2022a</a>)</span>, pues ejemplifica cómo añadir ray tracing en tiempo
real a un proyecto de Vulkan. Estos frameworks contienen asimismo otras
utilidades menores. Destacan <strong>GLFW</strong> (gestión de ventanas
en C++), <strong>imgui</strong> (interfaz de usuario) y
<strong>tinyobjloader</strong> (carga de <code>.obj</code> y
<code>.mtl</code>).</p>
<p>Nuestro repositorio utiliza las herramientas citadas anteriormente
para compilar su proyecto. El Makefile es una modificación del que se
usa para ejecutar los ejemplos de Nvidia. Por defecto, ejecuta una
aplicación muy simple que muestra un cubo mediante rasterización, la
cual modificaremos hasta añadir ray tracing en tiempo real. Por tanto,
la parte inicial del desarrollo consiste en adaptar Vulkan para usar la
extensión de ray tracing, extrayendo la información de la gráfica y
cargando correspondientemente el dispositivo.</p>
<div id="fig:raster" class="fignos">
<figure>
<img loading="lazy" src="./img/04/Raster.jpg" style="width:70.0%"
alt="Figura 15: Por defecto, el programa muestra un cubo rasterizado muy simple. Es, prácticamente, un hello world gráfico" />
<figcaption aria-hidden="true"><span>Figura 15:</span> Por defecto, el
programa muestra un cubo rasterizado muy simple. Es, prácticamente, un
<em>hello world</em> gráfico</figcaption>
</figure>
</div>
<h3 data-number="5.3.1" id="vistazo-general-a-la-estructura"><span
class="header-section-number">5.3.1</span> Vistazo general a la
estructura</h3>
<p>La estructura final del proyecto (es decir, la carpeta
<code>application</code>) es la siguiente:</p>
<ul>
<li>La carpeta <code>application/build</code> contiene todo lo
relacionado con CMake y el ejecutable final.</li>
<li>Las dependencias del proyecto se encuentran en el repositorio
<code>application/nvpro_core</code>. Se descargan automáticamente seguir
las instrucciones de compilación.</li>
<li>En <code>application/vulkan_ray_tracing/media/</code> se encuentran
todos los archivos <code>.obj</code>, <code>.mtl</code> y las
texturas.</li>
<li>La subcarpeta <code>application/vulkan_ray_tracing/src</code>
contiene el código fuente de la propia aplicación.
<ul>
<li>Toda la implementación relacionada con el motor (y por tanto,
Vulkan), se halla en <code>engine.h/cpp</code>. Una de las desventajas
de seguir un framework “de juguete” es que el acoplamiento es
considerablemente alto. Más adelante comentaremos los motivos.</li>
<li>Los parámetros de la aplicación (como tamaño de pantalla y otras
estructuras comunes) se encuetran en <code>globals.hpp</code>.</li>
<li>La carga de escenas y los objetos se gestionan en
<code>scene.hpp</code>.</li>
<li>En <code>main.cpp</code> se gestiona tanto el punto de entrada de la
aplicación como la actualización de la interfaz gráfica.</li>
<li>La carpeta <code>application/vulkan_ray_tracing/src/shaders</code>
contiene todos los shaders; tanto de rasterización, como de ray tracing.
<ul>
<li>Para ray tracing, se utilizan los <code>raytrace.*</code>,
<code>pathtrace.glsl</code> (que contiene el grueso del path
tracer).</li>
<li>En rasterización se usan principalmente
<code>frag_shader.frag</code>, <code>passthrough.vert</code>,
<code>post.frag</code>, <code>vert_shader.vert</code>.</li>
<li>El resto de shaders son archivos comunes a ambos o utilidades
varias, como pueden ser <code>sampling.glsl</code> (donde se implementan
distribuciones aleatorias) o <code>random.glsl</code> (que contiene
generadores de números aleatorios).</li>
</ul></li>
<li>Finalmente, la carpeta
<code>application/vulkan_ray_tracing/src/spv</code> contiene los shaders
compilados a SPIR-V.</li>
</ul></li>
</ul>
<h2 data-number="5.4" id="compilación-y-ejecución"><span
class="header-section-number">5.4</span> Compilación y ejecución</h2>
<p>Las dependencias necesarias son:</p>
<ol type="1">
<li><strong>CMake</strong>.</li>
<li>Un <strong>driver de Nvidia</strong> compatible con la extensión
<code>VK_KHR_ray_tracing_pipeline</code>.</li>
<li>El SDK de <strong>Vulkan</strong>, versión 1.2.161 o mayor.</li>
</ol>
<p>Ejecuta los siguientes comandos desde la terminal para compilar el
proyecto:</p>
<pre class="sh"><code>$ git clone --recursive --shallow-submodules https://github.com/Asmilex/Raytracing.git
$ cd .\Raytracing\application\vulkan_ray_tracing\
$ mkdir build
$ cd build
$ cmake ..
$ cmake --build .</code></pre>
<p>Si todo funciona correctamente, debería generarse un binario en
<code>./application/bin_x64/Debug</code> llamado
<code>asmiray.exe</code>. Desde la carpeta en la que deberías
encontrarte tras seguir las instrucciones, puedes conseguir ejecutarlo
con</p>
<pre class="sh"><code>$ ..\..\bin_x64\Debug\asmiray.exe</code></pre>
<h2 data-number="5.5" id="estructuras-de-aceleración"><span
class="header-section-number">5.5</span> Estructuras de aceleración</h2>
<p>El principal coste de ray tracing es el cálculo de las intersecciones
con objetos; hasta un 95% del tiempo de ejecución total <span
class="citation" data-cites="scratchapixel-2019">(<a
href="#ref-scratchapixel-2019" role="doc-biblioref">Scratchapixel
2019</a>)</span>. Reducir el número de test de intersección es
clave.</p>
<p>Las <strong>estructuras de aceleración</strong> son una forma de
representar la geometría de la escena. Aunque existen diferentes tipos,
en esencia, todos engloban a uno o varios objetos en una estructura con
la que resulta más eficiente hacer test de intersección. Son similares a
los grafos de escena de un rasterizador.</p>
<p>Uno de los tipos más comunes (y el que se usa en <span
class="citation" data-cites="Shirley2020RTW2">(<a
href="#ref-Shirley2020RTW2" role="doc-biblioref">Shirley
2020b</a>)</span>) es la <strong>Bounding Volume Hierarchy
(BVH)</strong>. Fue una técnica desarrollada por Kay y Kajilla en 1986.
Este método encierra un objeto en una caja (denomina una
<strong>bounding box</strong>), de forma que el test de intersección
principal se hace con la caja y no con la geometría. Si un rayo impacta
en la <em>bounding box</em>, entonces se pasa a testear la
geometría.</p>
<p>Se puede repetir esta idea repetidamente, de forma que agrupemos
varias <em>bounding boxes</em>. Así, creamos una jerarquía de objetos
–como si nodos de un árbol se trataran–. A esta jerarquía es a la que
llamamos BVH.</p>
<p>Es importante crear buenas divisiones de los objetos en la BVH.
Cuanto más compacta sea una BVH, más eficiente será el test de
intersección.</p>
<p>Una forma habitual de crear la BVH es mediante la división del
espacio en una rejilla. Esta técnica se llama <strong>Axis-Aligned
Bounding Box (AABB)</strong>. Usualmente se usa el método del
<em>slab</em> (también introducido por Kay y Kajilla). Se divide el
espacio en una caja n-dimensional alineada con los ejes, de forma que
podemos verla como <span class="math inline">\([x_0, x_1]
\times\)</span> <span class="math inline">\([y_0, y_1] \times\)</span>
<span class="math inline">\([z_0, z_1] \times \dots\)</span> De esta
forma, comprobar si un rayo impacta en una bounding box es tan sencillo
como comprobar que está dentro del intervalo. Este es el método que se
ha usado en Ray Tracing in One Weekend.</p>
<p>Vulkan gestiona las estructuras de aceleración diviéndolas en dos
partes: <strong>Top-Level Acceleration Structure</strong> (TLAS) y
<strong>Bottom-Level Acceleration Structure</strong> (BLAS).</p>
<div id="fig:TLAS" class="fignos">
<figure>
<img loading="lazy" src="./img/04/Acceleration%20structure.png" style="width:70.0%"
alt="Figura 16: La TLAS guarda información de las instancias de un objeto, así como una referencia a BLAS que contiene la geometría correspondiente. Fuente: (Nvidia 2022a)" />
<figcaption aria-hidden="true"><span>Figura 16:</span> La TLAS guarda
información de las instancias de un objeto, así como una referencia a
BLAS que contiene la geometría correspondiente. Fuente: <span
class="citation" data-cites="nvpro-samples-tutorial">(<a
href="#ref-nvpro-samples-tutorial" role="doc-biblioref">Nvidia
2022a</a>)</span></figcaption>
</figure>
</div>
<blockquote>
<p>TODO: Deberíamos cambiar esa foto por otra propia.</p>
</blockquote>
<h3 data-number="5.5.1"
id="botom-level-acceleration-structure-blas"><span
class="header-section-number">5.5.1</span> Botom-Level Acceleration
Structure (BLAS)</h3>
<p>Las <strong>estructuras de aceleración de bajo nivel</strong>
(<em>Bottom-Level Acceleration Structure</em>, BLAS) almacenan la
geometría de un objeto individual; esto es, los vértices y los índices
de los triángulos, además de una AABB que la encapsula.</p>
<p>Pueden almacenar varios modelos, puesto que alojan uno o más buffers
de vértices junto a sus matrices de transformación. Si un modelo es
instanciado varias veces <em>dentro de la misma BLAS</em>, la geometría
se duplica. Esto se hace para mejorar el rendimiento.</p>
<p>Como regla general, cuantas menos BLAS, mejor <span class="citation"
data-cites="nvidia-best-practices">(<a href="#ref-nvidia-best-practices"
role="doc-biblioref">Nvidia 2020</a>)</span>.</p>
<p>El código correspondiente a la creación de la BLAS en el programa es
el siguiente:</p>
<pre><code class="language-cpp">void Engine::createBottomLevelAS() {
    // BLAS - guardar cada primitiva en una geometría

    std::vector&lt;nvvk::RaytracingBuilderKHR::BlasInput&gt; allBlas;
    allBlas.reserve(m_objModel.size());

    for (const auto&amp; obj: m_objModel) {
        auto blas = objectToVkGeometryKHR(obj);

        // Podríamos añadir más geometrías en cada BLAS.
        // De momento, solo una.
        allBlas.emplace_back(blas);
    }

    m_rtBuilder.buildBlas(
        allBlas,
        VK_BUILD_ACCELERATION_STRUCTURE_PREFER_FAST_TRACE_BIT_KHR
    );
}</code></pre>
<h3 data-number="5.5.2" id="top-level-acceleration-structure-tlas"><span
class="header-section-number">5.5.2</span> Top-Level Acceleration
Structure (TLAS)</h3>
<p>Las Top-Level Acceleration Structures almacenan las instancias de los
objetos, cada una con su matriz de transformación y referencia a la BLAS
correspondiente.</p>
<p>Además, guardan información sobre el <em>shading</em>. Así, los
shaders pueden relacionar la geometría intersecada y el material de
dicho objeto. En esta última parte jugará un papel fundamental la <a
href="#shader-binding-table">Shader Binding Table</a>.</p>
<p>En el programa hacemos lo siguiente para construir la TLAS:</p>
<pre><code class="language-cpp">void Engine::createTopLevelAS() {
    std::vector&lt;VkAccelerationStructureInstanceKHR&gt; tlas;
    tlas.reserve(m_instances.size());

    for (const HelloVulkan::ObjInstance&amp; inst: m_instances) {
        VkAccelerationStructureInstanceKHR rayInst{};

        // Posición de la instancia
        rayInst.transform = nvvk::toTransformMatrixKHR(inst.transform);

        rayInst.instanceCustomIndex = inst.objIndex;

        // returns the acceleration structure device address of the blasId. The id correspond to the created BLAS in buildBlas.
        rayInst.accelerationStructureReference = m_rtBuilder.getBlasDeviceAddress(inst.objIndex);

        rayInst.flags = VK_GEOMETRY_INSTANCE_TRIANGLE_FACING_CULL_DISABLE_BIT_KHR;
        rayInst.mask  = 0xFF; // Solo registramos hit si rayMask &amp; instance.mask != 0
        rayInst.instanceShaderBindingTableRecordOffset = 0; // Usaremos el mismo hit group para todos los objetos

        tlas.emplace_back(rayInst);
    }

    m_rtBuilder.buildTlas(
        tlas,
        VK_BUILD_ACCELERATION_STRUCTURE_PREFER_FAST_TRACE_BIT_KHR
    );
}</code></pre>
<h2 data-number="5.6" id="la-ray-tracing-pipeline"><span
class="header-section-number">5.6</span> La ray tracing pipeline</h2>
<h3 data-number="5.6.1" id="descriptores-y-conceptos-básicos"><span
class="header-section-number">5.6.1</span> Descriptores y conceptos
básicos</h3>
<p>Primero, debemos introducir unas nociones básicas de Vulkan sobre
cómo gestiona la información que se pasa a los shaders.</p>
<p>Un <strong><em>resource descriptor</em></strong> (usualmente lo
abreviaremos como descriptor) es una forma de cargar recursos como
buffers o imágenes para que la tarjeta gráfica los pueda utilizar;
concretamente, los shaders. El <strong><em>descriptor
layout</em></strong> especifica el tipo de recurso que va a ser
accedido, mientras que el <strong><em>descriptor set</em></strong>
determina el buffer o imagen que se va a asociar al descriptor. Este set
es el que se utiliza en los <strong>drawing commands</strong>. Un
<strong>pipeline</strong> es una secuencia de operaciones que reciben
una geometría y sus texturas, y la transforma en unos pixels.</p>
<p>Si necesitas más información, todos estos conceptos aparecen
desarrollados extensamente en <span class="citation"
data-cites="overvoorde-2022">(<a href="#ref-overvoorde-2022"
role="doc-biblioref">Overvoorde 2022</a>, Descriptor layout and
buffer)</span>.</p>
<p>Tradicionalmente, en rasterización se utiliza un descriptor set por
tipo de material, y consecuentemente, un pipeline por cada tipo. En ray
tracing esto no es posible, puesto que <strong>no se sabe qué
material</strong> se va a usar: un rayo puede impactar en
<em>cualquier</em> material presente en la escena, lo cual invocaría un
shader específico. Debido a esto, empaquetaremos todos los recursos en
un único set de descriptores.</p>
<h3 data-number="5.6.2" id="la-shader-binding-table"><span
class="header-section-number">5.6.2</span> La Shader Binding Table</h3>
<p>Para solucionar esto, vamos a crear la <strong>Shader Binding
Table</strong> (SBT). Esta estructura permitirá cargar el shader
correspondiente dependiendo de dónde impacte un rayo.</p>
<p>Para cargar esta estructura, se debe hacer lo siguiente:</p>
<ol type="1">
<li>Cargar y compilar cada shader en un
<code>VkShaderModule</code>.</li>
<li>Juntar los cada <code>VkShaderModule</code> en un array
<code>VkPipelineShaderStageCreateInfo</code>.</li>
<li>Crear un array de <code>VkRayTracingShaderGroupCreateInfoKHR</code>.
Cada elemento se convertirá al final en una entrada de la Shader Binding
Table.</li>
<li>Compilar los dos arrays anteriores más un pipeline layout para
generar un <code>vkCreateRayTracingPipelineKHR</code>.</li>
<li>Conseguir los <em>handlers</em> de los shaders usando
<code>vkGetRayTracingShaderGroupHandlesKHR</code>.</li>
<li>Alojar un buffer con el bit
<code>VK_BUFFER_USAGE_SHADER_BINDING_TABLE_BIT_KHR</code> y copiar los
<em>handlers</em>.</li>
</ol>
<div id="fig:pipeline" class="fignos">
<figure>
<img loading="lazy" src="./img/04/Pipeline.png"
alt="Figura 17: La Shader Binding Table permite selccionar un tipo de shader dependiendo del objeto en el que se impacte. Para ello, se genera un rayo desde el shader raygen, el cual viaja a través de la Acceleration Structure. Dependiendo de dónde impacte, se utiliza un closest hit, any hit, o miss shaders. Fuente: (Adam Marrs and Wald 2021, 194)" />
<figcaption aria-hidden="true"><span>Figura 17:</span> La Shader Binding
Table permite selccionar un tipo de shader dependiendo del objeto en el
que se impacte. Para ello, se genera un rayo desde el shader
<code>raygen</code>, el cual viaja a través de la Acceleration
Structure. Dependiendo de dónde impacte, se utiliza un
<code>closest hit</code>, <code>any hit</code>, o <code>miss</code>
shaders. Fuente: <span class="citation" data-cites="Marrs2021">(<a
href="#ref-Marrs2021" role="doc-biblioref">Adam Marrs and Wald 2021,
194</a>)</span></figcaption>
</figure>
</div>
<p>Cada entrada de la SBT contiene un handler y una serie de parámetros
embebidos. A esto se le conoce como <strong>Shader Record</strong>.
Estos records se clasifican en:</p>
<ul>
<li><strong>Ray generation record</strong>: contiene el handler del ray
generation shader.</li>
<li><strong>Hit group record</strong>: se encargan de los handlers del
closest hit, anyhit (opcional), e intersection (opcional).</li>
<li><strong>Miss group record</strong>: se encarga del miss shader.</li>
<li><strong>Callable group record</strong>.</li>
</ul>
<p>Una de las partes más difíciles de la SBT es saber cómo se relacionan
record y geometría. Es decir, cuando un rayo impacta en una geometría,
¿a qué record de la SBT llamamos? Esto se determina mediante los
parámetros de la instancia, la llamada a <em>trace rays</em>, y el orden
de la geometría en la BLAS. En particular, resulta problemático de los
índices en los <em>hit groups</em>.</p>
<p>Para conocer a fondo cómo funciona la Shader Binding Table, puedes
visitar <span class="citation" data-cites="Marrs2021">(<a
href="#ref-Marrs2021" role="doc-biblioref">Adam Marrs and Wald 2021,
193</a>)</span> o <span class="citation"
data-cites="shader-binding-table">(<a href="#ref-shader-binding-table"
role="doc-biblioref">Will Usher 2019</a>)</span></p>
<div id="fig:SBT" class="fignos">
<figure>
<img loading="lazy" src="./img/04/SBT.png"
alt="Figura 18: Fuente: (Will Usher 2019)" />
<figcaption aria-hidden="true"><span>Figura 18:</span> Fuente: <span
class="citation" data-cites="shader-binding-table">(<a
href="#ref-shader-binding-table" role="doc-biblioref">Will Usher
2019</a>)</span></figcaption>
</figure>
</div>
<h3 data-number="5.6.3" id="tipos-de-shaders"><span
class="header-section-number">5.6.3</span> Tipos de shaders</h3>
<p>El pipeline soporta varios tipos de shaders diferentes que cubren la
funcionalidad esencial de un ray tracer:</p>
<ul>
<li><strong>Ray generation shader</strong>: es el punto de inicio del
viaje de un rayo. Calcula punto de inicio y procesa el resultado final.
Idealmente, solo se invocan rayos desde aquí. La implementación se
encuentra en
<code>application/vulkan_ray_tracing/src/shaders/raytrace.rgen</code>.</li>
<li><strong>Closest hit shader</strong>: este shader se ejecuta cuando
un rayo impacta en una geometría por primera vez. Se pueden trazar rayos
recursivamente desde aquí (por ejemplo, para calcular oclusión
ambiental). El archivo correspondiente es
<code>application/vulkan_ray_tracing/src/shaders/raytrace.rchit</code>.</li>
<li><strong>Any-hit shader</strong>: similar al closest hit, pero
invocado en cada intersección del camino del rayo que cumpla <span
class="math inline">\(t \in [t_{min}, t_{max})\)</span>. Es comúnmente
utilizado en los cálculos de transparencias (<em>alpha-testing</em>).
Puedes comprobarlo en
<code>application/vulkan_ray_tracing/src/shaders/raytrace_rahit.glsl</code>.</li>
<li><strong>Miss shader</strong>: si el rayo no choca con ninguna
geometría –pega con el infinito–, se ejecuta este shader. Normalmente,
añade una pequeña contribución ambiental al rayo. Se halla
<code>application/vulkan_ray_tracing/src/shaders/raytrace.rmiss</code>.</li>
<li><strong>Intersection shader</strong>: este shader es algo diferente
al resto. Su función es calcular el punto de impacto de un rayo con una
geometría. Por defecto se utiliza un test triángulo - rayo. En nuestro
path tracer lo dejaremos por defecto, pero podríamos definir algún
método como los que vimos en la sección <a
href="#intersecciones-rayo---objeto">intersecciones rayo -
objeto</a>.</li>
</ul>
<p>Existe otro tipo de shader adicional denominado <strong>callable
shader</strong>. Este es un shader que se invoca desde otro shader. Por
ejemplo, un shader de intersección puede invocar a un shader de
oclusión. Otro ejemplo sería un closest hit que reemplaza un bloque
if-else por un shader para hacer cálculos de iluminación. Este tipo de
shaders no se han implementado en el path tracer, pero se podrían añadir
con un poco de trabajo.</p>
<h3 data-number="5.6.4" id="traspaso-de-información-entre-shaders"><span
class="header-section-number">5.6.4</span> Traspaso de información entre
shaders</h3>
<p>En ray tracing, los shaders por sí solos no pueden realizar todos los
cálculos necesarios para conseguir la imagen final. Necesitaremos enviar
información de uno a otro. Para conseguirlo tenemos diferentes
mecanismos:</p>
<p>El primero de ellos son las <strong>push constants</strong>. Estas
son variables que se pueden traspasar a los shaders (es decir, de CPU a
GPU), pero que no se pueden modificar entre fases. Únicamente podemos
mandar un pequeño número de variables, el cual se puede consultar
mediante <code>VkPhysicalDeviceLimits.maxPushConstantSize</code>.
Además, es importante tener en cuenta el alineamiento de las estructuras
almacenadas.</p>
<p>Nuestro path tracer tiene implementado actualmente (19 de abril de
2022) las siguientes constantes:</p>
<pre><code class="language-cpp">struct PushConstantRay {
    vec4  clearColor;     // Color ambiental
    vec3  lightPosition;
    float lightIntensity;
    int   lightType;
    int   maxDepth;       // Cuántos rebotes máximos permitimos
    int   nb_samples;     // Para antialiasing
    int   frame;          // Para acumulación temporal
};</code></pre>
<p>¿Y si queremos pasar información mutable entre shaders?</p>
<p>Para eso están los <strong>payloads</strong>. Cada rayo puede llevar
información adicional, que se conoce como carga. En esencia, es como una
pequeña mochila: el rayo puede recoger información de un shader y
pasarlo a otro. Esto resulta <em>muy</em> útil, por ejemplo, a la hora
de calcular la radiancia de un camino, o saber desde qué punto venía el
rayo. Se crean mediante la estructura <code>rayPayloadEXT</code>, y se
reciben en otro shader mediante <code>rayPayloadInEXT</code>. Es
importante controlar que el tamaño de la carga no sea excesivamente
grande.</p>
<h3 data-number="5.6.5" id="creación-de-la-ray-tracing-pipeline"><span
class="header-section-number">5.6.5</span> Creación de la ray tracing
pipeline</h3>
<p>El código de la creación de la pipeline está encapsulado en la
función <code>Engine::createRtPipeline()</code>, que se puede consultar
en el archivo
<code>application/vulkan_ray_tracing/src/engine.cpp</code>.</p>
<p>En esencia, este método realiza las siguientes tareas:</p>
<ol type="1">
<li>Define las fases o <em>stages</em> que tendrán los shaders.</li>
<li>Prepara las estructuras <code>VkPipelineShaderStageCreateInfo</code>
para almacenar la información de cada fase.</li>
<li>Carga cada archivo de shader compilado <code>.spv</code> en la
estructura junto con sus parámetros correctos.</li>
<li>Configura correctamente cada <em>shader group</em>.</li>
<li>Prepara las <em>push constants</em>.</li>
<li>Hace el setup del <em>pipeline layout</em> junto a sus descriptor
sets.</li>
<li>Limpia la información innecesaria creada por la función.</li>
</ol>
<h2 data-number="5.7" id="materiales-y-objetos"><span
class="header-section-number">5.7</span> Materiales y objetos</h2>
<p>El formato de materiales y objetos usados es el
<strong>Wavefront</strong> (<code>.obj</code>). Aunque es un sistema
relativamente antiguo y sencillo, se han usado definiciones específicas
en los materiales para adaptarlo a Physically Based Rendering. Entre los
parámetros del archivo de materiales <code>.mtl</code>, destacan:</p>
<ul>
<li><span class="math inline">\(K_a \in [0, 1]^3\)</span>: representa el
color ambiental. Dado que esto es un path tracer físicamente realista,
no se usará.</li>
<li><span class="math inline">\(K_d \in [0, 1]^3\)</span>: componente
difusa.</li>
<li><span class="math inline">\(K_s \in [0, 1]^3\)</span>: componente
especular. Viene acompañada del exponente especular <span
class="math inline">\(N_s \in [0, 1000]\)</span>. Usualmente, <span
class="math inline">\(N_s = 10\)</span>. Controla los brillos en los
modelos de Blinn-Phong.</li>
<li><span class="math inline">\(d \in [0, 1]\)</span>
(<em>dissolve</em>): representa la transparencia. Alternativamente, se
usa <span class="math inline">\(T_r = 1 - d\)</span>.</li>
<li><span class="math inline">\(T_f \in [0, 1]^3\)</span>: filtro de
transmisión.</li>
<li><span class="math inline">\(N_i \in [0.001, 10]\)</span>: índice de
refracción. Usualmente <span class="math inline">\(N_i =
1\)</span>.</li>
<li><span class="math inline">\(K_e \in [0, 1]^3\)</span>: componente
emisiva (PBR).</li>
<li>Todos los valores con tres componentes pueden presentar un
<em>texture map</em>.</li>
</ul>
<p>Existe un parámetro adicional llamado <code>illum</code>. Controla el
modelo de iluminación usado. Nosotros lo usaremos para distinguir tipos
diferentes de materiales. Los códigos representan lo siguiente:</p>
<table>
<colgroup>
<col style="width: 14%" />
<col style="width: 35%" />
<col style="width: 26%" />
<col style="width: 23%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Modelo</strong></th>
<th><strong>Color</strong></th>
<th><strong>Reflejos</strong></th>
<th><strong>Transparencias</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><code>0</code></td>
<td>Difusa</td>
<td>No</td>
<td>No</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>1</code></td>
<td>Difusa, ambiental</td>
<td>No</td>
<td>No</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>2</code></td>
<td>Difusa, especular, ambiental</td>
<td>No</td>
<td>No</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>3</code></td>
<td>Difusa, especular, ambiental</td>
<td>Ray traced</td>
<td>No</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>4</code></td>
<td>Difusa, especular, ambiental</td>
<td>Ray traced</td>
<td>Cristal</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>5</code></td>
<td>Difusa, especular, ambiental</td>
<td>Ray traced (Fresnel)</td>
<td>No</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>6</code></td>
<td>Difusa, especular, ambiental</td>
<td>Ray traced</td>
<td>Refracción</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>7</code></td>
<td>Difusa, especular, ambiental</td>
<td>Ray traced (Fresnel)</td>
<td>Refracción</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>8</code></td>
<td>Difusa, especular, ambiental</td>
<td>Sí</td>
<td>No</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>9</code></td>
<td>Difusa, especular, ambiental</td>
<td>Sí</td>
<td>Cristal</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>10</code></td>
<td>Sombras arrojadizas</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<pre><code class="language-cpp">// host_device.h
struct WaveFrontMaterial
{
  vec3  ambient;
  vec3  diffuse;
  vec3  specular;
  vec3  transmittance;
  vec3  emission;
  float shininess;
  float ior;       // index of refraction
  float dissolve;  // 1 == opaque; 0 == fully transparent
  int   illum;     // illumination model (see http://www.fileformat.info/format/material/)
  int   textureId;
};</code></pre>
<h2 data-number="5.8" id="fuentes-de-luz"><span
class="header-section-number">5.8</span> Fuentes de luz</h2>
<p>La última estructura de datos importante que debemos estudiar es la
utilizada para las fuentes de luces. Desafortunadamente, en este trabajo
no se ha implementado una abstracción sólida.</p>
<p>Se ha reaprovechado la definición del <a
href="#setup-del-proyecto">rasterizador por defecto</a> para que tanto
el path tracer como el anterior utilicen fácilmente iluminación
estática.</p>
<p>La idea básica es que, en vez de depender de los elementos de la
escena para proporcionar luz, se conozca una fuente de iluminación en
todo momento. Dicha fuente puede ser puntual o direccional, y puede ser
controlada mediante la interfaz. El estado de la fuente se traspasa a
los shaders mediante una push constant:</p>
<pre><code>struct PushConstantRay
{
    ...
    vec3  light_position;
    float light_intensity;
    int   light_type;
};</code></pre>
<p>El parámetro <code>light_intensity</code> corresponde a la potencia
<span class="math inline">\(\Phi\)</span>, y el tipo
<code>light_type</code> puede ser <code>0</code> para puntual o
<code>1</code> para direccional.</p>
<p>Claramente esta decisión técnica favocere facilidad de implementación
en detrimento de flexibilidad, solidez y correctitud. Esta interfaz es
una de las áreas de futura mejora, y haría falta una revisión
considerable. Sin embargo, por el momento, funciona.</p>
<p>La implementación en los shaders es muy sencilla. Podemos usar lo
aprendido en <a
href="#next-event-estimation-o-muestreo-directo-de-fuentes-de-luz">muestreo
directo de fuentes de luz</a>. En el closest hit, primero calculamos la
información relativa a la posición y la intensidad de la luz:</p>
<pre><code class="language-glsl">vec3 L;
float light_intensity = pcRay.light_intensity;
float light_distance = 100000.0;

float pdf_light       = 1;  // prob. de escoger ese punto de la fuente de luz
float cos_theta_light = 1;  // Ángulo entre la la dir. del rayo y luz.

if (pcRay.light_type == 0) {         // Point light
    vec3 L_dir = pcRay.light_position - world_position;  // vector hacia la luz

    light_distance   = length(L_dir);
    light_intensity = pcRay.light_intensity / (light_distance * light_distance);
    L               = normalize(L_dir);
    // Solo tenemos un punto =&gt; pdf light = 1, cos_theta light = 1.
    cos_theta_light = dot(L, world_normal);
}
else if (pcRay.light_type == 1) {    // Directional light
    L = normalize(pcRay.light_position);
    cos_theta_light = dot(L, world_normal);
}</code></pre>
<p>Sin embargo, esto no es suficiente. Se nos olvida comprobar un
detalle sumamente importante:</p>
<p>¿Se ve la fuente de luz desde el punto de intersección?</p>
<p>Si no es así, ¡no tiene sentido que calculemos la influencia
luminaria de la fuente! La carne de burro no se transparenta, después de
todo. A no ser que sea un toro hecho de algún material que presente
transmitancia, en cuyo caso se debería refractar acordemente el rayo de
luz.</p>
<p>Volviendo al tema: este tipo de problemas de oclusión se suelen
resolver mediante algún tipo de test de visibilidad. El más habitual es
usar <strong>shadow rays</strong>. Al preparar la <a
href="#la-ray-tracing-pipeline">pipeline</a> fijamos el stage de los
shadow rays precisamente por este motivo.</p>
<p>La continuación del código quería de la siguiente forma:</p>
<pre><code class="language-glsl">if (dot(normal, L) &gt; 0) {
    // Preparar la invocación del shadow ray
    float tMin = 0.001;
    float tMax = light_distance;

    vec3 origin = gl_WorldRayOriginEXT + gl_WorldRayDirectionEXT * gl_HitTEXT;
    vec3 ray_dir = L;

    uint flags = gl_RayFlagsSkipClosestHitShaderEXT;
    prdShadow.is_hit = true;
    prdShadow.seed = prd.seed;

    traceRayEXT(topLevelAS,
        flags,       // rayFlags
        0xFF,        // cullMask
        1,           // sbtRecordOffset =&gt; invocar el shader de sombras
        0,           // sbtRecordStride
        1,           // missIndex
        origin,      // ray origin
        tMin,        // ray min range
        ray_dir,     // ray direction
        tMax,        // ray max range
        1            // payload (location = 1)
    );

    float attenuation = 1;

    if (!prdShadow.is_hit) {
        hit_value = hit_value + light_intensity*BSDF*cos_theta_light / pdf_light;
    }
    else {
        attenuation = 1.0 / (1.0 + light_distance);
    }
}</code></pre>
<p>Y con esto, hemos conseguido añadir dos tipos de fuentes de
iluminación.</p>
<h2 data-number="5.9"
id="antialiasing-mediante-jittering-y-acumulación-temporal"><span
class="header-section-number">5.9</span> Antialiasing mediante jittering
y acumulación temporal</h2>
<p>Normalmente, mandamos los rayos desde el centro de un pixel. Podemos
conseguir una mejora sustancial de la calidad con un pequeño truco: en
vez de generarlos siempre desde el mismo sitio, le aplicamos una pequeña
perturbación (<em>jittering</em>). Así, tendremos una variación de
colores para un mismo pixel, por lo que podemos hacer una ponderación de
todos ellos. A este proceso lo que llamamos <strong>acumulación
temporal</strong>.</p>
<p>Es importante destacar que el efecto de esta técnica solo es válido
cuando la <strong>cámara se queda estática</strong>. Al cambiar de
posición, la información del píxel se ve alterada significativamente,
por lo que debemos reconstruir las muestras desde el principio.</p>
<p>La implementación es muy sencilla. Está basada en el tutorial de
<span class="citation" data-cites="nvpro-samples-tutorial">(<a
href="#ref-nvpro-samples-tutorial" role="doc-biblioref">Nvidia
2022a</a>, jitter camera)</span>. Debemos modificar tanto el motor como
los shaders para llevar el recuento del número de frames en las push
constants.</p>
<p>Definimos el número máximo de frames que se pueden acumular:</p>
<pre><code class="language-cpp">// engine.h
class Engine {
    //...
    int m_maxAcumFrames {100};
}</code></pre>
<p>Las push constant deberán llevar un registro del frame en el que se
encuentran, así como un número máximo de muestras a acumular para un
pixel:</p>
<pre><code class="language-cpp">// host_device.h
struct PushConstantRay {
    //...
    int   frame;
    int   nb_samples
}</code></pre>
<p>El número de frame se reseteará cuando la cámara se mueva, la ventana
se reescale, o se produzca algún efecto similar en la aplicación.</p>
<p>Finalmente, en los shaders podemos implementar lo siguiente:</p>
<pre><code class="language-glsl">// raytrace.rgen
vec3 pixel_color = vec3(0);

for (int smpl = 0; smpl &lt; pcRay.nb_samples; smpl++) {
    pixel_color += sample_pixel(image_coords, image_res);
}

pixel_color = pixel_color / pcRay.nb_samples;

if (pcRay.frame &gt; 0) {
    vec3 old_color = imageLoad(image, image_coords).xyz;
    vec3 new_result = mix(
        old_color,
        pixel_color,
        1.f / float(pcRay.frame + 1)
    );

    imageStore(image, image_coords, vec4(new_result, 1.f));
}
else {
    imageStore(image, image_coords, vec4(pixel_color, 1.0));
}</code></pre>
<pre><code class="language-glsl">// pathtrace.glsl
vec3 sample_pixel() {
    float r1 = rnd(prd.seed);
    float r2 = rnd(prd.seed);

    // Subpixel jitter: mandar el rayo desde una pequeña perturbación del pixel para aplicar antialiasing
    vec2 subpixel_jitter = pcRay.frame == 0
        ? vec2(0.5f, 0.5f)
        : vec2(r1, r2);

    const vec2 pixelCenter = vec2(image_coords.xy) + subpixel_jitter;

    // ...

    vec3 radiance = pathtrace(rayo);
}</code></pre>
<blockquote>
<p>TODO: mostrar vídeo de ejemplo</p>
</blockquote>
<h2 data-number="5.10" id="corrección-de-gamma"><span
class="header-section-number">5.10</span> Corrección de gamma</h2>
<p>Con el código de la sección <a
href="#antialiasing-mediante-jittering-y-acumulación-temporal">anterior</a>,
existe un problema con los colores finales. El algoritmo de pathtracing
no limita el máximo valor que puede tomar un camino. Sin embargo, Vulkan
espera que la terna RGB provista esté en <span class="math inline">\([0,
1]^3\)</span>. Esto implica que los colores acabarán quemados.</p>
<div id="fig:quemado" class="fignos">
<figure>
<img loading="lazy" src="./img/04/Quemado.png"
alt="Figura 19: Fíjate en la parte de la izquierda. La pared roja aparece demasiado brillante; especialmente, aquella impactada por la fuente de luz." />
<figcaption aria-hidden="true"><span>Figura 19:</span> Fíjate en la
parte de la izquierda. La pared roja aparece demasiado brillante;
especialmente, aquella impactada por la fuente de luz.</figcaption>
</figure>
</div>
<p>Podemos corregir este problema mediante <strong>corrección de
gamma</strong>. Esta es una operación no lineal utilizada en fotografía
para corregir la luminacia, con el fin de compensar la percepción no
lineal del brillo por parte de los humanos. En este caso, lo haremos al
estilo <span class="citation" data-cites="Shirley2020RTW1">(<a
href="#ref-Shirley2020RTW1" role="doc-biblioref">Shirley
2020a</a>)</span>: tras tomar las muestras, aplicaremos una corrección
para <span class="math inline">\(\gamma = 2.2\)</span>, lo cual implica
elevar cada componente del píxel a la potencia <span
class="math inline">\(\frac{1}{2.2}\)</span>; es decir, <span
class="math inline">\((r_f, g_f, b_f) = (r^{\frac{1}{2.2}},
g^{\frac{1}{2.2}}, b^{\frac{1}{2.2}})\)</span>.</p>
<p>Tras esto, limitaremos el valor máximo de cada componente a 1 con la
operación <span class="math inline">\(clamp()\)</span>.</p>
<pre><code class="language-glsl">vec3 pixel_color = vec3(0);

for (int smpl = 0; smpl &lt; pcRay.nb_samples; smpl++) {
    pixel_color += sample_pixel(image_coords, image_res);
}

pixel_color = pixel_color / pcRay.nb_samples;

if (USE_GAMMA_CORRECTION == 1) {
    pixel_color = pow(pixel_color, vec3(1.0 / 2.2));  // Gamma correction for 2.2
    pixel_color = clamp(pixel_color, 0.0, 1.0);
}</code></pre>
<div id="fig:correccion_gamma" class="fignos">
<figure>
<img loading="lazy" src="./img/04/Corrección%20de%20gamma.png"
alt="Figura 20: Con la corección de gamma aplicada, vemos que los colores de la foto no son tan intensos." />
<figcaption aria-hidden="true"><span>Figura 20:</span> Con la corección
de gamma aplicada, vemos que los colores de la foto no son tan
intensos.</figcaption>
</figure>
</div>
<blockquote>
<p>Espera. Esa no parece la misma escena. ¿No han cambiado los colores
demasiado?</p>
</blockquote>
<p>¡Bien visto! Es cierto que los colores se ven significativamente
alterados. Esto es debido a la conversión de un espacio lineal de
respuesta de radiancia a uno logarítmico. Algunos autores como Íñigo
Quílez (coautor de la página Shader Toy) prefieren asumir esta
deficiencia, y modificar los materiales acordemente a esto <span
class="citation" data-cites="gamma-correction">(<a
href="#ref-gamma-correction" role="doc-biblioref">Íñigo Quílez 2013</a>,
The Color Space)</span>.</p>
<p>Nosotros no nos preocuparemos especialmente por esto. Este no es un
trabajo sobre teoría del color, aunque nos metamos en varias partes en
ella. El área de tone mapping es extensa y merecería su propio
estudio.</p>
<p>Es importante mencionar que sin acumulación temporal, el código
anterior produciría variaciones significativas para pequeños
movimientos. Hay otras formas de compensarlo, como dividir por el valor
promedio de las muestras más brillantes. Nosotros hemos optado por
mezclar los píxeles generados a lo largo del tiempo.</p>
<h1 data-number="6" id="análisis-de-rendimiento"><span
class="header-section-number">6</span> Análisis de rendimiento</h1>
<p>En este capítulo vamos a analizar el resultado final del proyecto.
Estudiaremos cómo se ve el motor, cómo rinde en términos de <em>frame
time</em>, y compararemos las imágenes producidas con otras similares;
tanto producidas por otros motores, como situaciones en la vida
real.</p>
<h2 data-number="6.1" id="usando-el-motor"><span
class="header-section-number">6.1</span> Usando el motor</h2>
<p>Una vez se ha <a href="#compilación-y-ejecución">compilado</a> el
proyecto, puedes encontrar el ejecutable en
<code>./application/bin_x64/Debug</code>. Abre el binario para entrar en
el programa.</p>
<div id="fig:asmiray" class="fignos">
<figure>
<img loading="lazy" src="./img/05/Asmiray.png" style="width:70.0%"
alt="Figura 21: Al abrir el motor, te encontrarás con una pantalla similar a esta: una escena cargada junto a un panel lateral con numerosas opciones." />
<figcaption aria-hidden="true"><span>Figura 21:</span> Al abrir el
motor, te encontrarás con una pantalla similar a esta: una escena
cargada junto a un panel lateral con numerosas opciones.</figcaption>
</figure>
</div>
<p>Si alguna vez has usado un motor de renderización en 3D (como
Blender, Unity, Unreal Engine o AutoCAD), el comportamiento debería
resultarte familiar. El uso de nuestro progama es muy similar al de los
anteriores:</p>
<ul>
<li>El <strong>botón izquierdo del ratón rota</strong> la cámara
alderedor del punto de mira.</li>
<li>Para acercar o alejar la cámara, utiliza la <strong>rueda de
scroll</strong> o el <strong>botón derecho del ratón + hacia arriba o
abajo</strong>.</li>
<li>Si quieres moverte lateralmente, mantén pulsado la tecla
<strong>control</strong> y utiliza el <strong>botón izquierdo +
arrastrar</strong>. Alternativamente, <strong>aprieta el click de la
rueda del ratón</strong> y múevete.</li>
<li>Para girar la cámara alderedor como si de un <em>first person
shooter</em> se tratara, pulsa <strong>alt + click
izquierdo</strong>.</li>
</ul>
<p>Puedes cambiar el modo de cámara en la pestaña “Extra” de la interfaz
gráfica. Los diferentes modos alternan entre las acciones listadas
anteriormente.</p>
<p>Para ocultar la interfaz gráfica, pulsa <strong>F10</strong>.</p>
<h3 data-number="6.1.1" id="cambio-de-escena"><span
class="header-section-number">6.1.1</span> Cambio de escena</h3>
<p>El programa viene acompañado de varios mapas. Desafortunadamente,
para cambiar de escena es necesario recompilar el programa. Las
instrucciones necesarias para conseguirlo son las siguientes:</p>
<ul>
<li>Ubica la sentencia <code>load_scene(Scene::escena, engine);</code>
que se encuentra en el archivo <code>main.cpp</code>.</li>
<li>Cambia el valor del primer parámetro: reemplaza
<code>Scene::escena</code> por alguna entrada del enumerado
<code>Scene</code>. Puedes encontrar sus posiblidades en el archivo
<code>Scenes.hpp</code>.</li>
<li>Recompila el programa.</li>
</ul>
<p>Las escenas son las siguientes:</p>
<table>
<colgroup>
<col style="width: 12%" />
<col style="width: 65%" />
<col style="width: 21%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>Nombre de escena</strong></th>
<th><strong>Descripción</strong></th>
<th><strong>Imagen</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>cube_default</code></td>
<td>La escena por defecto del programa. Muestra un simple cubo.</td>
<td><img loading="lazy" src="./img/05/cube_default.png" title="fig:" /></td>
</tr>
<tr class="even">
<td><code>any_hit</code></td>
<td>Desmostración de las capacidades del shader <em>anyhit</em>.</td>
<td><img loading="lazy" src="./img/05/any_hit.png" title="fig:" /></td>
</tr>
<tr class="odd">
<td><code>cube_reflective</code></td>
<td>Ejemplifica <em>ray traced reflections</em>.</td>
<td><img loading="lazy" src="./img/05/cube_reflective.png" title="fig:" /></td>
</tr>
<tr class="even">
<td><code>medieval</code> <code>_building</code></td>
<td>Una sencilla escena que contiene una casa medieval con
texturas.</td>
<td><img loading="lazy" src="./img/05/medieval_building.png" title="fig:" /></td>
</tr>
<tr class="odd">
<td><code>cubes</code></td>
<td>Dos cubos de diferente material sobre un plano reflectante.</td>
<td><img loading="lazy" src="./img/05/cubes.png" title="fig:" /></td>
</tr>
<tr class="even">
<td><code>cornell_box</code> <code>_original</code></td>
<td>Una reconstrucción de la caja de Cornell original <span
class="citation" data-cites="cornell-box-original">(<a
href="#ref-cornell-box-original" role="doc-biblioref">Cornell University
2005</a>)</span>.</td>
<td><img loading="lazy" src="./img/05/cornell_box_original.png" title="fig:" /></td>
</tr>
<tr class="odd">
<td><code>cornell_box</code> <code>_mirror</code></td>
<td>Similar a la caja original, esta escena es una recreación de <span
class="citation" data-cites="cornell-box-compare">(<a
href="#ref-cornell-box-compare" role="doc-biblioref">Cornell University
1998</a>)</span>.</td>
<td><img loading="lazy" src="./img/05/cornell_box_mirror.png" title="fig:" /></td>
</tr>
<tr class="even">
<td><code>cornell_box</code> <code>_esferas</code></td>
<td>Una caja de Cornell con esferas. Se puede comparar con <span
class="citation" data-cites="Jensen2001">(<a href="#ref-Jensen2001"
role="doc-biblioref">Jensen 2001, 107</a> fig. 9.10)</span>.</td>
<td><img loading="lazy" src="./img/05/cornell_box_esferas.png" title="fig:" /></td>
</tr>
<tr class="odd">
<td><code>cornell_box</code> <code>_saturada</code></td>
<td>Otra caja de Cornell similar a la original, pero con las paredes
saturadas.</td>
<td><img loading="lazy" src="./img/05/cornell_box_saturada.png" title="fig:" /></td>
</tr>
<tr class="even">
<td><code>cornell_box</code> <code>_glossy</code></td>
<td>En esta caja se encuentran dos esferas de diferente material. Se
puede comparar con <span class="citation"
data-cites="cornell-box-glossy">(<a href="#ref-cornell-box-glossy"
role="doc-biblioref">Jensen 1996, 17</a>, fig. 6)</span></td>
<td><img loading="lazy" src="./img/05/cornell_box_glossy.png" title="fig:" /></td>
</tr>
<tr class="odd">
<td><code>cornell_box</code> <code>_iow</code></td>
<td>La última caja de Cornell implementada en <span class="citation"
data-cites="Shirley2020RTW3">(<a href="#ref-Shirley2020RTW3"
role="doc-biblioref">Shirley 2020c</a>)</span>.</td>
<td><img loading="lazy" src="./img/05/cornell_box_iow.png" title="fig:" /></td>
</tr>
<tr class="even">
<td><code>cornell_box</code> <code>_vacia</code></td>
<td>La caja original sin las cajitas pequeñas dentro.</td>
<td><img loading="lazy" src="./img/05/cornell_box_vacia.png" title="fig:" /></td>
</tr>
<tr class="odd">
<td><code>cornell_box</code> <code>_vacia_an</code></td>
<td>Similar a la anterior, pero con las paredes naranjas y azules.</td>
<td><img loading="lazy" src="./img/05/cornell_box_vacia_an.png" title="fig:" /></td>
</tr>
<tr class="even">
<td><code>cornell_box</code> <code>_blanca</code></td>
<td>Una caja vacía. Es un benchmark infernal para el ruido generado por
la iluminación global.</td>
<td><img loading="lazy" src="./img/05/cornell_box_blanca.png" title="fig:" /></td>
</tr>
</tbody>
</table>
<p>Ten en cuenta que las imágenes de las escenas no son definitivas.
Están sujetas a cambios, pues se podría cambiar el comportamiento de los
shaders.</p>
<h2 data-number="6.2" id="path-tracing-showcase"><span
class="header-section-number">6.2</span> Path tracing showcase</h2>
<p>A lo largo de este trabajo hemos visto una gran variedad de conceptos
desde el punto de vista teórico. Ahora es el momento de ponerlo en
práctica.</p>
<h3 data-number="6.2.1" id="materiales"><span
class="header-section-number">6.2.1</span> Materiales</h3>
<p>Empecemos por materiales. Se han implementado unos cuantos tipos
diferentes, los cuales veremos ilustrados a continuación.</p>
<p>Los más simples son los <a
href="#reflexión-difusa-o-lamberiana">difusos</a>. La caja de Cornell
original contiene dos objetos de este tipo:</p>
<div id="fig:materiales_difusos" class="fignos">
<figure>
<img loading="lazy" src="./img/05/Materiales%20difusos.png" style="width:70.0%"
alt="Figura 22: Materiales difusos de la escena Cornell Box original. Vemos que la luz se esparce uniformemente al rebotar en el objeto." />
<figcaption aria-hidden="true"><span>Figura 22:</span> Materiales
difusos de la escena Cornell Box original. Vemos que la luz se esparce
uniformemente al rebotar en el objeto.</figcaption>
</figure>
</div>
<p>Los materiales <a href="#reflexión-especular-no-perfecta">especulares
<em>glossy</em></a> han sido modificados ligeramente para simular el
parámetro de <em>roughness</em> de los metales, para compararlos con los
de <span class="citation" data-cites="Shirley2020RTW1">(<a
href="#ref-Shirley2020RTW1" role="doc-biblioref">Shirley
2020a</a>)</span>:</p>
<div id="fig:materiales_glossy" class="fignos">
<figure>
<img loading="lazy" src="./img/05/Materiales%20glossy.png" style="width:70.0%"
alt="Figura 23: Materiales especulares metálicos de la escena Cornell Box glossy" />
<figcaption aria-hidden="true"><span>Figura 23:</span> Materiales
especulares metálicos de la escena Cornell Box glossy</figcaption>
</figure>
</div>
<p>Si hay algo en lo que destaca ray tracing, es en la simulación de <a
href="#reflexión-especular-perfecta">espejos</a>. En rasterización
debemos recurrir a técnicas específicas como reflejos planares o
<em>cubemaps</em>. Ray tracing solventa el problema con elegancia:</p>
<div id="fig:materiales_espejos" class="fignos">
<figure>
<img loading="lazy" src="./img/05/Materiales%20espejos.png" style="width:70.0%"
alt="Figura 24: Una caja que actúa como un espejo prácticamente perfecto en la escena cornell_box_mirror" />
<figcaption aria-hidden="true"><span>Figura 24:</span> Una caja que
actúa como un espejo prácticamente perfecto en la escena
<code>cornell_box_mirror</code></figcaption>
</figure>
</div>
<p>En la siguiente escena observamos dos esferas: una que presenta
refracción y otra que no. Ambas utilizan las ecuaciones de Fresnel para
modelar el comportamiento de la luz.</p>
<div id="fig:reflectantes" class="fignos">
<figure>
<img loading="lazy" src="./img/05/Materiales%20reflectantes.png" style="width:70.0%"
alt="Figura 25: La esfera de la derecha refracta la luz al pasar por ella, adquiriendo en el proceso un color más oscuro. También podemos ver la esfera de la izquierda recursivamente, dentro del propio reflejo de la esfera." />
<figcaption aria-hidden="true"><span>Figura 25:</span> La esfera de la
derecha refracta la luz al pasar por ella, adquiriendo en el proceso un
color más oscuro. También podemos ver la esfera de la izquierda
recursivamente, dentro del propio reflejo de la esfera.</figcaption>
</figure>
</div>
<p>Los materiales transparentes los gestiona el shader <a
href="#tipos-de-shaders">anyhit</a>. Permite descartar las
intersecciones con aquellos objetos transparentes para permitir pasar
algunos rayos:</p>
<div id="fig:materiales_transparentes" class="fignos">
<figure>
<img loading="lazy" src="./img/05/Materiales%20transparentes.png" style="width:70.0%"
alt="Figura 26: El modelo del Wuson, pero transparente." />
<figcaption aria-hidden="true"><span>Figura 26:</span> El modelo del
Wuson, pero transparente.</figcaption>
</figure>
</div>
<h3 data-number="6.2.2" id="fuentes-de-luz-1"><span
class="header-section-number">6.2.2</span> Fuentes de luz</h3>
<p>En la primera versión del motor, se han implementado dos tipos de
fuentes de luces: puntuales y direccionales.</p>
<p>Las <strong>fuentes de luz puntuales</strong> (<em>spotlights</em> en
inglés) emiten luz alrededor suya, como si de pequeños soles se
trataran. La figura [<a href="#fig:spotlights">27</a>] muestra cómo se
comportan en la caja de Cornell original.</p>
<div id="fig:spotlights" class="fignos">
<figure>
<img loading="lazy" src="./img/05/Spotlight.png" style="width:60.0%"
alt="Figura 27: Una fuente de luz puntual iluminando la caja de Cornell original. Vemos cómo se proyectan sombras hacian la pared." />
<figcaption aria-hidden="true"><span>Figura 27:</span> Una fuente de luz
puntual iluminando la caja de Cornell original. Vemos cómo se proyectan
sombras hacian la pared.</figcaption>
</figure>
</div>
<p>Por otro lado, las <strong>luces direccionales</strong>: imitan la
luz proporcionada por algún objeto infinitamente lejano. Puedes ver un
ejemplo en la figura [<a href="#fig:directional_lights">28</a>].</p>
<div id="fig:directional_lights" class="fignos">
<figure>
<img loading="lazy" src="./img/05/Directional.png" style="width:60.0%"
alt="Figura 28: La caja de Cornell original iluminada por una luz direccional" />
<figcaption aria-hidden="true"><span>Figura 28:</span> La caja de
Cornell original iluminada por una <strong>luz
direccional</strong></figcaption>
</figure>
</div>
<p>Como las puntuales se comportan de manera muy similar en la caja de
Cornell, podemos referirnos a la escena del edificio medieval para ver
una diferencia más sustancial [<a
href="#fig:spotlights_medieval">29</a>, <a
href="#fig:directional_medieval">30</a>]. En este caso, se aprecia el
radio de influencia de la luz puntual.</p>
<div id="fig:spotlights_medieval" class="fignos">
<figure>
<img loading="lazy" src="./img/05/Spotlight%202.png" style="width:60.0%"
alt="Figura 29: Luz puntual en la escena medieval_building." />
<figcaption aria-hidden="true"><span>Figura 29:</span> Luz puntual en la
escena <code>medieval_building</code>.</figcaption>
</figure>
</div>
<div id="fig:directional_medieval" class="fignos">
<figure>
<img loading="lazy" src="./img/05/Directional%202.png" style="width:60.0%"
alt="Figura 30: Luz direccional en la escena medieval_building." />
<figcaption aria-hidden="true"><span>Figura 30:</span> Luz direccional
en la escena <code>medieval_building</code>.</figcaption>
</figure>
</div>
<h3 data-number="6.2.3" id="iluminación-global"><span
class="header-section-number">6.2.3</span> Iluminación global</h3>
<p>La <strong>iluminación global</strong> es un fenómeno físico que se
refiere a luz que proviene de <em>todas</em> direcciones. Es el efecto
que propicia el rebote constante de los fotones emitidos por fuentes de
luz hacia una escena, adquiriendo las propiedades de los materiales en
los que rebotan.</p>
<p>Dicho de esta forma, es difícil imaginarse cómo se comporta en la
vida real. Para ilustrarlo, tomemos dos fotografías de una escena
similar a la caja de Cornell:</p>
<div id="fig:cornell_bath_1" class="fignos">
<figure>
<img loading="lazy" src="./img/05/Cornell%20bath%201.jpg" style="width:60.0%"
alt="Figura 31: Escena similar a la caja de Cornell, en la vida real. También es mi baño." />
<figcaption aria-hidden="true"><span>Figura 31:</span> Escena similar a
la caja de Cornell, en la vida real. También es mi baño.</figcaption>
</figure>
</div>
<p>En la escena [<a href="#fig:cornell_bath_1">31</a>] observamos cómo
la luz del sol entra desde la parte de la derecha, rebotando en todo el
espacio. Notamos cómo la escena tiene una tonalidad natural y cálida.
Sin embargo, esta impresión es fácilmente modificable si alteramos la
forma de arrojar la luz. Cerrando la puerta (la cual no puede ser vista
en la fotografía, pero se encuentra a la derecha y es idéntica al
cristal), la iluminación cambia completamente [<a
href="#fig:cornell_bath_2">32</a>].</p>
<div id="fig:cornell_bath_2" class="fignos">
<figure>
<img loading="lazy" src="./img/05/Cornell%20bath%202.jpg" style="width:60.0%"
alt="Figura 32: Cuando cambiamos la forma de iluminar la escena, los colores se ven drásticamente modificados" />
<figcaption aria-hidden="true"><span>Figura 32:</span> Cuando cambiamos
la forma de iluminar la escena, los colores se ven drásticamente
modificados</figcaption>
</figure>
</div>
<p>Podemos observar cómo todos los materiales adquieren un tinte rojizo,
debido a la influencia tanto difusa como especular de la pared de la
izquierda. Objetos que antes eran blancos inmaculados se vuelven rojos,
como el inodoro. Incluso aquellas zonas en sombra consiguen un color
rojizo. Esto es debido a que los fotones rebotan en el cristal rojo
cuando más energía tienen. De esta forma, en la siguiente dirección
tomada, los rayos transportan esta propiedad al resto de materiales, los
cuales se visualizan como una tonalidad roja.</p>
<p><strong>Path tracing consigue este efecto de manera natural</strong>
por diseño. Este es uno de sus mayores puntos fuertes, pero a la vez lo
hace computacionalmente caro. Dado que la escena de [<a
href="#fig:cornell_bath_1">31</a>] y [<a
href="#fig:cornell_bath_2">32</a>] es, esencialmente, una caja de
Cornell, deberíamos apreciar un efecto similar en nuestra escena,
¿verdad?</p>
<p>¡Así es! La figura [<a href="#fig:global_illumination_1">33</a>] es
muy similar a la [<a href="#fig:cornell_bath_2">32</a>]. Se pueden
apreciar los mismos efectos en la caja izquierda, los cuales no ocurrían
con tanta intensidad en la escena original [fig. <a
href="#fig:materiales_difusos">22</a>]</p>
<div id="fig:global_illumination_1" class="fignos">
<figure>
<img loading="lazy" src="./img/05/Ambient%20occlusion%201.png" style="width:60.0%"
alt="Figura 33: La caja de Cornell original con luz direccional apuntando a la pared de la izquierda" />
<figcaption aria-hidden="true"><span>Figura 33:</span> La caja de
Cornell original con luz direccional apuntando a la pared de la
izquierda</figcaption>
</figure>
</div>
<p>Lo mismo ocurre cuando cambiamos el foco a la pared de la derecha. Al
ser verde, tintará el resto de los materiales de dicho color [<a
href="#fig:global_illumination_2">34</a>].</p>
<div id="fig:global_illumination_2" class="fignos">
<figure>
<img loading="lazy" src="./img/05/Ambient%20occlusion%202.png" style="width:60.0%"
alt="Figura 34: Enfocando a la pared de la derecha, conseguimos un tinte verde para la escena" />
<figcaption aria-hidden="true"><span>Figura 34:</span> Enfocando a la
pared de la derecha, conseguimos un tinte verde para la
escena</figcaption>
</figure>
</div>
<p>Este efecto es esencial para proporcionar realismo a una imagen
digital. Sin iluminación global, los motores presentan un aspecto que
podríamos considerar <em>videojueguil</em>: imágenes planas, con sombras
abruptas y un aire de falsedad al que nos hemos llegado a acostumbrar.
Por ello se han implementado varias técnicas en rasterización para
suplir este efecto. Destacan los <em>lightmaps</em>, la <a
href="#materiales-y-objetos">componente ambiental</a> de los materiales,
<em>cubemaps</em>, oclusión ambiental e iluminación indirecta basada en
<em>probes</em>.</p>
<p>Para un vistazo más a fondo de la iluminación global, puedes
referirte al vídeo de Alex Battaglia en <span class="citation"
data-cites="df-global-illumination">(<a
href="#ref-df-global-illumination" role="doc-biblioref">Digital Foundry
2021</a>)</span>, en el cual cubre diferentes formas de resolver este
problema, tanto en el caso de ray tracing como en el de
rasterización.</p>
<h2 data-number="6.3" id="rendimiento"><span
class="header-section-number">6.3</span> Rendimiento</h2>
<p>Path tracing es un algoritmo costoso. Teniendo en cuenta que tratamos
de desarrollar una aplicación en tiempo real, debemos prestar especial
atención al coste de renderizar un <em>frame</em>. En esta sección vamos
a hacer una comparativa de las diferentes opciones que se han
implementado en el motor, estudiando la relación calidad de imagen y
rendimiento.</p>
<p>Utilizaremos principalmente dos escenas:
<code>cornell_box_original</code> y <code>cornell_box_esferas</code>.
Esto es debido a que ofrecen cierta complejidad y los materiales de los
objetos permiten estudiar los parámetros del motor.</p>
<p>Para los análisis del rendimiento, se ha utilizado un procesador
<strong>Intel i5 12600K</strong>, una tarjeta gráfica Nvidia
<strong>2070 Super</strong> con un ligero overclock a 1900MHz y
<strong>2x8GB DDR4 3200MHz</strong> de RAM. A no ser que se diga lo
contrario, todas las imágenes tienen una resolución de 1280 x 720. Con
el fin de realizar una comparación justa, se ha implementado un modo de
benchmarking que se puede activar en el archivo
<code>globals.hpp</code>.</p>
<h3 data-number="6.3.1" id="número-de-muestras"><span
class="header-section-number">6.3.1</span> Número de muestras</h3>
<p>El principal parámetro que podemos variar es el número de muestras
por píxel. En un estimador de Monte Carlo [<a
href="#eq:mc_integral">16</a>], <span class="math inline">\(\hat{I}_N =
\frac{1}{N} \sum_{i = 1}^{N}{f(X_i)}\)</span>, corresponde a <span
class="math inline">\(N \in \mathbb{N}\)</span>.</p>
<div id="fig:grafica_samples" class="fignos">
<figure>
<img loading="lazy" src="./img/graficas/CB_original_comparativa_samples.png"
alt="Figura 35: Para conseguir esta gráfica, iniciamos la escena cornell_box_original, y sin mover la cámara, vamos cambiando el número de muestras." />
<figcaption aria-hidden="true"><span>Figura 35:</span> Para conseguir
esta gráfica, iniciamos la escena <code>cornell_box_original</code>, y
sin mover la cámara, vamos cambiando el número de muestras.</figcaption>
</figure>
</div>
<p>La figura [<a href="#fig:grafica_samples">35</a>] muestra cómo afecta
al rendimiento el valor de <span class="math inline">\(N\)</span>. Vemos
cómo un número bajo de muestras (alrededor de 5) produce un frametime de
aproximadamente 12 milisegundos, lo cual corresponde a 83 frames por
segundo. Duplicando <span class="math inline">\(N\)</span> hasta las 10
muestras, produce un aumento del frametime hasta los 20 ms de media (50
FPS). Algo similar pasa con el resto de valores: 15 muestras suponen una
media de 28 ms (35 FPS) y 20 muestras unos 35 ms (28 FPS). Sacamos en
claro que, en esta escena, <strong>no debemos aumentar las muestras a un
valor superior a 20</strong>, pues entraríamos en terreno de renderizado
en diferido. No debemos superar la barrera de los 33 milisegundos, pues
supondría una tasa de refresco de imagen inferior a los 30 FPS.</p>
<p>Podemos concluir que, en esta escena, el <strong>coste de una muestra
por píxel</strong> es de aproximadamente <strong>2
milisegundos</strong>. Este valor puede ser hallado promediando el coste
medio de cada frame en cada valor del parámetro.</p>
<p>El número de muestras tiene un grandísimo efecto en la calidad de
imagen. Volviendo a la escena anterior, podemos ver cómo cambia el ruido
al variar el parámetro <code>samples</code>. Para las siguientes
imágenes, se ha deshabilitado la <a
href="#acumulación-temporal">acumulación temporal</a>, pues en esencia,
proporcionaría un mayor número de muestras en el tiempo.</p>
<p>Con una única muestra por píxel, la imagen final aparece muy ruidosa.
Aumentarlo a 5 mejora bastante la situación, pero sigue habiendo
demasiado ruido [<a href="#fig:samples_1">36</a>]</p>
<div id="fig:samples_1" class="fignos">
<figure>
<img loading="lazy" src="./img/05/1,%205%20samples.png"
alt="Figura 36: Izquierda: 1 muestra. Derecha: 5 muestras" />
<figcaption aria-hidden="true"><span>Figura 36:</span>
<strong>Izquierda</strong>: 1 muestra. <strong>Derecha</strong>: 5
muestras</figcaption>
</figure>
</div>
<p>De forma similar, aumentarlo a 10 y a 20 implica un aumento de la
calidad visual significativa.</p>
<div id="fig:samples_2" class="fignos">
<figure>
<img loading="lazy" src="./img/05/10,%2020%20samples.png"
alt="Figura 37: Izquierda: 10 muestras. Derecha: 20 muestras" />
<figcaption aria-hidden="true"><span>Figura 37:</span>
<strong>Izquierda</strong>: 10 muestras. <strong>Derecha</strong>: 20
muestras</figcaption>
</figure>
</div>
<p>No obstante, el cambio de 10 a 20 no es tan significativo como de 1 a
5. Esto sugiere que debemos usar otras técnicas para reducir la varianza
del estimador. ¡La fuerza bruta no suele ser la solución!</p>
<h3 data-number="6.3.2" id="profundidad-de-un-rayo"><span
class="header-section-number">6.3.2</span> Profundidad de un rayo</h3>
<p>Una de las decisiones que tenemos que tomar en el diseño del
algoritmo es saber cuándo cortar un camino. Hay varias formas de
hacerlo, aunque destacan principalmente dos: fijar un valor máximo de
profundidad o la <a href="#ruleta-rusa">ruleta rusa</a>.</p>
<p>Analicemos la primera opción, que es la que hemos implementado
nosotros. Para ello, usaremos a la escena
<code>cornell_box_esferas</code>, pues los materiales reflectivos y
refractantes de las esferas nos servirán de ayuda para estudiar el coste
de un camino.</p>
<div id="fig:grafica_depth" class="fignos">
<figure>
<img loading="lazy" src="./img/graficas/CB_original_comparativa_depth.png"
alt="Figura 38: Coste de un frame en función de la profundidad de" />
<figcaption aria-hidden="true"><span>Figura 38:</span> Coste de un frame
en función de la profundidad de</figcaption>
</figure>
</div>
<p>En esta figura [<a href="#fig:grafica_depth">38</a>] ocurre algo
similar a [<a href="#fig:grafica_samples">35</a>]: como es evidente,
aumentar la profundidad de un rayo aumenta el coste de renderizar un
frame. Sin embargo, hay algunos matices que debemos estudiar con más
detalle.</p>
<p>El primero es que cambiar la profundidad no es tan costoso como
aumentar el número de muestras. Aún quintuplicando el valor por defecto
de 10 rebotes a 50, vemos que el motor se mantiene por debajo de los 33
milisegundos. Para una profundidad de 10, el coste de un frame es de 19
milisegundos (52 FPS), mientras que para 50 es de 28 milisegundos (35
FPS). Tomando un valor intermedio de 20, el coste se vuelve de 24
milisegundos (41 FPS).</p>
<p>Llaman la atención las variaciones en el frametime conforme aumenta
la profundidad. Para un valor de <code>depth = 10</code>, observamos que
oscila entre los 18 y los 20 milisegundos. Sin embargo, para los otros
dos valores de 20 y 50 son habituales picos de varios frames, llegando
hasta los 5 milisegundos. Además, se aprecia cierta inconsistencia. Sin
embargo, esto no resulta un problema, pues la oscilación media es de
unos 3 milisegundos aproximadamente, lo cual supone un decremento de
unos 5 frames por segundo como máximo.</p>
<p>La naturaleza de la escena afecta en gran medida al resultado. Por
mera probabilidad, cuando un rayo rebota <em>dentro</em> de la caja,
puede salir disparado hacia muchas direcciones. Destacarían en este caso
dos situaciones:</p>
<ul>
<li>El rayo continúa rebotando en la caja, impactando múltiples veces en
las esferas. Esto hace que aumente el coste del camino.</li>
<li>Se escapa de la caja, llegando hasta el infinito y cortando el
camino. En este caso, no se alcanza la profundidad máxima, y el camino
se vuelve más barato.</li>
</ul>
<p>La diferencia de rendimiento es sustancial. Pero, <strong>¿merece la
pena el coste adicional?</strong>.</p>
<p>Para responder a esta pregunta, primero debemos conocer cómo actúa
este parámetro. Empezando con un número extremadamente bajo para los
rebotes, vemos que parte de la escena ni siquiera se renderiza [<a
href="#fig:depth_1">39</a>].</p>
<div id="fig:depth_1" class="fignos">
<figure>
<img loading="lazy" src="./img/05/1%20bounce.png" style="width:70.0%"
alt="Figura 39: depth = 1" />
<figcaption aria-hidden="true"><span>Figura 39:</span>
<code>depth = 1</code></figcaption>
</figure>
</div>
<p>Aumentar el número de rebotes progresivamente permite que el camino
adquiera mayor información. Con dos rebotes, permitimos que un camino
adquiera información sobre la caja por dentro, así como un reflejo
primitivo en las esferas [<a href="#fig:depth_2">40</a>].</p>
<div id="fig:depth_2" class="fignos">
<figure>
<img loading="lazy" src="./img/05/2%20bounces.png" style="width:70.0%"
alt="Figura 40: depth = 2" />
<figcaption aria-hidden="true"><span>Figura 40:</span>
<code>depth = 2</code></figcaption>
</figure>
</div>
<p>Con 3 rebotes, la esfera izquierda refleja casi en su totalidad la
esfera, pero vemos que el reflejo de la esfera derecha <em>dentro</em>
de la izquierda está oscurecido [<a href="#fig:depth_3">41</a>].</p>
<div id="fig:depth_3" class="fignos">
<figure>
<img loading="lazy" src="./img/05/3%20bounces.png" style="width:70.0%"
alt="Figura 41: depth = 3" />
<figcaption aria-hidden="true"><span>Figura 41:</span>
<code>depth = 3</code></figcaption>
</figure>
</div>
<p>Subiéndolo a 4 rebotes [<a href="#fig:depth_4">42</a>] se arregla
mayoritariamente esto.</p>
<div id="fig:depth_4" class="fignos">
<figure>
<img loading="lazy" src="./img/05/4%20bounces.png" style="width:70.0%"
alt="Figura 42: depth = 4" />
<figcaption aria-hidden="true"><span>Figura 42:</span>
<code>depth = 4</code></figcaption>
</figure>
</div>
<p>En esta escena, aumentar más allá de 5 o 6 rebotes produce una
situación de retornos reducidos. La calidad de imagen no aumenta
prácticamente nada, pero el coste se vuelve muy elevado [<a
href="#fig:depth_20">43</a>, <a href="#fig:depth_50">44</a>].</p>
<div id="fig:depth_20" class="fignos">
<figure>
<img loading="lazy" src="./img/05/20%20bounces.png" style="width:70.0%"
alt="Figura 43: depth = 20" />
<figcaption aria-hidden="true"><span>Figura 43:</span>
<code>depth = 20</code></figcaption>
</figure>
</div>
<div id="fig:depth_50" class="fignos">
<figure>
<img loading="lazy" src="./img/05/50%20bounces.png" style="width:70.0%"
alt="Figura 44: depth = 50" />
<figcaption aria-hidden="true"><span>Figura 44:</span>
<code>depth = 50</code></figcaption>
</figure>
</div>
<h3 data-number="6.3.3" id="acumulación-temporal"><span
class="header-section-number">6.3.3</span> Acumulación temporal</h3>
<p>La acumulación temporal proporcionará una mejora enorme de la calidad
visual sin perder rendimiento. Sin embargo, tiene como contrapartida que
necesita dejar la cámara estática. Dependiendo de la situación esto
podría ser un motivo factor no negociable, pero en nuestro caso, nos
servirá.</p>
<p>Utilizando una única muestra, pero un valor de acumulación temporal
de 100 frames máximos, proporciona una imagen sin apenas ruido [<a
href="#fig:acumulacion_temp">45</a>].</p>
<div id="fig:acumulacion_temp" class="fignos">
<figure>
<img loading="lazy" src="./img/05/Acumulación%20temporal%201.png" style="width:40.0%"
alt="Figura 45: 1 muestra, acumulación temporal de 100 frames. A diferencia de 36, el resultado es impecable." />
<figcaption aria-hidden="true"><span>Figura 45:</span> 1 muestra,
acumulación temporal de 100 frames. A diferencia de <a
href="#fig:samples_1">36</a>, el resultado es impecable.</figcaption>
</figure>
</div>
<p>Subiendo los parámetros a 200 frames de acumulación temporal y 10
muestras, se obtiene una imagen muy buena[<a
href="#fig:acumulacion_temp_2">46</a>].</p>
<div id="fig:acumulacion_temp_2" class="fignos">
<figure>
<img loading="lazy" src="./img/05/Acumulación%20temporal%202.png" style="width:40.0%"
alt="Figura 46: 10 muestras, acumulación temporal de 200 frames." />
<figcaption aria-hidden="true"><span>Figura 46:</span> 10 muestras,
acumulación temporal de 200 frames.</figcaption>
</figure>
</div>
<p>El tremendo efecto de esta técnica es debido a que actúa como
normalización entre imágenes. Interpolando linealmente los resultados de
diferentes frames, con el tiempo se conseguirá una foto de lo que se
debe ver realmente, eliminando así el ruido y las luciérnagas.</p>
<h3 data-number="6.3.4" id="resolución"><span
class="header-section-number">6.3.4</span> Resolución</h3>
<p>Como se ha mencionado en la introducción, todas las escenas
anteriores se han renderizado a 720p. Podemos controlar la resolución
interna del motor desde el archivo <code>globals.hpp</code>. Veamos cómo
escala al variarla.</p>
<div id="fig:resolucion" class="fignos">
<figure>
<img loading="lazy" src="./img/graficas/CB_original_comparativa_resolucion.png"
alt="Figura 47: Tiempo de renderización de un frame dependiendo de la resolución" />
<figcaption aria-hidden="true"><span>Figura 47:</span> Tiempo de
renderización de un frame dependiendo de la resolución</figcaption>
</figure>
</div>
<p>A 720p, la escena <code>cornell_box_original</code> corre a 105 FPS
(9.6 ms/frame), mientras que a 1080p, el motor corre a 47FPS (21
ms/frame) y a 1440p, a 28 FPS (36 ms/frame). Como vemos, la resolución
tiene un gran impacto en el rendimiento. El cambio de 720p a 1080p
implica un aumento del 125% en el número de píxeles a dibujar, por lo
que es natural que el coste sea proporcional a esta cantidad. 1440p
tiene 1.7 veces más píxeles que 1080p.</p>
<p>En la práctica, ray tracing no suele utilizar resoluciones internas
tan grandes. Se aplican otro tipo de técnicas para reducir el ruido,
como veremos en el capítulo de estado del arte.</p>
<h3 data-number="6.3.5" id="importance-sampling"><span
class="header-section-number">6.3.5</span> Importance sampling</h3>
<p>El <a href="#muestreo-por-importancia">muestreo por importancia</a>
consiste en tomar una función de densidad proporcional a la función a
integrar para reducir la varianza. Se ha implementado muestreo por
importancia para los materiales difusos. En particular, se ha utilizado
dos estrategias diferentes para escoger direcciones aleatorias, que
pueden observarse en la figura [<a
href="#fig:importance_sample">48</a>].</p>
<div id="fig:importance_sample" class="fignos">
<figure>
<img loading="lazy" src="./img/05/Importance%20sample.png"
alt="Figura 48: Izquierda: hemisphere sampling. Derecha: Cosine-Weighted Hemisphere Sampling.   Escena: cornell_box_original, 10 rebotes, acumulación temporal off, 10 muestras" />
<figcaption aria-hidden="true"><span>Figura 48:</span>
<strong>Izquierda</strong>: <em>hemisphere sampling</em>.
<strong>Derecha</strong>: <em>Cosine-Weighted Hemisphere Sampling</em>.
<br> Escena: <code>cornell_box_original</code>, 10 rebotes, acumulación
temporal off, 10 muestras</figcaption>
</figure>
</div>
<p>Para facilitar la comparativa, desde una posición estática se ha
tomado una foto, y hemos ampliado la caja de la derecha digitalmente. De
esta forma, podemos ver cómo la caja de la derecha contiene una menor
cantidad de ruido, aún manteniendo los mismos parámetros de
renderizado.</p>
<h2 data-number="6.4" id="comparativa-con-in-one-weekend"><span
class="header-section-number">6.4</span> Comparativa con In One
Weekend</h2>
<p>Con el fin de preparar este trabajo, se ha implementado la serie de
libros de P. Shirley: <em>In One Weekend</em> <span class="citation"
data-cites="Shirley2020RTW1">(<a href="#ref-Shirley2020RTW1"
role="doc-biblioref">Shirley 2020a</a>)</span>, <em>The Next Week</em>
<span class="citation" data-cites="Shirley2020RTW2">(<a
href="#ref-Shirley2020RTW2" role="doc-biblioref">Shirley
2020b</a>)</span> y <em>The Rest of your Life</em> <span
class="citation" data-cites="Shirley2020RTW3">(<a
href="#ref-Shirley2020RTW3" role="doc-biblioref">Shirley
2020c</a>)</span>. Teniendo en cuenta que el producto final de esos
libros es un <em>offline renderer</em>, sería interesante compararlo con
nuestro motor que corre en tiempo real.</p>
<p>En esta sección enseñaremos escenas similares, mostraremos cuánto
tarda en renderizar un frame en comparación a nuestro motor, y
estudiaremos las diferencias en la calidad visual.</p>
<h3 data-number="6.4.1"
id="sobre-la-implementación-de-in-one-weekend"><span
class="header-section-number">6.4.1</span> Sobre la implementación de In
One Weekend</h3>
<p>La implementación de los tres libros se encuentra en la carpeta
<code>./RT_in_one_weekend</code> del repositorio. Aunque el proyecto
presenta una gran complejidada por sí mismo, no comentaremos nada en
este trabajo. Sin embargo, comentaremos algunos detalles necesarios para
esta comparativa:</p>
<ul>
<li>La configuración se encuentra principalmente en el archivo
<code>./RT_in_one_weekend/src/main.cpp</code>. Los parámetros que se
pueden ajustar son el número de muestras
(<code>samples_per_pixel</code>), resolución de la imagen
(<code>image_width</code>) y profundidad del rayo
(<code>max_depth</code>).</li>
<li>Las escenas se han implementado en el archivo
<code>./RT_in_one_weekend/src/scenes.hpp</code>.</li>
<li>Para mantener la comparación lo más justa posible, se ha fijado la
profundidad del rayo a 10, y la resolución a 720 x 720.</li>
</ul>
<p>Es importante tener en mente que <strong>In One Weekend no está
optimizado</strong>. No está pensado para ser rápido; sino para ser
didáctico. Es por ello que el procesamiento está <strong>limitado a un
único hilo</strong>, y el renderizado se realiza <strong>únicamente por
CPU</strong>. Las imágenes que genera este motor utilizan el formato
<code>.ppm</code> y han sido reconvertidas a <code>.png</code> para este
trabajo.</p>
<h3 data-number="6.4.2" id="tiempos-de-renderizado"><span
class="header-section-number">6.4.2</span> Tiempos de renderizado</h3>
<p>Se ha implementado una escena específica para esta comparativa,
llamada <code>cornell_box_iow</code>. Es una situación análoga a la
última caja de Cornell del tercer libro. Para sacar las imágenes de In
One Weekend se han utilizado todas las técnicas vistas en los tres
libros, por lo que se espera que la calidad gráfica sea óptima. En
nuestra versión disponemos de prácticamente todos los métodos vistos en
este trabajo, variando diferentes parámetros con el fin de ver
resultados diferentes.</p>
<p>La siguiente tabla muestra una comparativa entre el coste de
renderizar un frame en In One Weekend y en nuestro motor, usando una
profundidad de 10 rebotes y una resolución de 720p:</p>
<table>
<colgroup>
<col style="width: 19%" />
<col style="width: 25%" />
<col style="width: 32%" />
<col style="width: 22%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Número de muestras</strong></th>
<th style="text-align: left;"><strong>In One Weekend</strong>
(ms/frame)</th>
<th style="text-align: left;"><strong>Nuestra implementación</strong>
(ms/frame)</th>
<th style="text-align: left;"><strong>Diferencia</strong>
(ms/frame)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">1</td>
<td style="text-align: left;"><code>1032</code></td>
<td style="text-align: left;"><code>2.6</code></td>
<td style="text-align: left;"><code>-1006</code></td>
</tr>
<tr class="even">
<td style="text-align: left;">5</td>
<td style="text-align: left;"><code>3934</code></td>
<td style="text-align: left;"><code>11</code></td>
<td style="text-align: left;"><code>-3923</code></td>
</tr>
<tr class="odd">
<td style="text-align: left;">10</td>
<td style="text-align: left;"><code>7459</code></td>
<td style="text-align: left;"><code>20.4</code></td>
<td style="text-align: left;"><code>-7255</code></td>
</tr>
<tr class="even">
<td style="text-align: left;">20</td>
<td style="text-align: left;"><code>14516</code></td>
<td style="text-align: left;"><code>39</code></td>
<td style="text-align: left;"><code>-14477</code></td>
</tr>
<tr class="odd">
<td style="text-align: left;">100</td>
<td style="text-align: left;"><code>69573</code></td>
<td style="text-align: left;"><code>~200</code></td>
<td style="text-align: left;"><code>-69373</code></td>
</tr>
<tr class="even">
<td style="text-align: left;">1000</td>
<td style="text-align: left;"><code>688388</code></td>
<td style="text-align: left;"><code>~2000</code></td>
<td style="text-align: left;"><code>-686388</code></td>
</tr>
</tbody>
</table>
<p>Como podemos observar, la diferencia es abismal. En el tiempo que
tarda In One Weekend en producir una imagen con una única muestra,
nuestro motor es capaz de generar una imagen de 500 muestras. Sin
embargo, este resultado es esperable, pues a fin de cuentas, In One
Weekend corre en la CPU con un único hilo, mientras que en nuestro motor
se utilizan todos los recursos posibles.</p>
<p>Ahora bien, debemos hacernos una pregunta: ¿cómo es la calidad
gráfica de cada uno?</p>
<p>Enfocaremos la respuesta desde dos puntos de vista diferentes: en el
primero, nos fijaremos puramente en el número de muestras; y en el
segundo, fijaremos un cierto margen de milisegundos por frame y
comprobaremos el resultado en cada motor.</p>
<h4 data-number="6.4.2.1" id="por-número-de-muestras"><span
class="header-section-number">6.4.2.1</span> Por número de muestras</h4>
<p>Comencemos la comparativa utilizando el número de muestras. Para las
primeras imágenes fijaremos la acumulación temporal a un único frame.
Explicaremos el motivo después.</p>
<p>Con una única muestra, se observa una diferencia enorme entre ambas
versiones [<a href="#fig:comparativa_1s">49</a>]. En nuestra
implementación no se observa prácticamente nada. Solo somos capaces de
distinguir la luz, un poco del reflejo de la caja izquierda y los
<em>caustics</em> causados por la luz del techo. Mientras tanto, en In
One Weekend, la imagen es ruidosa pero definida.</p>
<div id="fig:comparativa_1s" class="fignos">
<figure>
<img loading="lazy" src="./img/05/Comparativa_1s.png" style="width:85.0%"
alt="Figura 49: 1 muestra. Izquierda: In One Weekend. Derecha: nuestro motor" />
<figcaption aria-hidden="true"><span>Figura 49:</span> 1 muestra.
<strong>Izquierda</strong>: In One Weekend. <strong>Derecha</strong>:
nuestro motor</figcaption>
</figure>
</div>
<p>El <strong>motivo de esta diferencia</strong> es la <strong>forma de
muestrear la escena</strong>. In One Weekend implementa muestreo directo
de las fuentes de luz. Para conseguirlo, almacena la posición de la
lámpara del techo, y en cada intersección muestrea un punto aleatorio de
la fuente. En cambio, en nuestro motor, este tipo de fuentes no se
muestrean directamente, sino que debemos contar con el azar para que
aporten radiancia.</p>
<p>Una vez pasamos a 5 muestras [Figura <a
href="#fig:comparativa_5s">50</a>], nuestro motor consigue una imagen
más nítida, similar a la que In One Weekend genera con una muestra. En
cambio, In One Weekend consigue un resultado muy bueno, aunque con
muchas luciérnagas. Este fenómeno no ocurre en nuestra implementación
por el tipo de muestreo.</p>
<div id="fig:comparativa_5s" class="fignos">
<figure>
<img loading="lazy" src="./img/05/Comparativa_5s.png" style="width:85.0%"
alt="Figura 50: 5 muestras. Izquierda: In One Weekend. Derecha: nuestro motor" />
<figcaption aria-hidden="true"><span>Figura 50:</span> 5 muestras.
<strong>Izquierda</strong>: In One Weekend. <strong>Derecha</strong>:
nuestro motor</figcaption>
</figure>
</div>
<p>Con 20 muestras nuestra implementación aún muestra un resultado algo
ruidoso. ¿Se podría hacer algo para mejorarlo?</p>
<div id="fig:comparativa_20s" class="fignos">
<figure>
<img loading="lazy" src="./img/05/Comparativa_20s.png" style="width:85.0%"
alt="Figura 51: 20 muestras. Izquierda: In One Weekend. Derecha: nuestro motor" />
<figcaption aria-hidden="true"><span>Figura 51:</span> 20 muestras.
<strong>Izquierda</strong>: In One Weekend. <strong>Derecha</strong>:
nuestro motor</figcaption>
</figure>
</div>
<p>La respuesta es la acumulación temporal. Aunque, en esencia, la
acumulación temporal es una forma de aumentar el número de muestras con
respecto al tiempo, nuestra implementación utiliza interpolación para
mezclar los colores de los frames. De esta forma, se consigue el efecto
de normalización, lo cual elimina el ruido de la imagen con el tiempo.
De esta forma conseguimos equiparar la imagen de ambas versiones [Figura
<a href="#fig:comparativa_100s">52</a>].</p>
<p>Se puede observar cómo el tipo de ruido es diferente. En In One
Weekend, el ruido se presenta en forma de píxeles blancos, debido a las
luciérnagas generadas por el muestreo directo de la fuente de luz. En
contrapartida, en nuestra implementación el ruido es negro debido a los
rayos que no impactan en ninguna superficie tras rebotar.</p>
<div id="fig:comparativa_100s" class="fignos">
<figure>
<img loading="lazy" src="./img/05/Comparativa_100s.png" style="width:85.0%"
alt="Figura 52: 100 muestras. Izquierda: In One Weekend (100 muestras). Derecha: nuestro motor (7 muestras, 15 frames de acumulación temporal)" />
<figcaption aria-hidden="true"><span>Figura 52:</span> 100 muestras.
<strong>Izquierda</strong>: In One Weekend (100 muestras).
<strong>Derecha</strong>: nuestro motor (7 muestras, 15 frames de
acumulación temporal)</figcaption>
</figure>
</div>
<p>Por último, subiendo el número de muestras a 1000 conseguimos una
imagen muy nítida en ambas implementaciones [Figura <a
href="#fig:comparativa_1000s">53</a>]. Conseguimos apreciar una
diferencia en los bordes de la esfera de la derecha, la cual seguramente
se deba a un fallo en la implementación de la BRDF.</p>
<div id="fig:comparativa_1000s" class="fignos">
<figure>
<img loading="lazy" src="./img/05/Comparativa_1000s.png" style="width:100.0%"
alt="Figura 53: La imagen final, con 1000 muestras para cada versión. Izquierda: In One Weekend (1000 muestras). Derecha: nuestro motor (10 muestras, 100 frames de acumulación temporal)" />
<figcaption aria-hidden="true"><span>Figura 53:</span> La imagen final,
con 1000 muestras para cada versión. <strong>Izquierda</strong>: In One
Weekend (1000 muestras). <strong>Derecha</strong>: nuestro motor (10
muestras, 100 frames de acumulación temporal)</figcaption>
</figure>
</div>
<h4 data-number="6.4.2.2" id="por-presupuesto-de-tiempo"><span
class="header-section-number">6.4.2.2</span> Por presupuesto de
tiempo</h4>
<p>El presupuesto de tiempo (o <em>frame budget</em> en inglés) es la
cantidad de milisegundos que disponemos para renderizar un frame. Este
valor es importante cuando tratamos con aplicaciones en tiempo real. Por
ejemplo, si queremos que nuestro motor corra a 60 imágenes por segundo,
cada frame debe tardar un máximo de 16 milisegundos en ser generado. Una
comparativa interesante sería fijar un valor para el tiempo, y ver qué
calidad de imagen podemos conseguir con ambas implementaciones</p>
<p>Utilizando un presupuesto de 4000 milisegundos, In One Weekend es
capaz de utilizar 5 muestras para la imagen, mientras que nuestro motor
podría utilizar 10 muestras y 19 frames de acumulación temporal [Figura
<a href="#fig:comparativa_4000ms">54</a>].</p>
<p><span class="math display">\[
20.4 \frac{\text{ms}}{\text{10 muestras}} \cdot 19 \text{ frames de
temp. acum.} = 3876 \text{ ms}
\]</span></p>
<div id="fig:comparativa_4000ms" class="fignos">
<figure>
<img loading="lazy" src="./img/05/Comparativa_4000ms.png" style="width:85.0%"
alt="Figura 54: 4000 milisegundos de frame budget. Izquierda: In One Weekend. Derecha: nuestro motor" />
<figcaption aria-hidden="true"><span>Figura 54:</span> 4000 milisegundos
de <em>frame budget</em>. <strong>Izquierda</strong>: In One Weekend.
<strong>Derecha</strong>: nuestro motor</figcaption>
</figure>
</div>
<p>Como podemos observar, nuestra implementación consigue un resultado
abismalmente mejor en el mismo periodo de tiempo.</p>
<p>Usando un valor mucho más alto de 70000 milisegundos, nuestra
implementación se puede permitir utilizar 20 muestras y 1750 frames de
acumulación temporal: <span class="math display">\[
40 \frac{\text{ms}}{\text{20 muestras}} \cdot 1750 \text{ frames de
temp. acum.} = 70000 \text{ ms}
\]</span></p>
<p>Esencialmente, la imagen que genera nuestra implementación es
perfecta. No muestra ni un ápice de ruido; mientras que la de In One
Weeekend presenta un resultado poco nítido [Figura <a
href="#fig:comparativa_70000ms">55</a>].</p>
<div id="fig:comparativa_70000ms" class="fignos">
<figure>
<img loading="lazy" src="./img/05/Comparativa_70000ms.png" style="width:85.0%"
alt="Figura 55: 70000 milisegundos de frame budget. Izquierda: In One Weekend. Derecha: nuestro motor" />
<figcaption aria-hidden="true"><span>Figura 55:</span> 70000
milisegundos de <em>frame budget</em>. <strong>Izquierda</strong>: In
One Weekend. <strong>Derecha</strong>: nuestro motor</figcaption>
</figure>
</div>
<h4 data-number="6.4.2.3" id="conclusiones-de-la-comparativa"><span
class="header-section-number">6.4.2.3</span> Conclusiones de la
comparativa</h4>
<p>Analizando las imágenes proporcionadas por ambos motores, podemos
concluir que In One Weekend consigue un mejor resultado si nos basamos
en el ruido de la imagen por número de muestras. Sin embargo, la rapidez
de nuestro motor permite compensar los problemas de muestreo con un
mayor número de muestras por milisegundo, lo cual le permite superar el
resultado visual que In One Weekend consigue en un cierto periodo de
tiempo.</p>
<p>No obstante, es importante recordar que ambas implementaciones tienen
sus inconvenientes debido a la naturaleza de los respectivos trabajos,
por lo que en ambos casos se podría mejorar el rendimiento. En el caso
de In One Weekend, paralelizando el programa; y en el de nuestro motor,
haciendo más robusto el muestreo directo de fuentes de luz.</p>
<h1 data-number="7" id="conclusiones"><span
class="header-section-number">7</span> Conclusiones</h1>
<p>Al inicio de este trabajo nos propusimos crear un motor de path
tracing en tiempo real. Esta no era una tarea fácil, así que tuvimos que
empezar desde las bases de la informática gráfica.</p>
<p>Primero estudiamos qué es exactamente el <strong>algoritmo path
tracing</strong>, el cual es una forma de crear imágenes virtuales de
entornos tridimensionales. Dado que es un método físicamente realista,
necesitábamos comprender <strong>cómo se comporta la luz</strong>, con
el fin de conocer de qué color pintar un píxel de nuestra imagen.
Esencialmente, entendimos que los fotones emitidos por las fuentes de
iluminación rebotan por los diferentes objetos de un entorno,
adquiriendo partes de sus propiedades en el impacto.</p>
<p>No obstante, imitar a la realidad es una tarea titánica. Las
ecuaciones radiométricas resultan computacionalmente complejas, por lo
que no es posible crear una simulación perfecta de la luz. Por ese
motivo recurrimos a las <strong>técnicas de Monte Carlo</strong>, las
cuales utilizan sucesos aleatorios para estimar la cantidad de luz de un
punto. Estos métodos hicieron viables ciertos cálculos necesarios para
el algoritmo. Sin embargo, su naturaleza inexacta implica que existe un
error de estimación. Esto nos hizo explorar algunas formas de reducir el
ruido generado por las técnicas de integración de Monte Carlo.</p>
<p>Una vez adquirimos el fundamento teórico, <strong>diseñamos un
software</strong> que nos permitiera poner en práctica nuestros
conocimientos. Escogimos la API gráfica Vulkan y un framework de Nvidia
como base para el desarrollo. Debido a la complejidad del algoritmo
tuvimos que construir numerosas abstracciones que aceleraran el proceso
de renderizado, basándonos en tarjetas gráficas modernas. Aprender a
programar en tarjetas gráficas no es sencillo, así que tuvimos que
diseñar unos programas específicos de éstas llamados <em>shaders</em>;
específicamente, los shaders de ray tracing.</p>
<p>Al final, obtuvimos el resultado deseado: un motor de path tracing
capaz de producir imágenes de escenas virtuales.</p>
<div id="fig:pathtracing_showcase" class="fignos">
<figure>
<img loading="lazy" src="./img/06/Showcase.png"
alt="Figura 56: El motor es capaz de producir preciosas imágenes de objetos físicamente realistas que se mueven en tiempo real" />
<figcaption aria-hidden="true"><span>Figura 56:</span> El motor es capaz
de producir preciosas imágenes de objetos físicamente realistas que se
mueven en tiempo real</figcaption>
</figure>
</div>
<p>Como en cualquier otro trabajo, durante el proceso de desarrollo
tuvimos que algunas tomar decisiones técnicas que alteran la calidad del
producto. Por ello, realizamos un <strong>análisis del
rendimiento</strong> basándonos en el <em>frame time</em> –tiempo que
tarda en un frame en renderizarse–, variando los parámetros de path
tracing en el proceso. De esta forma, pudimos comprobar cuánto nos
cuesta sacar imágenes de gran nitidez.</p>
<p>Finalmente, para poner en contexto el motor, comparamos nuestra
implementación con el de otros autores. En este caso, con el path tracer
de Peter Shirley creado en Ray Tracing In One Weekend <span
class="citation" data-cites="Shirley2020RTW1">(<a
href="#ref-Shirley2020RTW1" role="doc-biblioref">Shirley
2020a</a>)</span>. Nos dimos cuenta de que, aunque nuestra versión
genera las muestras de forma más naïve, la rapidez con la que rinde
consigue compensar el ruido de la imagen, produciendo así resultados más
nítidos.</p>
<h2 data-number="7.1" id="posibles-mejoras"><span
class="header-section-number">7.1</span> Posibles mejoras</h2>
<p>Crear un software de este calibre es una tarea de una complejidad
enorme. Los motores de renderización requieren un equipo de desarrollo
de un tamaño considerable, una gran inversión y un esfuerzo constante.
Teniendo en cuenta el contexto del proyecto, en el camino ha sido
necesario tomar decisiones imperfectas. En esta sección exploraremos
algunas posibles mejoras para este trabajo.</p>
<h3 data-number="7.1.1" id="interfaces"><span
class="header-section-number">7.1.1</span> Interfaces</h3>
<p>La parte que más margen de mejora presenta es <strong>la interfaz de
las fuentes de luz</strong>. En su estado actual, únicamente es posible
utilizar dos tipos de fuentes externas a la escena: luces puntuales y
direccionales. Aunque se muestrean de forma directa. Además, solo puede
existir una única fuente de este tipo.</p>
<p>Este diseño propicia un error relacionado con la nitidez de la
imagen. Una de las ideas clave de path tracing es generar muestras de
<em>forma inteligente</em>: dado que calcular caminos es caro, tira
rayos hacia zonas que aporten mucha información. En dichas zonas deben
encontrarse, esencialmente, fuentes de luz. ¡Pero la interfaz <strong>no
conoce dónde se encuentran los materiales emisivos de la
escena</strong>! Eso implica que algunos rayos toman direcciones que no
aportan nada. Esto lo pudimos comprobar empíricamente en la <a
href="#comparativa-con-in-one-weekend">comparativa</a>.</p>
<p>Otro tipo de interfaz que necesita una mejora sustancial es la de
materiales y objetos. En el estado actual del programa, los elementos de
la escena son cargados desde un fichero <code>.obj</code>. Las
propiedades del material son determinadas en el momento del impacto de
un rayo basándonos en los parámetros del archivo de materiales asociado
<code>.mtl</code>. Esta estrategia funciona suficientemente bien en este
caso, pues el ámbito de desarrollo es bastante reducido.</p>
<p>Sin embargo, a la larga sería beneficioso <strong>reestructurar la
carga y almacenamiento de los objetos</strong>. De esta forma, atacamos
el problema anterior relacionado con los materiales emisivos,
conseguiríamos mayor granularidad en los tipos de objetos y podríamos
diseñar estrategias específicas para algunos tipos de materiales (como
separar en diferentes capas de impacto los objetos transparentes). Esto
también nos permitiría añadir nuevos tipos de materiales más fácilmente,
como pueden ser objetos dieléctricos, plásticos, mezclas entre varios
tipos, <em>subsurface scattering</em>…</p>
<p>Estas nuevas interfaces deberían ir acompañadas de una
<strong>refactorización de la clase <code>Engine</code></strong>. El
framework que escogimos, Nvidia nvpro-samples <span class="citation"
data-cites="nvpro-samples">(<a href="#ref-nvpro-samples"
role="doc-biblioref">Nvidia 2022b</a>)</span>, está destinado a ser
didáctico y no eficiente. Por tanto, el software presenta un alto
acoplamiento. Separar en varias clases más reducidas, como una clase
para rasterización y otra para ray tracing, sería esencial. Sin embargo,
Vulkan es una API muy compleja, por lo que requeriría de una gran
cantidad de trabajo.</p>
<h3 data-number="7.1.2" id="nuevas-técnicas-de-reducción-de-ruido"><span
class="header-section-number">7.1.2</span> Nuevas técnicas de reducción
de ruido</h3>
<ul>
<li>Motion blur.</li>
<li>Uso de ruido (blue noise, perlin noise)</li>
<li>Multiple importance sampling</li>
</ul>
<h1 data-number="8" id="el-presente-y-futuro-de-rt"><span
class="header-section-number">8</span> El presente y futuro de RT</h1>
<blockquote>
<p>TODO: de momento, se queda como está. Es un capítulo bastante fácil
de escribir, así que en unos tres días como muchísimo podría estar todo
listo.</p>
</blockquote>
<h2 data-number="8.1" id="denoising"><span
class="header-section-number">8.1</span> Denoising</h2>
<p>https://alain.xyz/blog/ray-tracing-denoising</p>
<h2 data-number="8.2" id="filtering"><span
class="header-section-number">8.2</span> Filtering</h2>
<p>https://alain.xyz/blog/ray-tracing-filtering</p>
<h2 data-number="8.3" id="offline-renderers"><span
class="header-section-number">8.3</span> Offline renderers</h2>
<p>https://www.disneyanimation.com/technology/hyperion/</p>
<h2 data-number="8.4" id="la-industria-del-videojuego"><span
class="header-section-number">8.4</span> La industria del
videojuego</h2>
<h3 data-number="8.4.1" id="ray-tracing-híbrido"><span
class="header-section-number">8.4.1</span> Ray tracing híbrido</h3>
<p>https://www.khronos.org/blog/vulkan-ray-tracing-best-practices-for-hybrid-rendering</p>
<h3 data-number="8.4.2" id="productos-comerciales"><span
class="header-section-number">8.4.2</span> Productos comerciales</h3>
<ul>
<li>Control
<ul>
<li>Híbrido</li>
<li>https://alain.xyz/blog/frame-analysis-control</li>
<li>https://www.youtube.com/watch?v=blbu0g9DAGA</li>
</ul></li>
<li>Minecraft RTX
<ul>
<li>Path tracing</li>
<li>https://alain.xyz/blog/frame-analysis-minecraftrtx</li>
<li>https://www.youtube.com/watch?v=s_eeWr622Ss</li>
<li>https://www.youtube.com/watch?v=TVtSsJf86_Y</li>
</ul></li>
<li>Cyberpunk 2077
<ul>
<li>Híbrido</li>
</ul></li>
<li>Quake II RTX
<ul>
<li>Path tracing</li>
</ul></li>
<li>Doom RTX
<ul>
<li>Path tracing</li>
</ul></li>
<li>Metro Exodus</li>
</ul>
<h3 data-number="8.4.3" id="unreal-engine-5"><span
class="header-section-number">8.4.3</span> Unreal Engine 5</h3>
<p>https://twitter.com/Yurukuyaru/status/1523643949826588674</p>
<h3 data-number="8.4.4" id="la-última-generación-de-consolas"><span
class="header-section-number">8.4.4</span> La última generación de
consolas</h3>
<h2 data-number="8.5" id="posibles-mejoras-del-trabajo"><span
class="header-section-number">8.5</span> Posibles mejoras del
trabajo</h2>
<ul>
<li>Test Driven Development
<ul>
<li><em>White furnace test</em> (01_lights.pdf, p.61)</li>
</ul></li>
<li>Debugging https://alain.xyz/blog/graphics-debugging</li>
<li>Materiales
<ul>
<li>Physically based materials</li>
<li>Diferentes tipos de materiales</li>
<li>Subsurface scattering</li>
</ul></li>
<li>HDR</li>
<li>Cámara
<ul>
<li>Diferentes tipos de cámaras</li>
<li>Focal lenght, depth of field, motion blur</li>
</ul></li>
</ul>
<h4 data-number="8.5.0.1" id="blue-noise"><span
class="header-section-number">8.5.0.1</span> Blue noise</h4>
<ul>
<li>https://blog.demofox.org/2020/05/16/using-blue-noise-for-raytraced-soft-shadows/</li>
<li>https://alain.xyz/blog/ray-tracing-filtering</li>
<li>Minecraft RTX usa blue noise para el sampling:
https://alain.xyz/blog/frame-analysis-minecraftrtx/assets/diffuse-1spp.jpg</li>
</ul>
<h4 data-number="8.5.0.2" id="forced-random-sampling"><span
class="header-section-number">8.5.0.2</span> Forced random sampling</h4>
<p>http://drivenbynostalgia.com/ (ctrl + f -&gt; forced random
sampling)</p>
<h4 data-number="8.5.0.3" id="sampling-importance-resampling"><span
class="header-section-number">8.5.0.3</span> Sampling importance
resampling</h4>
<ul>
<li>https://blog.demofox.org/2022/03/02/sampling-importance-resampling/</li>
<li>https://research.nvidia.com/sites/default/files/pubs/2020-07_Spatiotemporal-reservoir-resampling/ReSTIR.pdf</li>
</ul>
<h4 data-number="8.5.0.4" id="low-discrepancy-sampling"><span
class="header-section-number">8.5.0.4</span> Low discrepancy
sampling</h4>
<hr>
<h2 class="unlisted unnumbered" id="referencias-1">Referencias</h2>

<h1 data-number="9" id="metodología-de-trabajo"><span
class="header-section-number">9</span> Metodología de trabajo</h1>
<p>Cualquier proyecto de una envergadura considerable necesita ser
planificado con antelación. En este capítulo vamos a hablar de cómo se
ha realizado este trabajo: mostraremos las herramientas usadas, los
ciclos de desarrollo, integración entre documentación y path tracer, y
otras influencias que han afectado al producto final.</p>
<h2 data-number="9.1" id="influencias"><span
class="header-section-number">9.1</span> Influencias</h2>
<p>Antes de comenzar con la labor, primero uno se debe hacer una simple
pregunta:</p>
<blockquote>
<p><em>“Y esto, ¿por qué me importa?”</em></p>
</blockquote>
<p>Dar una respuesta contundente a este tipo de cuestiones nunca es
fácil. Sin embargo, sí que puedo proporcionar motivos por los que he
querido escribir sobre ray tracing.</p>
<p>Una de las principales influencias ha sido <span class="citation"
data-cites="digital-foundry">(<a href="#ref-digital-foundry"
role="doc-biblioref">Digital Foundry 2022</a>)</span>. Este grupo de
divulgación se dedica al estudio de las técnicas utilizadas en el mundo
de los videojuegos. El inicio de la era del ray tracing en tiempo real
les llevó a dedicar una serie de vídeos y artículos a esta tecnología, y
a las diferentes maneras en las que se ha implementado. Se puede ver un
ejemplo en <span class="citation" data-cites="digital-foundry-2020">(<a
href="#ref-digital-foundry-2020" role="doc-biblioref">Digital Foundry
2020</a>)</span>.</p>
<iframe width="784" height="441" src="https://www.youtube.com/embed/6bqA8F6B6NQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
<p>Dado que esta área combina tanto informática, matemáticas y una
visión artística, ¿por qué no explorarlo a fondo</p>
<p>Ahora que se ha decidido el tema, es hora de ver cómo atacarlo.</p>
<p>Soy un fiel creyente del aprendizaje mediante el juego. Páginas como
<em>Explorable Explanations</em> <span class="citation"
data-cites="explorable-explanations">(<a
href="#ref-explorable-explanations" role="doc-biblioref">Ncase
2022</a>)</span>, el blog de <span class="citation"
data-cites="ciechanowski">(<a href="#ref-ciechanowski"
role="doc-biblioref">Bartosz Ciechanowski 2022</a>)</span>, el proyecto
<em>The napkin</em> <span class="citation" data-cites="napkin">(<a
href="#ref-napkin" role="doc-biblioref">Evan Chen 2022</a>)</span> o el
divulgador 3Blue1Brown <span class="citation"
data-cites="3blue1brown">(<a href="#ref-3blue1brown"
role="doc-biblioref">Grant Sanderson 2022</a>)</span> repercuten
inevitablemente en la manera en la que te planteas cómo comunicar textos
científicos. Por ello, aunque esto a fin de cuentas es un trabajo de fin
de grado de una carrera, quería ver hasta dónde era capaz de
llevarlo.</p>
<p>Otro punto importante es la <em>manera</em> de escribir. No me gusta
especialmente la escritura formal. Prefiero ser distendido. Por suerte,
parece que el mundo científico se está volviendo más informal <span
class="citation" data-cites="nature-2016">(<a href="#ref-nature-2016"
role="doc-biblioref">Nature 2016</a>)</span>, así que no soy el único
que aprueba esta tendencia. Además, la estructura clásica de un escrito
matemático de “teorema, lema, demostración, corolario” no me agrada
especialmente. He intentado preservar su estructura, pero sin ser tan
explícito. Estos dos puntos, en conjunto, suponen un balance entre
formalidad y distensión difícil de mantener.</p>
<h2 data-number="9.2" id="ciclos-de-desarrollo"><span
class="header-section-number">9.2</span> Ciclos de desarrollo</h2>
<p>Este proyecto está compuesto por 2 grandes pilares: documentación –lo
que estás leyendo, ya sea en PDF o en la web– y software.</p>
<p>La metodología que se ha seguido es, en esencia, una versión de Agile
muy laxa <span class="citation" data-cites="beck2001agile">(<a
href="#ref-beck2001agile" role="doc-biblioref">Beck et al.
2001</a>)</span>.</p>
<p>Para empezar, se implementaron los tres libros de Shirley de la
“serie In One Weekend”: In One Weekend <span class="citation"
data-cites="Shirley2020RTW1">(<a href="#ref-Shirley2020RTW1"
role="doc-biblioref">Shirley 2020a</a>)</span>, The Next Week <span
class="citation" data-cites="Shirley2020RTW2">(<a
href="#ref-Shirley2020RTW2" role="doc-biblioref">Shirley
2020b</a>)</span>, y The Rest of your Life <span class="citation"
data-cites="Shirley2020RTW3">(<a href="#ref-Shirley2020RTW3"
role="doc-biblioref">Shirley 2020c</a>)</span>.</p>
<p>Tras esto, comenzó a <a href="#setup-del-proyecto">desarrollarse</a>
el motor por GPU. Cuando se consiguió una base sólida (que se puede ver
en el issue del repositorio número 25), se empezó a alternar entre
escritura de documentación y desarrollo del software. A fin de cuentas,
no tiene sentido implementar algo que no se conoce.</p>
<p>Para apoyar el desarrollo, se ha utilizado <a
href="#github">Github</a>. Más adelante hablaremos de cómo esta
plataforma ha facilitado el trabajo.</p>
<h2 data-number="9.3" id="presupuesto"><span
class="header-section-number">9.3</span> Presupuesto</h2>
<blockquote>
<p>TODO: ahora mismo, no tengo ni idea de cómo empezar esto.</p>
</blockquote>
<h2 data-number="9.4" id="arquitectura-del-software"><span
class="header-section-number">9.4</span> Arquitectura del software</h2>
<blockquote>
<p>TODO: especificaciones de los requerimientos y metodología de
desarrollo, así como los planos del proyecto que contendrán las
historias de usuario o casos de uso, diagrama conceptual, de iteración,
de diseño, esquema arquitectónico y bocetos de las interfaces de
usuario. Se describirán las estructuras de datos no fundamentales y
algoritmos no triviales</p>
<p>(Odio los diagramas de clases. Me gustaría evitar como sea
incluirlos.)</p>
</blockquote>
<h2 data-number="9.5" id="diseño"><span
class="header-section-number">9.5</span> Diseño</h2>
<p>El diseño juega un papel fundamental en este proyecto. Todos los
elementos visuales han sido escogidos con cuidado, de forma que se
preserve la estética.</p>
<p>Se ha creado <strong>un diseño que preserve el equilibrio entre la
profesionalidad y la distensión</strong>.</p>
<h3 data-number="9.5.1" id="bases-del-diseño"><span
class="header-section-number">9.5.1</span> Bases del diseño</h3>
<p>Para la documentación en versión PDF, usamos como base la
<em>template</em> <span class="citation" data-cites="eisvogel">(<a
href="#ref-eisvogel" role="doc-biblioref">Pascal Wagler
2022</a>)</span>. Esta es una elegante plantilla fácil de usar para
LaTeX. Uno de sus puntos fuertes es la personalización, la cual
aprovecharemos para darle un toque diferente.</p>
<p>La web utiliza como base el estilo generado por Pandoc, el
microframework de CSS <span class="citation" data-cites="bamboo">(<a
href="#ref-bamboo" role="doc-biblioref">Tran Ngoc Tuan Anh
2022</a>)</span> y unas modificaciones personales.</p>
<h3 data-number="9.5.2" id="tipografías"><span
class="header-section-number">9.5.2</span> Tipografías</h3>
<p>Un apartado al que se le debe prestar especial énfasis es a la
combinación de tipografías. A fin de cuentas, esto es un libro; así que
escoger un tipo de letra correcto facilitará al lector comprender los
conceptos. Puede parecer trivial a priori, pero es importante.</p>
<p>Para este trabajo, se han escogido las siguientes tipografías:</p>
<ul>
<li><strong>Crimson Pro</strong>, por <span class="citation"
data-cites="crimson-pro">(<a href="#ref-crimson-pro"
role="doc-biblioref">Jacques Le Bailly 2022</a>)</span>: una tipografía
serif clara, legible y contemporánea. Funciona muy bien en densidades
más bajas, como 11pt. Es ideal para la versión en PDF. Además, liga
estupendamente con Source Sans Pro, utilizada para los títulos en la
plantilla Eisvogel.</li>
<li><strong>Fraunces</strong>, por <span class="citation"
data-cites="fraunces">(<a href="#ref-fraunces"
role="doc-biblioref">Undercase Type 2022</a>)</span>: de lejos, la
fuente más interesante de todo este proyecto. Es una soft-serif <em>old
style</em>, pensada para títulos y similares (lo que se conoce como
<em>display</em>). Es usada en los títulos de la web. Una de sus
propiedades más curiosas es que modifica activamente los glifos
dependiendo del valor del <em>optical size axis</em>, el peso y
similares. Recomiendo echarle un ojo a su repositorio de Github, pues
incluyen detalles sobre la implementación.</li>
<li><strong>Rubik</strong>, por <span class="citation"
data-cites="rubik">(<a href="#ref-rubik" role="doc-biblioref">Hubert and
Fischer 2022</a>)</span>: La elección de Rubik es peculiar. Por sí sola,
no casa con el proyecto. Sin embargo, combinada con Fraunces,
proporcionan un punto de elegancia y familiaridad a la web. Su principal
fuerte es la facilidad para la comprensión lectora en pantallas, algo
que buscamos para la página web.</li>
<li><strong>Julia Mono</strong>, por <span class="citation"
data-cites="julia-mono">(<a href="#ref-julia-mono"
role="doc-biblioref">Cormullion 2022</a>)</span>: monoespaciada, pensada
para computación científica. Llevo usándola bastante tiempo, y combia
bien con Crimson Pro.</li>
<li><strong>Jetbrains Mono</strong>, por <span class="citation"
data-cites="jetbrains-mono">(<a href="#ref-jetbrains-mono"
role="doc-biblioref">Philipp Nurullin 2022</a>)</span>: otra tipografía
monoespaciada open source muy sólida, producida por la compañía
Jetbrains. Se utiliza en la web para los bloques de código.</li>
</ul>
<p>Todas estas fuentes permiten un uso no comercial gratuito.</p>
<blockquote>
<p>TODO: Añadir imagen comparativa con las fuentes</p>
</blockquote>
<h3 data-number="9.5.3" id="paleta-de-colores"><span
class="header-section-number">9.5.3</span> Paleta de colores</h3>
<p>A fin de mantener consistencia, se ha creado una paleta de colores
específica.</p>
<figure>
<img loading="lazy" src="./img/08/Paleta%20de%20colores.png" width="400"
alt="La paleta de colores del proyecto" />
<figcaption aria-hidden="true">La paleta de colores del
proyecto</figcaption>
</figure>
<p>El principal objetivo es <strong>transmitir tranquilidad</strong>,
pero a la misma vez, <strong>profesionalidad</strong>. De nuevo,
buscamos la idea de profesionalidad distendida que ya hemos repetido un
par de veces.</p>
<p>Partiendo del rojo que traía Eisvogel (lo que para nosotros sería el
rojo primario), se han creado el resto. En principio, con 5 tonalidades
diferentes nos basta. Todas ellas vienen acompañadas de sus respectivas
variaciones oscuras, muy oscuras, claras y muy claras. Corresponderían a
los <code>color-100, color-300, color-500, color-700, color-900</code>
que estamos acostumbrados en diseño web. Para la escala de grises, se
han escogido 7 colores en vez de 9. Son más que suficientes para lo que
necesitamos. Puedes encontrar las definiciones en el fichero de estilos,
ubicado en <code>./docs/headers/style.css</code>.</p>
<p>Todos los colores que puedes ver en este documento se han extraído de
la paleta. ¡La consistencia es clave!</p>
<h2 data-number="9.6" id="flujo-de-trabajo-y-herramientas"><span
class="header-section-number">9.6</span> Flujo de trabajo y
herramientas</h2>
<p>Encontrar una herramienta que se adapte a un <em>workflow</em> es
complicado. Aunque hay muchos programas maravilosos, debemos hacerlos
funcionar en conjunto. En este apartado, vamos a describir cuáles son
las que hemos usado.</p>
<p>Principalmente destacan tres de ellas: <strong>Github</strong>,
<strong>Pandoc</strong> y <strong>Figma</strong>. La primera tendrá <a
href="#github">su propia sección</a>, así que hablaremos de las
otras.</p>
<blockquote>
<p>TODO: foto del workflow.</p>
</blockquote>
<h3 data-number="9.6.1" id="pandoc"><span
class="header-section-number">9.6.1</span> Pandoc</h3>
<p><strong>Pandoc</strong> <span class="citation"
data-cites="pandoc">(<a href="#ref-pandoc" role="doc-biblioref">John
MacFarlane 2021</a>)</span> es una estupendísima de conversión de
documentos. Se puede usar para convertir un tipo de archivo a otro. En
este caso, se usa para convertir una serie de ficheros Markdown (los
capítulos) a un fichero HTML (la web) y a PDF. Su punto más fuerte es
que permite escribir LaTeX de forma simplificada, como si se tratara de
<em>sugar syntax</em>. Combina la simplicidad de Markdown y la
correctitud de LaTeX.</p>
<p>Su funcionamiento en este proyecto es el siguiente: Primero, recoge
los capítulos que se encuentra en <code>docs/chapters</code>, usando una
serie de cabeceras en YAML que especifican ciertos parámetros (como
autor, fecha, título, etc.), así como scripts de Lua. Estas caceberas se
encuentran en <code>docs/headers</code>. En particular:</p>
<ol type="1">
<li><code>meta.md</code> recoge los parámetros base del trabajo.</li>
<li><code>pdf.md</code> y <code>web.md</code> contienen algunas
definiciones específicas de sus respectivos formatos. Por ejemplo, el
YAML del PDF asigna las variables disponibles de la plantilla Eisvogel;
mientras que para la web se incluyen las referencias a algunas
bibliotecas de Javascript necesarias o los estilos
(<code>docs/headers/style.css</code>, usando como base Bamboo.</li>
<li><code>math.md</code> contiene las definiciones de LaTeX.</li>
<li>Se utilizan algunos filtros específicos de Lua para simplificar la
escritura. En específico, <code>standard-code.lua</code> formatea
correctamente los bloques de código para la web.</li>
</ol>
<p>Un fichero Makefile (<code>docs/Makefile</code>) contiene varias
órdenes para generar ambos formatos. Tienen varios parámetros
adicionales de por sí, como puede ser la bibliografía
(<code>docs/chapters/bibliography.bib</code>).</p>
<h3 data-number="9.6.2" id="figma"><span
class="header-section-number">9.6.2</span> Figma</h3>
<p><strong>Figma</strong> <span class="citation" data-cites="figma">(<a
href="#ref-figma" role="doc-biblioref">Figma 2022</a>)</span> es otro de
esos programas que te hace preguntarte por qué es gratis. Es una
aplicación en la web usada para diseño gráfico. Es muy potente,
intuitiva, y genera unos resultados muy buenos en poco tiempo. Todos los
diseños de este trabajo se han hecho con esta herramienta.</p>
<figure>
<img loading="lazy" src="./img/08/Figma.png"
alt="Tablón principal del proyecto de Figma, a día 15 de abril de 2022" />
<figcaption aria-hidden="true">Tablón principal del proyecto de Figma, a
día 15 de abril de 2022</figcaption>
</figure>
<p>Una de las características más útiles es poder exportar rápidamente
la imagen. Esto permite hacer cambios rápidos y registrarlos en el
repositorio fácilmente. Además, permite instalar plugins. Uno de ellos
ha resultado especialmente útil: Latex Complete <span class="citation"
data-cites="latex-complete">(<a href="#ref-latex-complete"
role="doc-biblioref">Max Krieger 2022</a>)</span>. Esto nos permite
incrustar código LaTeX en el documento en forma de SVG.</p>
<h3 data-number="9.6.3" id="otros-programas"><span
class="header-section-number">9.6.3</span> Otros programas</h3>
<p>Como es normal, hay muchos otros programas que han intervenido en el
desarrollo. Estos son algunos de ellos:</p>
<ul>
<li>El editor por excelencia VSCode <span class="citation"
data-cites="vscode">(<a href="#ref-vscode"
role="doc-biblioref">Microsoft 2022</a>)</span>. Ha facilitado en gran
medida el desarrollo de la aplicación y la documentación. En particular,
se ha usado una extensión denominada <em>Trigger Task on Save</em> <span
class="citation" data-cites="trigger-task">(<a href="#ref-trigger-task"
role="doc-biblioref">Gruntfuggly 2022</a>)</span> que compila la
documentación HTML automáticamente al guardar un fichero. ¡Muy útil y
rápido!</li>
<li><strong>Vectary</strong> <span class="citation"
data-cites="vectary">(<a href="#ref-vectary"
role="doc-biblioref">Vectary 2022</a>)</span> para hacer los diseños en
3D fácilmente. Permite exportar una escena rápidamente a png para
editarla en Figma.</li>
<li>Como veremos más adelante, la documentación se compila en el
repositorio usando un contenedor de <strong>Docker</strong> <span
class="citation" data-cites="docker">(<a href="#ref-docker"
role="doc-biblioref">Docker 2022</a>)</span></li>
<li>Cualquier proyecto informático debería usar <code>git</code>. Este
no es una excepción.</li>
</ul>
<h2 data-number="9.7" id="github"><span
class="header-section-number">9.7</span> Github</h2>
<p>La página <strong>Github</strong> <span class="citation"
data-cites="github">(<a href="#ref-github" role="doc-biblioref">Github
2022</a>)</span> ha alojado prácticamente todo el contenido del trabajo;
desde el programa, hasta la documentación online. El repositorio se
puede consultar en <code>github.com/Asmilex/Raytracing</code> <span
class="citation" data-cites="asmilex-raytracing-repo">(<a
href="#ref-asmilex-raytracing-repo" role="doc-biblioref">Andrés Millán
2022b</a>)</span>.</p>
<p>Se ha escogido Github en vez de sus competidores por los siguientes
motivos:</p>
<ol type="1">
<li>Llevo usándola toda la carrera. Es mi página de hosting de
repositorios favorita.</li>
<li>Los repositorios de Nvidia se encontraban en Github, por lo que
resulta más fácil sincronizarlos.</li>
<li>La documentación se puede desplegar usando Github Pages.</li>
<li>Las Github Actions son particularmente cómodas y sencillas de
usar.</li>
</ol>
<p>Entremos en detalle en algunos de los puntos anteriores:</p>
<h3 data-number="9.7.1"
id="integración-continua-con-github-actions-y-github-pages"><span
class="header-section-number">9.7.1</span> Integración continua con
Github Actions y Github Pages</h3>
<p>Cuando hablamos de <strong>integración continua</strong>, nos
referimos a ciertos programas que corren en un repositorio y se encargan
de hacer ciertas transformaciones al código, de forma que este se
prepare para su presentación final. En esencia, automatizan algunas
tareas habituales de un desarrollo de software. <span class="citation"
data-cites="jj">(<a href="#ref-jj" role="doc-biblioref">Merelo
2021</a>)</span></p>
<p>En este trabajo lo usaremos para compilar la documentación. De esta
forma, no necesitamos lidiar con “proyecto final”, “proyecto final
definitivo”, “proyecto final final v2”, etc. Simplemente, cuando
registremos un cambio en los ficheros Markdown (lo que se conoce en git
como un <code>commit</code>), y lo subamos a Github (acción de
<code>push</code>), se ejecutará un denominado <code>Action</code> que
operará sobre nuestros archivos.</p>
<p>Tendremos dos tipos de <code>Actions</code>: uno que se encarga de
compilar la web, y otro el PDF. En esencia, operan de la siguiente
manera:</p>
<ol type="1">
<li>Comprueba si se ha modificado algún fichero <code>.md</code> en el
último commit subido. Si no es el caso, para.</li>
<li>Si sí se ha modificado, accede a la carpeta del repositorio y
compila la documentación mediante <code>pandoc</code>.
<ol type="1">
<li>La web se genera en <code>docs/index.html</code>. Publica la web a
Github Pages.</li>
<li>El PDF se crea en <code>docs/TFG.pdf</code></li>
</ol></li>
<li>Commitea los archivos y termina.</li>
</ol>
<figure>
<img loading="lazy" src="./img/08/Github%20Actions.png"
alt="La pestaña de Github Actions permite controlar con facilidad el resultado de un workflow y cuánto tarda en ejecutarse" />
<figcaption aria-hidden="true">La pestaña de Github Actions permite
controlar con facilidad el resultado de un workflow y cuánto tarda en
ejecutarse</figcaption>
</figure>
<p>El workflow de la web corre automáticamente, mientras que para
generar el PDF hace falta activación manual. Aunque no es <em>del
todo</em> correcto almacenar ficheros binarios en un repositorio de git,
no me resulta molesto personalmente. Así que, cuando considero que es el
momento oportuno, lo hago manualmente. Además, también se activa por
cada <em>release</em> que se crea.</p>
<p>Volviendo a la web, Github permite alojar páginas web para un
repositorio. Activando el parámetro correcto en las opciones del
repositorio, y configurándolo debidamente, conseguimos que lea el
archivo <code>index.html</code> generado por el Action y lo despliegue.
Esto es potentísimo: con solo editar una línea de código y subir los
cambios, conseguimos que la web se actualice al instante.</p>
<p>Para generar los archivos nos hace falta una distribución de LaTeX,
Pandoc, y todas las dependencias (como filtros). Como no encontré ningún
contenedor que sirviera mi propósito, decidí crear uno. Se encuentra en
el repositorio de Dockerhub <span class="citation"
data-cites="asmilex-raytracing-docker">(<a
href="#ref-asmilex-raytracing-docker" role="doc-biblioref">Andrés Millán
2022a</a>)</span>. Esta imagen está basada en
<code>dockershelf/latex:full</code> <span class="citation"
data-cites="dockershelf">(<a href="#ref-dockershelf"
role="doc-biblioref">dockershelf 2022</a>)</span>. Por desgracia, es
<em>muy</em> pesada para ser un contenedor. Desafortunadamente, una
instalación de LaTeX ocupa una cantidad de espacio considerable; y para
compilar el PDF necesitamos una muy completa, por lo que debemos lidiar
con este <em>overhead</em>. Puedes encontrar el Dockerfile en
<code>./Dockerfile</code>.</p>
<h3 data-number="9.7.2" id="issues-y-github-projects"><span
class="header-section-number">9.7.2</span> Issues y Github Projects</h3>
<p>Las tareas pendientes se gestionan mediante issues. Cada vez que se
tenga un objetivo particular para el desarrollo, se anota un issue.
Cuando se genere un commit que avance dicha tarea, se etiqueta con el
número correspondiente al issue. De esta forma, todas las confirmaciones
relacionadas con la tarea quedan recogidas en la página web.</p>
<p>Esto permite una gestión muy eficiente de los principales problemas y
objetivos pendientes de la aplicación.</p>
<figure>
<img loading="lazy" src="./img/08/Issues.png"
alt="Pestaña de issues, día 16 de abril de 2022" />
<figcaption aria-hidden="true">Pestaña de issues, día 16 de abril de
2022</figcaption>
</figure>
<p>Los issues se agrupan en <em>milestones</em>, o productos mínimamente
viables. Estos issues suelen estar relacionados con algún apartado
importante del desarrollo.</p>
<figure>
<img loading="lazy" src="./img/08/Milestones.png"
alt="Los milestones agrupan una serie de issues relacionados con un punto clave del desarrollo" />
<figcaption aria-hidden="true">Los <em>milestones</em> agrupan una serie
de issues relacionados con un punto clave del desarrollo</figcaption>
</figure>
<p>De esta forma, podemos ver todo lo que queda pendiente para la fecha
de entrega.</p>
<p>Para añadir mayor granularidad a la gestión de tareas y proporcionar
una vista informativa, se utiliza Github Projects. En esencia, esta
aplicación es un acompañante del repositorio estilo Asana.</p>
<figure>
<img loading="lazy" src="./img/08/Projects.png"
alt="Projects agrupa los issues y les asigna prioridades" />
<figcaption aria-hidden="true">Projects agrupa los issues y les asigna
prioridades</figcaption>
</figure>
<p>Una de las alternativas que se planteó al inicio fue
<strong>Linear</strong> <span class="citation" data-cites="linear">(<a
href="#ref-linear" role="doc-biblioref">Linear 2022</a>)</span>, una
aplicación de gestión de issues similar a Projects. Sin embargo, la
conveniencia de tener Projects integrado en Github supuso un punto a
favor para este gestor. De todas formas, el equipo de desarrollo se
compone de una persona, así que no hace falta complicar excesivamente el
workflow.</p>
<p>El desarrollo general de la documentación no ha seguido este sistema
de issues, pues está sujeta a cambios constantes y cada commit está
marcado con <code>[:notebook:]</code>. No obstante, ciertos problemas
relacionados con ella, como puede ser el formato de entrega, sí que
quedan recogidos como un issue.</p>
<p>Finalmente, cuando se produce un cambio significativo en la
aplicación (como puede ser una refactorización, una implementación
considerablemente más compleja…) se genera una nueva rama. Cuando se ha
cumplido el objetivo, se <em>mergea</em> la rama con la principal
<code>main</code> mediante un <em>pull request</em>. Esto proporciona un
mecanismo de robustez ante cambios complejos.</p>
<h3 data-number="9.7.3" id="estilo-de-commits"><span
class="header-section-number">9.7.3</span> Estilo de commits</h3>
<p>Una de los detalles que has podido apreciar si has entrado al
repositorio es un estilo de commit un tanto inusual. Aunque parece un
detalle de lo más insustancial, añadir emojis a los mensajes de commits
añade un toque particular al repositorio, y permite identificar
rápidamente el tipo de cambio.</p>
<p>Cada uno tiene un significado particular. En esta tabla se recogen
sus significados:</p>
<figure>
<img loading="lazy" src="./img/08/Commits.png"
alt="Los emojis permiten reconocer el objetivo de cada commit. Esta tabla recoge el significado de cada uno" />
<figcaption aria-hidden="true">Los emojis permiten reconocer el objetivo
de cada commit. Esta tabla recoge el significado de cada
uno</figcaption>
</figure>
<h1 data-number="10" id="glosario-de-términos"><span
class="header-section-number">10</span> Glosario de términos</h1>
<blockquote>
<p><em>It’s dangerous to go alone, take this.</em></p>
</blockquote>
<p>Tener en mente <em>todos</em> los conceptos y sus expresiones que
aparecen en un libro como este es prácticamente imposible. Tampoco hay
necesidad de ello, realmente. ¡Vaya desperdicio de cabeza! Por eso, aquí
tienes recopilada una lista con todos los elementos importantes y un
enlace a sus secciones correspondientes.</p>
<h2 data-number="10.1" id="notación"><span
class="header-section-number">10.1</span> Notación</h2>
<table>
<colgroup>
<col style="width: 28%" />
<col style="width: 71%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Concepto</strong></th>
<th style="text-align: left;"><strong>Notación</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Puntos</strong></td>
<td style="text-align: left;">Letras mayúsculas: <span
class="math inline">\(P, Q, \dots\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Escalares</strong></td>
<td style="text-align: left;">Letras minúsculas: <span
class="math inline">\(a, b, c, k, \dots\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Vectores</strong></td>
<td style="text-align: left;">Letras minúsculas en negrita: <span
class="math inline">\(\mathbf{v, w, n}, \dots\)</span>. Si están
normalizados, se les pone gorrito (por ejemplo, <span
class="math inline">\(\hat{\mathbf{n}}\)</span>)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Matrices</strong></td>
<td style="text-align: left;">Letras mayúsculas en negrita: <span
class="math inline">\(\mathbf{M}\)</span>. Por columnas.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Producto escalar</strong></td>
<td style="text-align: left;"><span class="math inline">\(\mathbf{v}
\cdot \mathbf{w}\)</span>. Si es el producto escalar de un vector
consigo mismo, a veces pondremos <span
class="math inline">\(\mathbf{v}^2\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Producto vectorial</strong></td>
<td style="text-align: left;"><span class="math inline">\(\mathbf{v}
\times \mathbf{w}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><a
href="#repaso-de-probabilidad"><strong>Variables
aleatorias</strong></a></td>
<td style="text-align: left;"><span class="math inline">\(X, Y\)</span>.
<span class="math inline">\(\xi\)</span> representa una variable
aleatoria con distribución uniforme en <span class="math inline">\([0,
1)\)</span>.</td>
</tr>
</tbody>
</table>
<h2 data-number="10.2" id="radiometría"><span
class="header-section-number">10.2</span> <a
href="#radiometría">Radiometría</a></h2>
<table>
<colgroup>
<col style="width: 17%" />
<col style="width: 82%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Concepto</strong></th>
<th style="text-align: left;"><strong>Expresiones</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><a href="#ángulos-sólidos"><strong>Ángulo
sólido</strong></a>, derivada [<a href="#eq:d_omega">2</a>]</td>
<td style="text-align: left;"><span
class="math inline">\(\begin{aligned} &amp;\omega = \frac{A}{r^2} \\
&amp; d\omega = \sin\theta\ d\theta\ d\phi\end{aligned}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Hemisferio de direcciones
alrededor de un vector</strong></td>
<td style="text-align: left;"><span
class="math inline">\(H^2(\mathbf{n})\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><a href="#unidades-básicas"><strong>Carga
de energía</strong></a></td>
<td style="text-align: left;"><span class="math inline">\(Q = hf =
\frac{hc}{\lambda}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong><a href="#potencia">Flujo
radiante, potencia</a></strong></td>
<td style="text-align: left;"><span
class="math inline">\(\begin{aligned}&amp; \Phi = \frac{dQ}{dt} \\ &amp;
\Phi = \int_{A}\int_{H^2(\mathbf{n})}{L_o(p, \omega) d\omega^\bot
dA}\end{aligned}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong><a
href="#irradiancia">Irradiancia, radiancia emitida</a></strong></td>
<td style="text-align: left;"><span
class="math inline">\(\begin{aligned} &amp; E = \frac{\Phi}{A} \\ &amp;
E(p) = \frac{d\Phi}{dA} \\ &amp; E(p, \mathbf{n}) = \int_{\Omega}{L_i(p,
\omega) \left\lvert cos\theta \right\rvert d\omega} \\ &amp; E(p,
\mathbf{n}) = \int_{0}^{2\pi}\int_{0}^{\pi/2}{L_i(p, \theta, \phi)
\cos\theta\ \sin\theta\ d\theta\ d\phi} \\ &amp; E(p, \mathbf{n}) =
\int_{A}{L\cos\theta\ \frac{\cos\theta_o}{r^2}dA}
\end{aligned}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong><a
href="#intensidad_radiante">Intensidad radiante</a></strong></td>
<td style="text-align: left;"><span
class="math inline">\(\begin{aligned}I =
\frac{d\Phi}{d\omega}\end{aligned}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong><a
href="#radiancia">Radiancia</a></strong></td>
<td style="text-align: left;"><span
class="math inline">\(\begin{aligned} &amp; L(p, \omega) =
\frac{dE_\omega(p)}{d\omega} \\ &amp; L(p, \omega) = \frac{d^2\Phi(p,
\omega)}{d\omega\ dA^\bot} = \frac{d^2\Phi(p, \omega)}{d\omega\ dA\
\cos\theta} \\ &amp; L^+(p, \omega) = \lim_{t \to 0^+}{L(p +
t\mathbf{n_p}, \omega)} \\ &amp; L^-(p, \omega) = \lim_{t \to 0^-}{L(p +
t\mathbf{n_p}, \omega)} \end{aligned}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong><a href="#radiancia">Radiancia
incidente</a></strong></td>
<td style="text-align: left;"><span
class="math inline">\(\begin{aligned} L_i(p, \omega) = \begin{cases}
L^+(p, -\omega) &amp; \text{si } \omega \cdot \mathbf{n_p} &gt; 0 \\
L^-(p, -\omega) &amp; \text{si } \omega \cdot \mathbf{n_p} &lt; 0
\end{cases}\end{aligned}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong><a href="#radiancia">Radiancia
reflejada, radiancia de salida</a></strong></td>
<td style="text-align: left;"><span
class="math inline">\(\begin{aligned} &amp; L_o(p, \omega) =
\begin{cases} L^+(p, \omega) &amp; \text{si } \omega \cdot \mathbf{n_p}
&gt; 0 \\ L^-(p, \omega) &amp; \text{si } \omega \cdot \mathbf{n_p} &lt;
0 \end{cases} \end{aligned}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong><a href="">Ecuación de
dispersión</a></strong></td>
<td style="text-align: left;"><span
class="math inline">\(\begin{aligned}L_o(p, \omega_o) =
\int_{\mathbb{S}^2}{f(p, \omega_o \leftarrow \omega_i)L_i(p,
\omega_i)\left\lvert \cos\theta_i \right\rvert
d\omega_i}\end{aligned}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong><a
href="#la-función-de-distribución-de-reflectancia-bidireccional-brdf">BRDF</a></strong></td>
<td style="text-align: left;"><span
class="math inline">\(\begin{aligned}&amp; f_r(p, \omega_o \leftarrow
\omega_i) = \frac{dL_o(p, \omega_o)}{dE(p, \omega_i)} = \frac{dL_o(p,
\omega_o)}{L_i(p, \omega_i) \cos\theta_i\
d\omega_i}\end{aligned}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong><a
href="#la-función-de-distribución-de-transmitancia-bidireccional-btdf">BTDF</a></strong></td>
<td style="text-align: left;"><span class="math inline">\(f_t(p,
\omega_o \leftarrow \omega_i)\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong><a
href="#juntando-la-brdf-y-la-btdf">BSDF</a></strong></td>
<td style="text-align: left;"><span class="math inline">\(f(p, \omega_o
\leftarrow \omega_i)\)</span></td>
</tr>
</tbody>
</table>
<h1 class="unnumbered" id="bibliografía">Bibliografía</h1>
<div id="refs" class="references csl-bib-body hanging-indent"
role="doc-bibliography">
<div id="ref-Marrs2021" class="csl-entry" role="doc-biblioentry">
Adam Marrs, Peter Shirley, and I. Wald, eds. 2021. <span>“Ray tracing
gems II.”</span> Apress. 2021. <a
href="http://raytracinggems.com/rtg2">http://raytracinggems.com/rtg2</a>.
</div>
<div id="ref-alain-API" class="csl-entry" role="doc-biblioentry">
Alain Galvan. 2022. <span>“A comparison of modern graphics APIs.”</span>
2022. Accessed May 13, 2022. <a
href="https://alain.xyz/blog/comparison-of-modern-graphics-apis">https://alain.xyz/blog/comparison-of-modern-graphics-apis</a>.
</div>
<div id="ref-asmilex-raytracing-docker" class="csl-entry"
role="doc-biblioentry">
Andrés Millán. 2022a. <span>“Docker del TFG.”</span> 2022. Accessed May
7, 2022. <a
href="https://hub.docker.com/r/asmilex/raytracing">https://hub.docker.com/r/asmilex/raytracing</a>.
</div>
<div id="ref-asmilex-raytracing-repo" class="csl-entry"
role="doc-biblioentry">
———. 2022b. <span>“Repositorio del TFG.”</span> 2022. Accessed May 7,
2022. <a
href="https://github.com/Asmilex/Raytracing">https://github.com/Asmilex/Raytracing</a>.
</div>
<div id="ref-video" class="csl-entry" role="doc-biblioentry">
———. 2022c. <span>“Path tracing showcase.”</span> May 18, 2022. Accessed
May 22, 2022. <a
href="https://www.youtube.com/watch?v=pXrD3K69MqE">https://www.youtube.com/watch?v=pXrD3K69MqE</a>.
</div>
<div id="ref-arneback-2019" class="csl-entry" role="doc-biblioentry">
Arnebäck. 2019. <span>“An explanation of the rendering equation.”</span>
January 10, 2019. Accessed April 9, 2022. <a
href="https://www.youtube.com/watch?v=eo_MTI-d28s">https://www.youtube.com/watch?v=eo_MTI-d28s</a>.
</div>
<div id="ref-ciechanowski" class="csl-entry" role="doc-biblioentry">
Bartosz Ciechanowski. 2022. <span>“Lights and shadows.”</span> 2022.
Accessed May 7, 2022. <a
href="https://ciechanow.ski/lights-and-shadows/">https://ciechanow.ski/lights-and-shadows/</a>.
</div>
<div id="ref-beck2001agile" class="csl-entry" role="doc-biblioentry">
Beck, K., M. Beedle, A. van Bennekum, A. Cockburn, W. Cunningham, M.
Fowler, J. Grenning, et al. 2001. <span>“Manifesto for Agile Software
Development.”</span> Accessed April 20, 2022. <a
href="http://www.agilemanifesto.org/">http://www.agilemanifesto.org/</a>.
</div>
<div id="ref-berkeley-cs184" class="csl-entry" role="doc-biblioentry">
Berkeley cs184. 2022. <span>“Computer graphics and imaging.”</span>
2022. Accessed March 20, 2022. <a
href="https://cs184.eecs.berkeley.edu/sp22">https://cs184.eecs.berkeley.edu/sp22</a>.
</div>
<div id="ref-carlos-path-tracing" class="csl-entry"
role="doc-biblioentry">
Carlos Ureña. 2021. <span>“Realismo e iluminación global.”</span> 2021.
<a href="https://lsi2.ugr.es/curena/">https://lsi2.ugr.es/curena/</a>.
</div>
<div id="ref-caulfield-2020" class="csl-entry" role="doc-biblioentry">
Caulfield, B. 2020. <span>“What’s the difference between ray tracing,
rasterization?”</span> May 22, 2020. Accessed April 22, 2022. <a
href="https://blogs.nvidia.com/blog/2018/03/19/whats-difference-between-ray-tracing-rasterization/">https://blogs.nvidia.com/blog/2018/03/19/whats-difference-between-ray-tracing-rasterization/</a>.
</div>
<div id="ref-julia-mono" class="csl-entry" role="doc-biblioentry">
Cormullion. 2022. <span>“JuliaMono - a monospaced font for scientific
and technical computing.”</span> 2022. Accessed May 7, 2022. <a
href="https://juliamono.netlify.app">https://juliamono.netlify.app</a>.
</div>
<div id="ref-cornell-box-compare" class="csl-entry"
role="doc-biblioentry">
Cornell University. 1998. <span>“Cornell box comparison.”</span> April
30, 1998. Accessed May 16, 2022. <a
href="http://www.graphics.cornell.edu/online/box/compare.html">http://www.graphics.cornell.edu/online/box/compare.html</a>.
</div>
<div id="ref-cornell-box-original" class="csl-entry"
role="doc-biblioentry">
———. 2005. <span>“Photograhic images of the cornell box.”</span>
February 2, 2005. Accessed May 16, 2022. <a
href="http://www.graphics.cornell.edu/online/box/data.html">http://www.graphics.cornell.edu/online/box/data.html</a>.
</div>
<div id="ref-crytek-2020" class="csl-entry" role="doc-biblioentry">
Crytek. 2020. <span>“Crysis remastered brings ray tracing to current-gen
consoles.”</span> September 11, 2020. Accessed April 17, 2022. <a
href="https://www.cryengine.com/news/view/crysis-remastered-brings-ray-tracing-to-current-gen-consoles">https://www.cryengine.com/news/view/crysis-remastered-brings-ray-tracing-to-current-gen-consoles</a>.
</div>
<div id="ref-digital-foundry-2020" class="csl-entry"
role="doc-biblioentry">
Digital Foundry. 2020. <span>“Cyberpunk 2077 PC: What does ray tracing
deliver... And is it worth it?”</span> December 19, 2020. Accessed April
10, 2022. <a
href="https://www.youtube.com/watch?v=6bqA8F6B6NQ">https://www.youtube.com/watch?v=6bqA8F6B6NQ</a>.
</div>
<div id="ref-df-global-illumination" class="csl-entry"
role="doc-biblioentry">
———. 2021. <span>“Tech focus: Global illumination - what it is, how does
it work and why do we need it?”</span> July 24, 2021. Accessed May 20,
2022. <a
href="https://www.youtube.com/watch?v=yEkryaaAsBU">https://www.youtube.com/watch?v=yEkryaaAsBU</a>.
</div>
<div id="ref-digital-foundry" class="csl-entry" role="doc-biblioentry">
———. 2022. <span>“Canal de youtube de digital foundry.”</span> 2022.
Accessed May 7, 2022. <a
href="https://www.youtube.com/user/DigitalFoundry">https://www.youtube.com/user/DigitalFoundry</a>.
</div>
<div id="ref-docker" class="csl-entry" role="doc-biblioentry">
Docker. 2022. <span>“Docker - the world’s most popular container
platform.”</span> 2022. Accessed May 7, 2022. <a
href="https://www.docker.com/">https://www.docker.com/</a>.
</div>
<div id="ref-dockershelf" class="csl-entry" role="doc-biblioentry">
dockershelf. 2022. <span>“Repository for docker images of latex. Test
driven, lightweight and reliable. Rebuilt daily.”</span> 2022. Accessed
May 7, 2022. <a
href="https://hub.docker.com/r/dockershelf/latex">https://hub.docker.com/r/dockershelf/latex</a>.
</div>
<div id="ref-berkeley-mc-lecture" class="csl-entry"
role="doc-biblioentry">
Eric C. Anderson. 1999. <span>“Monte carlo methods and importance
sampling.”</span> 1999. Accessed May 9, 2022. <a
href="https://ib.berkeley.edu/labs/slatkin/eriq/classes/guest_lect/mc_lecture_notes.pdf">https://ib.berkeley.edu/labs/slatkin/eriq/classes/guest_lect/mc_lecture_notes.pdf</a>.
</div>
<div id="ref-robust-monte-carlo" class="csl-entry"
role="doc-biblioentry">
Eric Veach. December 1997. <span>“Robust monte carlo methods for light
transport simulation.”</span> December 1997. Accessed May 9, 2022. <a
href="https://graphics.stanford.edu/papers/veach_thesis/">https://graphics.stanford.edu/papers/veach_thesis/</a>.
</div>
<div id="ref-napkin" class="csl-entry" role="doc-biblioentry">
Evan Chen. 2022. <span>“The napkin project.”</span> 2022. Accessed May
7, 2022. <a
href="https://web.evanchen.cc/napkin.html">https://web.evanchen.cc/napkin.html</a>.
</div>
<div id="ref-pellacini-marschner-2017" class="csl-entry"
role="doc-biblioentry">
Fabio Pellacini, S. M. 2022. <span>“Fundamentals of computer
graphics.”</span> 2022. Accessed April 9, 2022. <a
href="https://pellacini.di.uniroma1.it/teaching/graphics17b/">https://pellacini.di.uniroma1.it/teaching/graphics17b/</a>.
</div>
<div id="ref-figma" class="csl-entry" role="doc-biblioentry">
Figma. 2022. <span>“Figma - a design tool for digital art.”</span> 2022.
Accessed May 7, 2022. <a
href="https://www.figma.com/">https://www.figma.com/</a>.
</div>
<div id="ref-galvin-no-date" class="csl-entry" role="doc-biblioentry">
Galvin. n.d. <span>“Random variables.”</span> Accessed March 20, 2022.
<a
href="https://www3.nd.edu/~dgalvin1/10120/10120_S16/Topic17_8p4_Galvin_class.pdf">https://www3.nd.edu/~dgalvin1/10120/10120_S16/Topic17_8p4_Galvin_class.pdf</a>.
</div>
<div id="ref-github" class="csl-entry" role="doc-biblioentry">
Github. 2022. <span>“Github - where the world builds software.”</span>
2022. Accessed May 7, 2022. <a
href="https://github.com">https://github.com</a>.
</div>
<div id="ref-3blue1brown" class="csl-entry" role="doc-biblioentry">
Grant Sanderson. 2022. <span>“3Blue1Brown.”</span> 2022. Accessed May 7,
2022. <a
href="https://www.3blue1brown.com/">https://www.3blue1brown.com/</a>.
</div>
<div id="ref-trigger-task" class="csl-entry" role="doc-biblioentry">
Gruntfuggly. 2022. <span>“Trigger task on save.”</span> 2022. Accessed
May 7, 2022. <a
href="https://marketplace.visualstudio.com/items?itemName=Gruntfuggly.triggertaskonsave">https://marketplace.visualstudio.com/items?itemName=Gruntfuggly.triggertaskonsave</a>.
</div>
<div id="ref-Haines2019" class="csl-entry" role="doc-biblioentry">
Haines, E., and T. Akenine-Möller, eds. 2019. <span>“Ray tracing
gems.”</span> Apress. 2019. <a
href="http://raytracinggems.com">http://raytracinggems.com</a>.
</div>
<div id="ref-rubik" class="csl-entry" role="doc-biblioentry">
Hubert and Fischer, C., Meir Sadan. 2022. <span>“Rubik
typography.”</span> 2022. Accessed May 7, 2022. <a
href="https://fonts.google.com/specimen/Rubik">https://fonts.google.com/specimen/Rubik</a>.
</div>
<div id="ref-metodos-monte-carlo" class="csl-entry"
role="doc-biblioentry">
Illana, J. I. 2013. <span>“Métodos de monte carlo.”</span> 2013.
Accessed May 8, 2022. <a
href="https://www.ugr.es/~jillana/Docencia/FM/mc.pdf">https://www.ugr.es/~jillana/Docencia/FM/mc.pdf</a>.
</div>
<div id="ref-gamma-correction" class="csl-entry" role="doc-biblioentry">
Íñigo Quílez. 2013. <span>“Outdoors lightning.”</span> 2013. Accessed
May 16, 2022. <a
href="https://iquilezles.org/articles/outdoorslighting/">https://iquilezles.org/articles/outdoorslighting/</a>.
</div>
<div id="ref-intel-arc" class="csl-entry" role="doc-biblioentry">
Intel. 2022. <span>“A new player has entered the game.”</span> 2022.
Accessed May 13, 2022. <a
href="https://www.intel.com/content/www/us/en/products/docs/arc-discrete-graphics/overview.html">https://www.intel.com/content/www/us/en/products/docs/arc-discrete-graphics/overview.html</a>.
</div>
<div id="ref-crimson-pro" class="csl-entry" role="doc-biblioentry">
Jacques Le Bailly. 2022. <span>“Crimson pro typography.”</span> 2022.
Accessed May 7, 2022. <a
href="https://fonts.google.com/specimen/Crimson+Pro">https://fonts.google.com/specimen/Crimson+Pro</a>.
</div>
<div id="ref-cornell-box-glossy" class="csl-entry"
role="doc-biblioentry">
Jensen, H. W. 1996. <span>“Global Illumination Using Photon
Maps.”</span> In <em>Rendering Techniques ’96</em>, edited by Xavier
Pueyo and Peter Schröder, 21–30. Vienna: Springer Vienna.
</div>
<div id="ref-Jensen2001" class="csl-entry" role="doc-biblioentry">
———. 2001. <em>Realistic Image Synthesis Using Photon Mapping</em>. A K
Peters/<span>CRC</span> Press. <a
href="https://doi.org/10.1201/9780429294907">https://doi.org/10.1201/9780429294907</a>.
</div>
<div id="ref-pandoc" class="csl-entry" role="doc-biblioentry">
John MacFarlane. 2021. <span>“Pandoc, a universal document
converter.”</span> 2021. Accessed May 7, 2022. <a
href="https://pandoc.org">https://pandoc.org</a>.
</div>
<div id="ref-quantumfracture-2021" class="csl-entry"
role="doc-biblioentry">
José Luis Crespo. 2021. <span>“Ya, en serio, ¿qué es la luz?”</span>
December 10, 2021. Accessed April 22, 2022. <a
href="https://www.youtube.com/watch?v=DkcEAz09Buo">https://www.youtube.com/watch?v=DkcEAz09Buo</a>.
</div>
<div id="ref-linear" class="csl-entry" role="doc-biblioentry">
Linear. 2022. <span>“Linear - the issue tracking tool you’ll enjoy
using.”</span> 2022. Accessed May 7, 2022. <a
href="https://linear.app/">https://linear.app/</a>.
</div>
<div id="ref-quasi-monte-carlo" class="csl-entry"
role="doc-biblioentry">
Martin Roberts. 2018. <span>“The unreasonable effectiveness of
quasirandom sequences.”</span> 2018. Accessed May 9, 2022. <a
href="http://extremelearning.com.au/unreasonable-effectiveness-of-quasirandom-sequences/">http://extremelearning.com.au/unreasonable-effectiveness-of-quasirandom-sequences/</a>.
</div>
<div id="ref-latex-complete" class="csl-entry" role="doc-biblioentry">
Max Krieger. 2022. <span>“Latex complete - typeset math in your
designs.”</span> 2022. Accessed May 7, 2022. <a
href="https://www.figma.com/community/plugin/793023817364007801/LaTeX-Complete">https://www.figma.com/community/plugin/793023817364007801/LaTeX-Complete</a>.
</div>
<div id="ref-McGuire2018GraphicsCodex" class="csl-entry"
role="doc-biblioentry">
McGuire, M. 2021. <em>The Graphics Codex</em>. 2.17 ed. Casual Effects.
<a href="https://graphicscodex.com">https://graphicscodex.com</a>.
</div>
<div id="ref-jj" class="csl-entry" role="doc-biblioentry">
Merelo. 2021. <span>“Infraestructura virtual.”</span> 2021. Accessed
April 16, 2022. <a
href="http://jj.github.io/IV/documentos/temas/Integracion_continua">http://jj.github.io/IV/documentos/temas/Integracion_continua</a>.
</div>
<div id="ref-vscode" class="csl-entry" role="doc-biblioentry">
Microsoft. 2022. <span>“VSCode - code editing. redefined.”</span> 2022.
Accessed May 7, 2022. <a
href="https://code.visualstudio.com/">https://code.visualstudio.com/</a>.
</div>
<div id="ref-nature-2016" class="csl-entry" role="doc-biblioentry">
Nature. 2016. <span>“Scientific language is becoming more
informal.”</span> 2016. Accessed April 10, 2022. <a
href="https://doi.org/10.1038/539140a">https://doi.org/10.1038/539140a</a>.
</div>
<div id="ref-explorable-explanations" class="csl-entry"
role="doc-biblioentry">
Ncase. 2022. <span>“Explorable explanations.”</span> 2022. Accessed May
7, 2022. <a href="https://explorabl.es/">https://explorabl.es/</a>.
</div>
<div id="ref-nvidia-best-practices" class="csl-entry"
role="doc-biblioentry">
Nvidia. 2020. <span>“Best practices: Using nvidia RTX ray
tracing.”</span> October 10, 2020. Accessed May 13, 2022. <a
href="https://developer.nvidia.com/blog/best-practices-using-nvidia-rtx-ray-tracing/">https://developer.nvidia.com/blog/best-practices-using-nvidia-rtx-ray-tracing/</a>.
</div>
<div id="ref-nvpro-samples-tutorial" class="csl-entry"
role="doc-biblioentry">
———. 2022a. <span>“Nvidia DesignWorks KHR tutorial.”</span> 2022.
Accessed May 13, 2022. <a
href="https://github.com/nvpro-samples/vk_raytracing_tutorial_KHR">https://github.com/nvpro-samples/vk_raytracing_tutorial_KHR</a>.
</div>
<div id="ref-nvpro-samples" class="csl-entry" role="doc-biblioentry">
———. 2022b. <span>“Nvidia DesignWorks samples.”</span> 2022. Accessed
May 13, 2022. <a
href="https://github.com/nvpro-samples">https://github.com/nvpro-samples</a>.
</div>
<div id="ref-overvoorde-2022" class="csl-entry" role="doc-biblioentry">
Overvoorde, A. 2022. <span>“Introduction - vulkan tutorial.”</span>
2022. Accessed April 18, 2022. <a
href="https://vulkan-tutorial.com/">https://vulkan-tutorial.com/</a>.
</div>
<div id="ref-mcbook" class="csl-entry" role="doc-biblioentry">
Owen, A. B. 2013. <em>Monte Carlo Theory, Methods and Examples</em>. <a
href="https://artowen.su.domains/mc/">https://artowen.su.domains/mc/</a>.
</div>
<div id="ref-eisvogel" class="csl-entry" role="doc-biblioentry">
Pascal Wagler. 2022. <span>“Eisvogel pandoc template.”</span> 2022.
Accessed May 7, 2022. <a
href="https://github.com/Wandmalfarbe/pandoc-latex-template">https://github.com/Wandmalfarbe/pandoc-latex-template</a>.
</div>
<div id="ref-PBRT3e" class="csl-entry" role="doc-biblioentry">
Pharr, M., W. Jakob, and G. Humphreys. 2016. <span>“Physically based
rendering: From theory to implementation (3rd ed.).”</span> San
Francisco, CA, USA: Morgan Kaufmann Publishers Inc. November 2016. <a
href="https://www.pbr-book.org/3ed-2018/contents">https://www.pbr-book.org/3ed-2018/contents</a>.
</div>
<div id="ref-jetbrains-mono" class="csl-entry" role="doc-biblioentry">
Philipp Nurullin. 2022. <span>“JetBrains mono, a typeface for
developers.”</span> 2022. Accessed May 7, 2022. <a
href="https://www.jetbrains.com/es-es/lp/mono/">https://www.jetbrains.com/es-es/lp/mono/</a>.
</div>
<div id="ref-xkcd-size" class="csl-entry" role="doc-biblioentry">
Randall Munroe. n.d. <span>“XKCD - angular size.”</span> Accessed May 7,
2022. <a href="https://xkcd.com/1276/">https://xkcd.com/1276/</a>.
</div>
<div id="ref-unknown-author-no-date" class="csl-entry"
role="doc-biblioentry">
<span>“Rendering.”</span> n.d. Accessed March 20, 2022. <a
href="https://sciencebehindpixar.org/pipeline/rendering#:%7E:text=They%20said%20it%20takes%20at,to%20render%20that%20many%20frames">https://sciencebehindpixar.org/pipeline/rendering#:%7E:text=They%20said%20it%20takes%20at,to%20render%20that%20many%20frames</a>.
</div>
<div id="ref-https://doi.org/10.1111/1467-8659.1330233"
class="csl-entry" role="doc-biblioentry">
Schlick, C. 1994. <span>“An Inexpensive BRDF Model for Physically-Based
Rendering.”</span> <em>Computer Graphics Forum</em> 13 (3): 233–46.
https://doi.org/<a
href="https://doi.org/10.1111/1467-8659.1330233">https://doi.org/10.1111/1467-8659.1330233</a>.
</div>
<div id="ref-scratchapixel-2019" class="csl-entry"
role="doc-biblioentry">
Scratchapixel. 2019. <span>“Learn computer graphics from
scratch!”</span> 2019. Accessed April 17, 2022. <a
href="https://www.scratchapixel.com/index.php?redirect">https://www.scratchapixel.com/index.php?redirect</a>.
</div>
<div id="ref-Shirley2020RTW1" class="csl-entry" role="doc-biblioentry">
Shirley, P. 2020a. <span>“Ray tracing in one weekend.”</span> 2020. <a
href="https://raytracing.github.io/books/RayTracingInOneWeekend.html">https://raytracing.github.io/books/RayTracingInOneWeekend.html</a>.
</div>
<div id="ref-Shirley2020RTW2" class="csl-entry" role="doc-biblioentry">
———. 2020b. <span>“Ray tracing: The next week.”</span> 2020. <a
href="https://raytracing.github.io/books/RayTracingTheNextWeek.html">https://raytracing.github.io/books/RayTracingTheNextWeek.html</a>.
</div>
<div id="ref-Shirley2020RTW3" class="csl-entry" role="doc-biblioentry">
———. 2020c. <span>“Ray tracing: The rest of your life.”</span> 2020. <a
href="https://raytracing.github.io/books/RayTracingTheRestOfYourLife.html">https://raytracing.github.io/books/RayTracingTheRestOfYourLife.html</a>.
</div>
<div id="ref-ShirleyRRT" class="csl-entry" role="doc-biblioentry">
Shirley, P., and R. K. Morley. 2003. <em>Realistic Ray Tracing</em>. 2nd
ed. USA: A. K. Peters, Ltd. <a
href="https://www.taylorfrancis.com/books/mono/10.1201/9780429294891/realistic-ray-tracing-peter-shirley-keith-morley">https://www.taylorfrancis.com/books/mono/10.1201/9780429294891/realistic-ray-tracing-peter-shirley-keith-morley</a>.
</div>
<div id="ref-Szirmay-Kalos00monte-carlomethods" class="csl-entry"
role="doc-biblioentry">
Szirmay-Kalos, L. 2000. <span>“Monte-Carlo Methods in Global
Illumination.”</span> <a
href="https://doi.org/10.1.1.36.361">https://doi.org/10.1.1.36.361</a>.
</div>
<div id="ref-vulkan" class="csl-entry" role="doc-biblioentry">
The Khronos Vulkan Working Group. 2022. <span>“Vulkan® 1.2.210 - a
specification (with KHR extensions).”</span> March 29, 2022. Accessed
April 1, 2022. <a
href="- https://www.khronos.org/registry/vulkan/specs/1.2-khr-extensions/html/chap1.html">-
https://www.khronos.org/registry/vulkan/specs/1.2-khr-extensions/html/chap1.html</a>.
</div>
<div id="ref-wikipedia-contributors-2022E" class="csl-entry"
role="doc-biblioentry">
tracing, W. R. 2022. <span>“Ray tracing (graphics).”</span> March 7,
2022. Accessed April 22, 2022. <a
href="https://en.wikipedia.org/wiki/Ray_tracing_(graphics)">https://en.wikipedia.org/wiki/Ray_tracing_(graphics)</a>.
</div>
<div id="ref-bamboo" class="csl-entry" role="doc-biblioentry">
Tran Ngoc Tuan Anh. 2022. <span>“Bamboo CSS.”</span> 2022. Accessed May
7, 2022. <a
href="https://github.com/rilwis/bamboo">https://github.com/rilwis/bamboo</a>.
</div>
<div id="ref-fraunces" class="csl-entry" role="doc-biblioentry">
Undercase Type, F. Z., Phaedra Charles. 2022. <span>“Fraunces
typography.”</span> 2022. Accessed May 7, 2022. <a
href="https://fonts.google.com/specimen/Fraunces">https://fonts.google.com/specimen/Fraunces</a>.
</div>
<div id="ref-proton" class="csl-entry" role="doc-biblioentry">
Valve Software. 2022a. <span>“Proton.”</span> 2022. Accessed May 16,
2022. <a
href="https://github.com/ValveSoftware/Proton">https://github.com/ValveSoftware/Proton</a>.
</div>
<div id="ref-valve" class="csl-entry" role="doc-biblioentry">
———. 2022b. <span>“Valve software.”</span> 2022. Accessed May 16, 2022.
<a
href="https://www.valvesoftware.com/es/">https://www.valvesoftware.com/es/</a>.
</div>
<div id="ref-vectary" class="csl-entry" role="doc-biblioentry">
Vectary. 2022. <span>“Vectary - bringing unlimited creativity to 3D
design.”</span> 2022. Accessed May 7, 2022. <a
href="https://www.vectary.com/">https://www.vectary.com/</a>.
</div>
<div id="ref-disney-brdfs" class="csl-entry" role="doc-biblioentry">
Walt Disney Animation Studios. 2019. <span>“BRDF explorer.”</span> 2019.
Accessed May 7, 2022. <a
href="https://github.com/wdas/brdf">https://github.com/wdas/brdf</a>.
</div>
<div id="ref-directx-12" class="csl-entry" role="doc-biblioentry">
Wikipedia. 2022a. <span>“DirectX raytracing.”</span> 2022. Accessed May
16, 2022. <a
href="https://en.wikipedia.org/wiki/DirectX_Raytracing">https://en.wikipedia.org/wiki/DirectX_Raytracing</a>.
</div>
<div id="ref-wikipedia-nvidia" class="csl-entry" role="doc-biblioentry">
Wikipedia. 2022b. <span>“List of nvidia graphics processing
units.”</span> 2022. Accessed May 13, 2022. <a
href="https://en.wikipedia.org/wiki/List_of_Nvidia_graphics_processing_units">https://en.wikipedia.org/wiki/List_of_Nvidia_graphics_processing_units</a>.
</div>
<div id="ref-optix" class="csl-entry" role="doc-biblioentry">
Wikipedia. 2022c. <span>“OptiX.”</span> 2022. Accessed May 16, 2022. <a
href="https://en.wikipedia.org/wiki/OptiX">https://en.wikipedia.org/wiki/OptiX</a>.
</div>
<div id="ref-wikipedia-radeon" class="csl-entry" role="doc-biblioentry">
Wikipedia. 2022d. <span>“Radeon.”</span> 2022. Accessed May 13, 2022. <a
href="https://en.wikipedia.org/wiki/Radeon">https://en.wikipedia.org/wiki/Radeon</a>.
</div>
<div id="ref-wikipedia-contributors-2022G" class="csl-entry"
role="doc-biblioentry">
Wikipedia: Barycentric coordinate system. 2022. <span>“Barycentric
coordinate system.”</span> March 14, 2022. Accessed April 22, 2022. <a
href="https://en.wikipedia.org/wiki/Barycentric_coordinate_system">https://en.wikipedia.org/wiki/Barycentric_coordinate_system</a>.
</div>
<div id="ref-wikipedia-contributors-2022C" class="csl-entry"
role="doc-biblioentry">
Wikipedia: Computer. 2022. <span>“Computer.”</span> March 13, 2022.
Accessed April 22, 2022. <a
href="https://en.wikipedia.org/wiki/Computer">https://en.wikipedia.org/wiki/Computer</a>.
</div>
<div id="ref-wikipedia-contributors-2022O" class="csl-entry"
role="doc-biblioentry">
Wikipedia: Differential geometry of surfaces. 2022. <span>“Differential
geometry of surfaces.”</span> January 14, 2022. Accessed April 22, 2022.
<a
href="https://en.wikipedia.org/wiki/Differential_geometry_of_surfaces">https://en.wikipedia.org/wiki/Differential_geometry_of_surfaces</a>.
</div>
<div id="ref-wikipedia-contributors-2022A" class="csl-entry"
role="doc-biblioentry">
Wikipedia: history of photography. 2022. <span>“History of
photography.”</span> March 18, 2022. Accessed April 22, 2022. <a
href="https://en.wikipedia.org/wiki/History_of_photography">https://en.wikipedia.org/wiki/History_of_photography</a>.
</div>
<div id="ref-wikipedia-contributors-2022B" class="csl-entry"
role="doc-biblioentry">
Wikipedia: Kodak. 2022. <span>“Kodak.”</span> March 14, 2022. Accessed
April 22, 2022. <a
href="https://es.wikipedia.org/wiki/Kodak">https://es.wikipedia.org/wiki/Kodak</a>.
</div>
<div id="ref-wikipedia-contributors-2022D" class="csl-entry"
role="doc-biblioentry">
Wikipedia: rendering (computer graphics). 2022. <span>“Rendering
(computer graphics).”</span> March 8, 2022. Accessed April 22, 2022. <a
href="https://en.wikipedia.org/wiki/Rendering_(computer_graphics)">https://en.wikipedia.org/wiki/Rendering_(computer_graphics)</a>.
</div>
<div id="ref-shader-binding-table" class="csl-entry"
role="doc-biblioentry">
Will Usher. 2019. <span>“The RTX shader binding table three
ways.”</span> November 20, 2019. Accessed May 13, 2022. <a
href="https://www.willusher.io/graphics/2019/11/20/the-sbt-three-ways">https://www.willusher.io/graphics/2019/11/20/the-sbt-three-ways</a>.
</div>
</div>
<section class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>No entraremos en detalle sobre la
naturaleza de la luz. Sin embargo, si te pica la curiosidad, hay muchos
divulgadores como <span class="citation"
data-cites="quantumfracture-2021">(<a href="#ref-quantumfracture-2021"
role="doc-biblioref">José Luis Crespo 2021</a>)</span> que han tratado
el tema con suficiente profundidad.<a href="#fnref1"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>Recuerda que estamos omitiendo la
longitud de onda <span class="math inline">\(\lambda\)</span>.<a
href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>En su defecto, si tenemos una función
de densidad <span class="math inline">\(f_X\)</span>, podemos hallar la
función de distribución haciendo <span class="math inline">\(F_X(x) =
P[X &lt; x] = \int_{x_{min}}^{x}{f_X(t)dt}\)</span>.<a href="#fnref3"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4" role="doc-endnote"><p>Esto no es del todo cierto. Aunque
generalmente suelen ser excepciones debido al coste computacional de RT
en tiempo real, existen algunas implementaciones que son capaces de
correrlo por software. Notablemente, el motor de Crytek, CryEngine, es
capaz de mover ray tracing basado en hardware y en software <span
class="citation" data-cites="crytek-2020">(<a href="#ref-crytek-2020"
role="doc-biblioref">Crytek 2020</a>)</span><a href="#fnref4"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5" role="doc-endnote"><p>Afortunadamente, esto tampoco es
completamente cierto. La compañía desarrolladora y distribuidora de
videojuegos Valve Corporation <span class="citation"
data-cites="valve">(<a href="#ref-valve" role="doc-biblioref">Valve
Software 2022b</a>)</span> ha creado una pieza de software fascinante:
Proton <span class="citation" data-cites="proton">(<a href="#ref-proton"
role="doc-biblioref">Valve Software 2022a</a>)</span>. Proton utiliza
Wine para emular software en Linux que solo puede correr en plataformas
Windows. La versión 2.5 añadió soporte para traducción de bindings de
DXR a KHR, lo que permite utilizar DirectX12 ray tracing en sistemas
basados en Linux. El motivo de este software es expandir el mercado de
videojuegos disponibles en su consola, la Steam Deck.<a href="#fnref5"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
</body>
</html>
