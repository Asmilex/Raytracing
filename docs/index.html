<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Andrés Millán Muñoz" />
  <meta name="keywords" content="raytracing, ray tracing, Monte
Carlo, Monte Carlo integration, radiometry, path tracing, Vulkan" />
  <title>Los fundamentos de Ray Tracing</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
  <link rel="stylesheet" href="https://unpkg.com/bamboo.css/dist/light.min.css">
  <link rel="stylesheet" href="./headers/style.css">
  <link rel="icon" type="image/x-icon" href="./img/favicon.svg">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.26.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.26.0/plugins/autoloader/prism-autoloader.min.js"></script>
  
  <!-- pandoc-eqnos: equation style -->
  <style>
    .eqnos { display: inline-block; position: relative; width: 100%; }
    .eqnos br { display: none; }
    .eqnos-number { position: absolute; right: 0em; top: 50%; line-height: 0; }
  </style>
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Los fundamentos de Ray Tracing</h1>
<p class="author">Andrés Millán Muñoz</p>
</header>
<nav id="TOC" role="doc-toc">
<h2 id="toc-title">Tabla de contenidos</h2>
<ul>
<li><a href="#abstract">Abstract</a></li>
<li><a href="#a-brief-overview">A brief overview</a></li>
<li><a href="#dedicatoria">Dedicatoria</a></li>
<li><a href="#introducción">Introducción</a>
<ul>
<li><a href="#nota-histórica">Nota histórica</a></li>
<li><a href="#qué-es-ray-tracing">¿Qué es ray tracing?</a></li>
<li><a href="#objetivos-del-trabajo">Objetivos del trabajo</a></li>
<li><a href="#técnicas-empleadas-para-la-resolución">Técnicas empleadas
para la resolución</a></li>
<li><a href="#principales-fuentes-consultadas">Principales fuentes
consultadas</a></li>
</ul></li>
<li><a href="#las-bases">Las bases</a>
<ul>
<li><a href="#eligiendo-direcciones">Eligiendo direcciones</a></li>
<li><a href="#intersecciones-rayo---objeto">Intersecciones rayo -
objeto</a>
<ul>
<li><a href="#superficies-implícitas">Superficies implícitas</a></li>
<li><a href="#superficies-paramétricas">Superficies
paramétricas</a></li>
<li><a href="#intersecciones-con-esferas">Intersecciones con
esferas</a></li>
<li><a href="#intersecciones-con-triángulos">Intersecciones con
triángulos</a></li>
</ul></li>
</ul></li>
<li><a href="#transporte-de-luz">Transporte de luz</a>
<ul>
<li><a href="#introducción-a-la-radiometría">Introducción a la
radiometría</a>
<ul>
<li><a href="#potencia">Potencia</a></li>
<li><a href="#irradiancia">Irradiancia</a></li>
<li><a href="#ángulos-sólidos">Ángulos sólidos</a></li>
<li><a href="#intensidad-radiante">Intensidad radiante</a></li>
<li><a href="#radiancia">Radiancia</a></li>
<li><a href="#integrales-radiométricas">Integrales
radiométricas</a></li>
<li><a href="#fotometría-y-radiometría">Fotometría y
radiometría</a></li>
</ul></li>
<li><a href="#dispersión-de-luz">Dispersión de luz</a>
<ul>
<li><a
href="#la-función-de-distribución-de-reflectancia-bidireccional-brdf">La
función de distribución de reflectancia bidireccional (BRDF)</a></li>
<li><a
href="#la-función-de-distribución-de-transmitancia-bidireccional-btdf">La
función de distribución de transmitancia bidireccional (BTDF)</a></li>
<li><a
href="#la-función-de-distribución-de-dispersión-bidireccional-bsdf">La
función de distribución de dispersión bidireccional (BSDF)</a></li>
<li><a href="#reflectancia-hemisférica">Reflectancia
hemisférica</a></li>
<li><a href="#tipos-de-dispersión">Tipos de dispersión</a></li>
<li><a href="#modelos-analíticos-de-shading">Modelos analíticos de
<em>shading</em></a></li>
<li><a href="#relejos">Relejos</a></li>
<li><a href="#reflejos-especulares-perfectos">Reflejos especulares
perfectos</a></li>
</ul></li>
<li><a href="#la-rendering-equation">La rendering equation</a></li>
</ul></li>
<li><a href="#integración-de-monte-carlo">Integración de Monte Carlo</a>
<ul>
<li><a href="#repaso-de-probabilidad">Repaso de probabilidad</a>
<ul>
<li><a href="#variables-aleatorias-discretas">Variables aleatorias
discretas</a></li>
<li><a href="#variables-aleatorias-continuas">Variables aleatorias
continuas</a></li>
<li><a href="#esperanza-y-varianza-de-una-variable-aleatoria">Esperanza
y varianza de una variable aleatoria</a></li>
<li><a href="#estimadores">Estimadores</a></li>
</ul></li>
<li><a href="#el-estimador-de-monte-carlo">El estimador de Monte
Carlo</a></li>
<li><a href="#importance-sampling">Importance sampling</a></li>
<li><a href="#multiple-importance-sampling">Multiple importance
sampling</a></li>
<li><a href="#escogiendo-puntos-aleatorios">Escogiendo puntos
aleatorios</a>
<ul>
<li><a href="#método-de-la-transformada-inversa">Método de la
transformada inversa</a></li>
<li><a href="#método-del-rechazo">Método del rechazo</a></li>
<li><a href="#distribuciones-unidimensionales">Distribuciones
unidimensionales</a></li>
<li><a href="#distribuciones-bidimensionales">Distribuciones
bidimensionales</a></li>
</ul></li>
<li><a
href="#técnicas-de-reducción-de-varianza-basadas-en-muestras">Técnicas
de reducción de varianza basadas en muestras</a>
<ul>
<li><a href="#ruleta-rusa">Ruleta rusa</a></li>
<li><a href="#next-event-estimation">Next Event Estimation</a></li>
<li><a href="#blue-noise">Blue noise</a></li>
<li><a href="#forced-random-sampling">Forced random sampling</a></li>
<li><a href="#sampling-importance-resampling">Sampling importance
resampling</a></li>
<li><a href="#low-discrepancy-sampling">Low discrepancy
sampling</a></li>
</ul></li>
</ul></li>
<li><a href="#construyamos-un-path-tracer">¡Construyamos un path
tracer!</a>
<ul>
<li><a href="#requisitos-de-ray-tracing-en-tiempo-real">Requisitos de
ray tracing en tiempo real</a>
<ul>
<li><a href="#arquitecturas-de-gráficas">Arquitecturas de
gráficas</a></li>
<li><a href="#frameworks-y-api-de-ray-tracing-en-tiempo-real">Frameworks
y API de ray tracing en tiempo real</a></li>
</ul></li>
<li><a href="#setup-del-proyecto">Setup del proyecto</a>
<ul>
<li><a href="#vistazo-general-a-la-estructura">Vistazo general a la
estructura</a></li>
</ul></li>
<li><a href="#compilación">Compilación</a></li>
<li><a href="#estructuras-de-aceleración">Estructuras de aceleración</a>
<ul>
<li><a href="#botom-level-acceleration-structure-blas">Botom-Level
Acceleration Structure (BLAS)</a></li>
<li><a href="#top-level-acceleration-structure-tlas">Top-Level
Acceleration Structure (TLAS)</a></li>
</ul></li>
<li><a href="#la-ray-tracing-pipeline">La ray tracing pipeline</a>
<ul>
<li><a href="#descriptores-y-conceptos-básicos">Descriptores y conceptos
básicos</a></li>
<li><a href="#la-shader-binding-table">La Shader binding table</a></li>
<li><a href="#tipos-de-shaders">Tipos de shaders</a></li>
<li><a href="#traspaso-de-información-entre-shaders">Traspaso de
información entre shaders</a></li>
<li><a href="#creación-de-la-ray-tracing-pipeline">Creación de la ray
tracing pipeline</a></li>
</ul></li>
<li><a href="#transporte-de-luz-en-la-práctica">Transporte de luz en la
práctica</a>
<ul>
<li><a href="#estimando-la-rendering-equation-con-monte-carlo">Estimando
la rendering equation con Monte Carlo</a></li>
<li><a href="#pseudocódigo-de-un-path-tracer">Pseudocódigo de un path
tracer</a></li>
<li><a
href="#antialiasing-mediante-jittering-y-acumulación-temporal">Antialiasing
mediante jittering y acumulación temporal</a></li>
<li><a href="#materiales-y-objetos">Materiales y objetos</a></li>
</ul></li>
<li><a href="#fuentes-de-luz">Fuentes de luz</a>
<ul>
<li><a href="#point-lights-spotlights">Point lights +
spotlights</a></li>
<li><a href="#fuentes-de-área">Fuentes de área</a></li>
</ul></li>
</ul></li>
<li><a href="#análisis-de-rendimiento">Análisis de rendimiento</a></li>
<li><a href="#el-presente-y-futuro-de-rt">El presente y futuro de RT</a>
<ul>
<li><a href="#denoising">Denoising</a></li>
<li><a href="#filtering">Filtering</a></li>
<li><a href="#offline-renderers">Offline renderers</a></li>
<li><a href="#la-industria-del-videojuego">La industria del
videojuego</a>
<ul>
<li><a href="#ray-tracing-híbrido">Ray tracing híbrido</a></li>
<li><a href="#productos-comerciales">Productos comerciales</a></li>
<li><a href="#unreal-engine-5">Unreal Engine 5</a></li>
<li><a href="#la-última-generación-de-consolas">La última generación de
consolas</a></li>
</ul></li>
<li><a href="#posibles-mejoras-del-trabajo">Posibles mejoras del
trabajo</a></li>
</ul></li>
<li><a href="#metodología-de-trabajo">Metodología de trabajo</a>
<ul>
<li><a href="#influencias">Influencias</a></li>
<li><a href="#ciclos-de-desarrollo">Ciclos de desarrollo</a></li>
<li><a href="#presupuesto">Presupuesto</a></li>
<li><a href="#arquitectura-del-software">Arquitectura del
software</a></li>
<li><a href="#diseño">Diseño</a>
<ul>
<li><a href="#bases-del-diseño">Bases del diseño</a></li>
<li><a href="#tipografías">Tipografías</a></li>
<li><a href="#paleta-de-colores">Paleta de colores</a></li>
</ul></li>
<li><a href="#flujo-de-trabajo-y-herramientas">Flujo de trabajo y
herramientas</a>
<ul>
<li><a href="#pandoc">Pandoc</a></li>
<li><a href="#figma">Figma</a></li>
<li><a href="#otros-programas">Otros programas</a></li>
</ul></li>
<li><a href="#github">Github</a>
<ul>
<li><a
href="#integración-continua-con-github-actions-y-github-pages">Integración
continua con Github Actions y Github Pages</a></li>
<li><a href="#issues-y-github-projects">Issues y Github
Projects</a></li>
<li><a href="#estilo-de-commits">Estilo de commits</a></li>
</ul></li>
</ul></li>
<li><a href="#glosario-de-términos">Glosario de términos</a>
<ul>
<li><a href="#notación">Notación</a></li>
<li><a href="#radiometría"><span>Radiometría</span></a></li>
</ul></li>
<li><a href="#bibliografía">Bibliografía</a></li>
</ul>
</nav>
<h1 class="unnumbered" id="abstract">Abstract</h1>
<p>Este trabajo explorará las técnicas modernas de informática gráfica
físicamente fieles basadas en <em>ray tracing</em> en tiempo real. Para
ello, se usarán métodos de integración de Monte Carlo dado que
disminuyen el tiempo necesario de cómputo.</p>
<p>Para conseguirlo, se ha diseñado un software basado en la interfaz de
programación de aplicaciones gráficas Vulkan, usando como base un
entorno de desarrollo de Nvidia conocido como nvpro-samples. El software
implementa un motor gráfico basado en <em>path tracing</em>. Este motor
será capaz de muestrear fuentes de iluminación de forma directa, lo que
se conoce como <em>next-event estimation</em>. Para disminuir el tiempo
de cómputo y hacerlo viable en tiempo real, se usarán técnicas de Monte
Carlo para integrar radiancia. Se explorarán cómo afectan los diferentes
métodos al ruido final de la imagen.</p>
<p>Este motor se comparará con una implementación puramente en CPU
basada en el software desarrollado en los libros de <span
class="citation" data-cites="Shirley2020RTW1">(<a
href="#ref-Shirley2020RTW1" role="doc-biblioref">Shirley
2020a</a>)</span> “Ray Tracing in One Weekend series”. Se han estudiado
las diferencias de tiempo entre una implementación y otra, sus ventajas
y desventajas y el ruido de las imágenes producidas.</p>
<p><em>Palabras clave: raytracing, ray tracing, Monte Carlo, Monte Carlo
integration, radiometry, path tracing, Vulkan.</em></p>
<hr>
<h1 class="unnumbered" id="a-brief-overview">A brief overview</h1>
<blockquote>
<p>TODO</p>
</blockquote>
<p><em>Keywords: raytracing, ray tracing, Monte Carlo, Monte Carlo
integration, radiometry, path tracing, Vulkan.</em></p>
<h1 class="unnumbered" id="dedicatoria">Dedicatoria</h1>
<p>¡Parece que has llegado un poco pronto! Si lo has hecho
voluntariamente, ¡muchas gracias! Este proyecto debería estar finalizado
en verano de 2022. Mientras tanto, actualizaré poco a poco el contenido.
Si quieres ir comprobando los progresos, puedes visitar <a
href="github.com/Asmilex/Raytracing">Asmilex/Raytracing</a> en Github
para ver el estado del desarrollo.</p>
<p>Aun así, hay mucha gente que me ha ayudado a sacar este proyecto
hacia delante.</p>
<p>Gracias, en primer lugar, a mi familia por permitirme acabar la
carrera. A Cristina, Jorge, Jose OC, Lucas, Mari, Marina y Paula, Sergio
por ayudarme con el contenido, feedback del desarrollo y guía de
diseño.</p>
<h1 id="introducción">Introducción</h1>
<p>Este trabajo puede visualizarse en la web <a
href="https://asmilex.github.io/Raytracing/">asmilex.github.io/Raytracing</a>
o en el <a
href="https://github.com/Asmilex/Raytracing/raw/main/docs/TFG.pdf">PDF</a>
disponible en el repositorio del trabajo <a
href="https://github.com/Asmilex/Raytracing">Asmilex/Raytracing</a>.</p>
<p>La página web contiene la versión más actualizada, además de recursos
adicionales como vídeos.</p>
<h2 id="nota-histórica">Nota histórica</h2>
<p>Ser capaces de capturar un momento.</p>
<p>Desde siempre, este ha sido uno de los sueños de la humanidad. La
capacidad de retener lo que ven nuestros ojos comenzó con simples
pinturas ruprestres. Con el tiempo, el arte evolucionó, así como la
capacidad de retratar nuestra percepción con mayor fidelidad.</p>
<p>A inicios del siglo XVIII, se caputaron las primeras imágenes con una
cámara gracias a Nicéphore Niépce. Sería una imagen primitiva, claro;
pero era funcional. Gracias a la compañía Kodak, la fotografía se
extendió al consumidor rápidamente sobre 1890. Más tarde llegaría la
fotografía digital, la cual simplificaría muchos de los problemas de las
cámaras tradicionales.</p>
<p>Hablando de digital. Los ordenadores personales modernos nacieron
unos años más tarde. Los usuarios eran capaces de mostrar imágenes en
pantalla, que cambiaban bajo demanda. Y, entonces, nos hicimos una
pregunta…</p>
<p>¿Podríamos <strong>simular la vida real</strong> para mostrarla en
pantalla?</p>
<p>Como era de esperar, esto es complicado de lograr. Para conseguirlo,
hemos necesitado crear abstracciones de conceptos que nos resultan
naturales, como objetos, luces y seres vivos. <em>“Cosas”</em> que un
ordenador no entiende, y sin embargo, para nosotros
<em>funcionan</em>.</p>
<p>Así, nació la geometría, los puntos de luces, texturas, sombreados, y
otros elementos de un escenario digital. Pero, por muchas abstracciones
elegantes que tengamos, no nos basta. Necesitamos visualizarlas. Y como
podemos imaginarnos, esto es un proceso costoso.</p>
<p>La <strong>rasterización</strong> es el proceso mediante el cual
estos objetos tridimensionales se transforman en bidimensionales.
Proyectando acordemente el entorno a una cámara, conseguimos colorear un
pixel, de forma que represente lo que se ve en ese mundo.</p>
<blockquote>
<p>TODO insertar imagen rasterización.</p>
<p>NOTE ¿quizás debería extender un poco más esta parte? Parece que se
queda algo coja la explicación.</p>
</blockquote>
<p>Aunque esta técnica es bastante eficiente en términos de computación
y ha evolucionado mucho, rápidamente saturamos sus posibilidades.
Conceptos como <em>shadow maps</em>, <em>baked lightning</em>, o
<em>reflection cubemaps</em> intentan solventar lo que no es posible con
rasterización: preguntrarnos <em>qué es lo que se encuentra alrededor
nuestra</em>.</p>
<p>En parte, nos olvidamos de la intuitiva realidad, para centrarnos en
aquello computacionalmente viable.</p>
<p>Y, entonces, en 1960 el trazado de rayos con una simple idea
intuitiva.</p>
<h2 id="qué-es-ray-tracing">¿Qué es ray tracing?</h2>
<p>En resumidas cuentas, <em>ray tracing</em> (o trazado de rayos en
español), se basa en disparar fotones en forma de rayo desde nuestra
cámara digital y hacerlos rebotar en la escena.</p>
<p>De esta forma, simulamos cómo se comporta la luz. Al impactar en un
objeto, sufre un cambio en su trayectoria. Este cambio origina nuevos
rayos, que vuelven a dispersarse por la escena. Estos nuevos rayos
dependerán de las propiedades del objeto con el que hayan impactado. Con
el tiempo necesario, lo que veremos desde nuestra cámara será una
representación fotorealista de lo que habita en ese universo.</p>
<p>Esta técnica, tan estúpidamente intuitiva, se ha hecho famosa por su
simpleza y su elegancia. <em>Pues claro</em> que la respuesta a
“<em>¿Cómo simulamos fielmente una imagen en un ordenador?</em>” es
“<em>Representando la luz de forma realista</em>”.</p>
<p>Aunque, quizás intuitiva no sea la palabra. Podemos llamarla
<em>natural</em>, eso sí. A fin de cuentas, fue a partir del siglo XVIII
cuando empezamos a entender que podíamos capturar la luz. Nuestros
antepasados tenían teorías, pero no podían explicar por qué
<em>veíamos</em> el mundo.</p>
<p>Ahora sí que sabemos cómo funciona. Entendiendo el por qué lo hace
nos permitirá programarlo. Y, resulta que funciona impresionantemente
bien.</p>
<p>Atrás se quedan los <em>hacks</em> necesarios para rasterización. Los
cubemaps no son esenciales para los reflejos, y no necesitamos cámaras
virtuales para calcular sombras. Ray tracing permite simular fácilmente
efectos como reflejos, refracción, desenfoque de movimiento, aberración
cromática… Incluso fenómenos físicos propios de las particulas y las
ondas.</p>
<blockquote>
<p>Espera. Si tan bueno es, ¿por qué no lo usamos en todos lados?</p>
</blockquote>
<p>Por desgracia, el elefante en la sala es el rendimiento. Como era de
esperar, disparar rayos a diestro y siniestro es costoso. <strong>Muy
costoso</strong>.</p>
<p>A diferencia del universo, nosotros no nos podemos permitir el lujo
de usar fotones de tamaño infinitesimal y dispersiones casi infinitas.
Nos pasaríamos una eternidad esperando. Y para ver una imagen en nuestra
pantalla necesitaremos estar vivos, claro.</p>
<p>Debemos evitar la fuerza bruta. Dado que la idea es tan elegante, la
respuesta no está en el <em>“qué”</em>, sino en el <em>“cómo”</em>. Si
<strong>disparamos y dispersamos rayos con cabeza</strong> seremos
capaces de obtener lo que buscamos en un tiempo razonable.</p>
<p>Hace unos años, al hablar de tiempo razonable, nos referiríamos a
horas. Quizás días. Producir un <em>frame</em> podría suponer una
cantidad de tiempo impensable para un ordenador de consumidor. Hoy en
día también ocurre esto, claro está. Pero la tecnología evoluciona.</p>
<p>Podemos bajarlo a milisegundos.</p>
<p>Hemos entrado en la era del <strong>real time ray
tracing</strong>.</p>
<h2 id="objetivos-del-trabajo">Objetivos del trabajo</h2>
<p>Los objetivos del trabajo iniciales son los siguientes:</p>
<ul>
<li>Análisis de los algoritmos modernos de visualización en 3D basados
en métodos de Monte Carlo.</li>
<li>Revisión de las técnicas de Monte Carlo, examinando puntos fuertes y
débiles de cada una. Se busca minimizar el error en la reconstrucción de
la imagen y minimizar el tiempo de ejecución.</li>
<li>Implementación de dichos algoritmos en hardware gráfico moderno
(GPUs) específicamente diseñado para aceleración de ray tracing.</li>
<li>Diseño e implementación de un software de síntesis de imágenes
realistas por path tracing y muestreo directo de fuentes de luz por
GPU.</li>
<li>Análisis del rendimiento del motor con respecto al tiempo de
ejecución y calidad de imagen.</li>
<li>Comparación del motor desarrollado con una implementación por
CPU.</li>
<li>Investigación de las técnicas modernas y sobre el futuro del
área.</li>
</ul>
<blockquote>
<p>TODO: determinar si lo siguiente es cierto.</p>
</blockquote>
<p>Afortunadamente, <strong>se ha conseguido realizar exitosamente cada
uno de los objetivos</strong>. Esta memoria cubrirá todo el trabajo que
ha sido necesario realizar para lograrlo.</p>
<h2 id="técnicas-empleadas-para-la-resolución">Técnicas empleadas para
la resolución</h2>
<blockquote>
<p>TODO: echarle un ojo a esto cuando termine el trabajo.</p>
</blockquote>
<p>Además del antedicho algoritmo ray tracing y su versión más pura path
tracing, se han empleado técnicas de Monte Carlo para calcular la luz
resultante de un punto.</p>
<p>En particular, con respecto a la <a
href="#integración-de-monte-carlo">matemática</a> empleada, estudiaremos
diferentes formas de generar números aleatorios mediante distribuciones
particulares, <em>(multiple) importance sampling</em>, next event
estimation, …</p>
<p>En un área híbrida se encuentra la <a
href="#transporte-de-luz">radiometría</a>. Dado que estamos tratando con
transporte de luz, será esencial introducir los conceptos más
importantes de la radiometría. Trataremos con algunos términos como
irradiancia, ángulos sólidos, radiancia, funciones de distribuciones de
reflectancia y transmitancia bidireccionales, etc.</p>
<p>Finalmente, la parte <a
href="#construyamos-un-path-tracer">informática</a> usará en la API
gráfica Vulkan junto a un framework de Nvidia para acelerar la adopción
de ray tracing en KHR. Veremos qué se necesita para implementar ray
tracing en tiempo real, lo que nos llevará aprender sobre programación
en Vulkan, las estructuras de aceleración de nivel alto y bajo (TLAS y
BLAS), la Shader Binding Table, comunicación con CPU y GPU, etc.</p>
<p>Todo este programa estará alojado en Github. En el <a
href="#metodología-de-trabajo">apéndice</a>, aprenderemos cómo se ha
usado la plataforma para integrar la documentación, el código fuente y
los ciclos de desarrollo.</p>
<p>Como podemos ver, esta área relaciona íntimamente la matemática y la
informática, con un poco de física de por medio.</p>
<h2 id="principales-fuentes-consultadas">Principales fuentes
consultadas</h2>
<p>Esencialmente, este trabajo ha sido posible gracias a los siguientes
recursos:</p>
<ul>
<li>La serie de libros de <em>Ray Tracing</em> de <em>Peter
Shirley</em>, conocidos como “Ray tracing In One Weekend Series” <span
class="citation" data-cites="Shirley2020RTW1">(<a
href="#ref-Shirley2020RTW1" role="doc-biblioref">Shirley
2020a</a>)</span>, <span class="citation"
data-cites="Shirley2020RTW2">(<a href="#ref-Shirley2020RTW2"
role="doc-biblioref">Shirley 2020b</a>)</span>, <span class="citation"
data-cites="Shirley2020RTW3">(<a href="#ref-Shirley2020RTW3"
role="doc-biblioref">Shirley 2020c</a>)</span>. El motor desarrollado en
estos libros es el que se utilizará para la comparación.</li>
<li>Physically Based Rendering: From Theory to Implementation (3rd ed.)
<span class="citation" data-cites="PBRT3e">(<a href="#ref-PBRT3e"
role="doc-biblioref">Pharr, Jakob, and Humphreys 2016</a>)</span>,
considerado como el santo grial de la informática gráfica moderna.</li>
<li>Ray Tracing Gems I y II <span class="citation"
data-cites="Haines2019">(<a href="#ref-Haines2019"
role="doc-biblioref">Haines and Akenine-Möller 2019</a>)</span>, <span
class="citation" data-cites="Marrs2021">(<a href="#ref-Marrs2021"
role="doc-biblioref">Adam Marrs and Wald 2021</a>)</span>, una colección
de papers esenciales sobre ray tracing publicada por Nvidia.</li>
<li>El autor <a href="https://users.cg.tuwien.ac.at/zsolnai/">Károly
Zsolnai, de Two Minute Papers</a>. No solo ha inspirado parte del
trabajo, sino que su curso sobre transporte de luz de la <a
href="https://www.cg.tuwien.ac.at/courses/Rendering/VU.SS2019.html">universidad
de Austria</a> ha sido una gran fuente de información para el
trabajo.</li>
</ul>
<blockquote>
<p>TODO: tengo que ver exactamente cómo cito esa fuente anterior.</p>
</blockquote>
<hr>
<h2 class="unlisted unnumbered" id="referencias">Referencias</h2>
<p><span class="citation" data-cites="wikipedia-contributors-2022A">(<a
href="#ref-wikipedia-contributors-2022A" role="doc-biblioref">Wikipedia:
history of photography n.d.</a>)</span>, <span class="citation"
data-cites="wikipedia-contributors-2022B">(<a
href="#ref-wikipedia-contributors-2022B" role="doc-biblioref">Wikipedia:
Kodak n.d.</a>)</span>, <span class="citation"
data-cites="wikipedia-contributors-2022C">(<a
href="#ref-wikipedia-contributors-2022C" role="doc-biblioref">Wikipedia:
Computer n.d.</a>)</span>, <span class="citation"
data-cites="wikipedia-contributors-2022D">(<a
href="#ref-wikipedia-contributors-2022D" role="doc-biblioref">Wikipedia:
rendering (computer graphics) n.d.</a>)</span>, <span class="citation"
data-cites="caulfield-2020">(<a href="#ref-caulfield-2020"
role="doc-biblioref">Caulfield n.d.</a>)</span>, <span class="citation"
data-cites="wikipedia-contributors-2022E">(<a
href="#ref-wikipedia-contributors-2022E" role="doc-biblioref">tracing
n.d.</a>)</span>, <span class="citation"
data-cites="unknown-author-no-date">(<a
href="#ref-unknown-author-no-date"
role="doc-biblioref"><span>“Rendering”</span> n.d.</a>)</span>, <span
class="citation" data-cites="Haines2019">(<a href="#ref-Haines2019"
role="doc-biblioref">Haines and Akenine-Möller 2019</a>)</span></p>
<ul>
<li>https://www.cg.tuwien.ac.at/courses/Rendering/VU.SS2019.html</li>
</ul>
<h1 id="las-bases">Las bases</h1>
<p>Empecemos por definir lo que es un rayo.</p>
<p>Un rayo es una función <span class="math inline">\(P(t) = O +
tD\)</span>, donde <span class="math inline">\(O\)</span> es el origin,
<span class="math inline">\(D\)</span> la dirección, y <span
class="math inline">\(t \in \mathbb{R}\)</span>. Podemos considerarlo
una interpolación entre dos puntos en el espacio, donde <span
class="math inline">\(t\)</span> controla la posición en la que nos
encontramos.</p>
<p>Por ejemplo, si <span class="math inline">\(t = 0\)</span>,
obtendremos el origen. Si <span class="math inline">\(t = 1\)</span>,
obtendremos el punto correspondiente a la dirección. Usando valores
negativos vamos <em>hacia atrás</em>.</p>
<figure>
<img src="./img/01/Rayo%20básico.png" data-margin="auto"
alt="El parámetro t nos permite controlar los puntos del rayo" />
<figcaption aria-hidden="true">El parámetro <span
class="math inline">\(t\)</span> nos permite controlar los puntos del
rayo</figcaption>
</figure>
<p>Dado que estos puntos estarán generalmente en <span
class="math inline">\(\mathbb{R}^3\)</span>, podemos escribirlo como</p>
<p><span class="math display">\[
P(t) = (O_x, O_y, O_z) + t (D_x, D_y, D_z)
\]</span></p>
<p>Estos rayos los <em>dispararemos</em> a través de una cámara virtual,
que estará enfocando a la escena. De esta forma, los haremos rebotar con
los objetos que se encuentren en el camino del rayo. A este proceso lo
llamaremos <strong>ray casting</strong>.</p>
<figure>
<img src="./img/01/Ray%20casting.png" alt="Diagrama de ray casting" />
<figcaption aria-hidden="true">Diagrama de ray casting</figcaption>
</figure>
<p>Generalmente, nos quedaremos con el primer objeto que nos encontremos
en su camino. Aunque, a veces, nos interesará saber todos con los que se
encuentre.</p>
<p>Cuando un rayo impacta con un objeto, adquirirá parte de las
propiedades lumínicas del punto de impacto. Por ejemplo, cuánta luz
proporciona la lámpara que tiene encima la esfera de la figura
anterior.</p>
<p>Una vez recojamos la información que nos interese, aplicaremos otro
raycast desde el nuevo punto de impacto, escogiendo una nueva dirección
determinada. Esta dirección dependerá del tipo de material del objeto.
Y, de hecho, algunos serán capaces de invocar varios rayos.</p>
<p>Por ejemplo, los espejos reflejan la luz casi de forma perfecta;
mientras que otros elementos como el agua o el cristal reflejan
<em>y</em> refractan luz, así que necesitaremos generar dos nuevos
raycast.</p>
<p>Usando suficientes rayos obtendremos la imagen de la escena. A este
proceso de <strong>ray casting recursivo</strong> es lo que se conoce
como ray tracing.</p>
<p>Como este proceso puede continuar indefinidamente, tendremos que
controlar la profundidad de la recursión. A mayor profundidad, mayor
calidad de imagen; pero también, mayor tiempo de ejecución.</p>
<h2 id="eligiendo-direcciones">Eligiendo direcciones</h2>
<p>Una de las partes más importantes de ray tracing, y a la que quizás
dedicaremos más tiempo, es a la elección de la dirección.</p>
<p>Hay varios factores que entran en juego a la hora de decidir qué
hacemos cuando impactamos con un nuevo objeto:</p>
<ol type="1">
<li><strong>¿Cómo es la superficie del material?</strong> A mayor
rugosidad, mayor aleatoriedad en la dirección. Por ejemplo, no es lo
mismo el asfalto de una carretera que una lámina de aluminio
impecable.</li>
<li><strong>¿Cómo de fiel es nuestra geometría?</strong></li>
<li><strong>¿Dónde se encuentran las luces en la escena?</strong>
Dependiendo de la posición, nos interesará muestrear la luz con mayor
influencia.</li>
</ol>
<p>Estas cuestiones las exploraremos a fondo en las siguientes
secciones.</p>
<h2 id="intersecciones-rayo---objeto">Intersecciones rayo - objeto</h2>
<p>Como dijimos al principio del capítulo, representaremos un rayo
como</p>
<p><span class="math display">\[
\begin{aligned}
P(t) &amp; = (O_x, O_y, O_z) + t (D_x, D_y, D_z) = \\
&amp; = (O_x + t D_x, O_y + t D_y, O_y + t D_z)
\end{aligned}
\]</span></p>
<p>Por ejemplo, tomando <span class="math inline">\(O = (1, 3, 2), D =
(1, 2, 1)\)</span>:</p>
<ul>
<li>Para <span class="math inline">\(t = 0\)</span>, <span
class="math inline">\(P(t) = (1, 3, 2)\)</span>.</li>
<li>Para <span class="math inline">\(t = 1\)</span>, <span
class="math inline">\(P(t) = (1, 3, 2) + (1, 2, 1) = (2, 5,
3)\)</span>.</li>
</ul>
<p>Nos resultará especialmente útil limitar los valores que puede tomar
<span class="math inline">\(t\)</span>. Restringiremos los posibles
puntos del dominio de forma que <span class="math inline">\(t \in
[t_{min}, t_{max})\)</span>, con <span class="math inline">\(t_{min}
&lt; t_{max}\)</span>. En general, nos interesará separarnos de las
superficies un pequeño pero no despreciable <span
class="math inline">\(\varepsilon\)</span> para evitar errores de
redondeo.</p>
<figure>
<img src="./img/01/Límites%20de%20un%20rayo.png"
alt="Separarnos un poquito del origen evitará errores de coma flotante" />
<figcaption aria-hidden="true">Separarnos un poquito del origen evitará
errores de coma flotante</figcaption>
</figure>
<p>Una de las principales cuestiones que debemos hacernos es saber
cuándo un rayo impacta con una superficie. Lo definiremos
analíticamente.</p>
<h3 id="superficies-implícitas">Superficies implícitas</h3>
<p>Generalmente, cuando hablemos de superficies, nos referiremos
superficies diferenciables <span class="citation"
data-cites="wikipedia-contributors-2022O">(<a
href="#ref-wikipedia-contributors-2022O" role="doc-biblioref">Wikipedia:
Differential geometry of surfaces n.d.</a>)</span>, pues nos interesará
conocer el vector normal en cada punto.</p>
<p>Una superficie implícita es una superficie en un espacio euclidiano
definida como</p>
<p><span class="math display">\[
F(x, y, z) = 0
\]</span></p>
<p>Esta ecuación implícita define una serie de puntos del espacio <span
class="math inline">\(\mathbb{R}^3\)</span> que se encuentran en la
superficie.</p>
<p>Por ejemplo, la esfera se define como <span class="math inline">\(x^2
+ y^2 + z^2 - 1 = 0\)</span>.</p>
<p>Consideremos una superficie <span class="math inline">\(S\)</span> y
un punto regular de ella <span class="math inline">\(P\)</span>; es
decir, un punto tal que el gradiente de <span
class="math inline">\(F\)</span> en <span
class="math inline">\(P\)</span> no es 0. Se define el vector normal
<span class="math inline">\(\mathbf{n}\)</span> a la superficie en ese
punto como</p>
<p><span class="math display">\[
\mathbf{n} = \nabla F(P) = \left( \frac{\partial F(P)}{\partial x},
\frac{\partial F(P)}{\partial y}, \frac{\partial F(P)}{\partial z}\right
)
\]</span></p>
<blockquote>
<p>TODO: dibujo de la normal a una superficie.</p>
</blockquote>
<p>Dado un punto <span class="math inline">\(Q \in
\mathbb{R}^3\)</span>, queremos saber dónde interseca un rayo <span
class="math inline">\(P(t)\)</span>. Es decir, para qué <span
class="math inline">\(t\)</span> se cumple que <span
class="math inline">\(F(P(t)) = 0 \iff F(O + tD) = 0\)</span>.</p>
<p>Consideremos por ejemplo un plano, como en <span class="citation"
data-cites="ShirleyRRT">(<a href="#ref-ShirleyRRT"
role="doc-biblioref">Shirley and Morley 2003</a>)</span>. Para ello, nos
tomamos un punto <span class="math inline">\(Q_0\)</span> del plano y un
vector normal a la superficie <span
class="math inline">\(\mathbf{n}\)</span>.</p>
<p>La ecuación implícita del plano será</p>
<p><span class="math display">\[
F(Q) = (Q - Q_0) \cdot \mathbf{n} = 0
\]</span></p>
<p>Si pinchamos nuestro rayo en la ecuación,</p>
<p><span class="math display">\[
\begin{aligned}
F(P(t)) &amp; = (P(t) - Q_0) \cdot \mathbf{n} \\
        &amp; = (O + tD - Q_0) \cdot \mathbf{n} = 0 \\
\end{aligned}
\]</span></p>
<p>Resolviendo para <span class="math inline">\(t\)</span>, esto se da
si</p>
<p><span class="math display">\[
\begin{aligned}
O \cdot \mathbf{n} + tD \cdot \mathbf{n} - Q_0 \cdot \mathbf{n} &amp; =
0 &amp; \iff \\
tD \cdot \mathbf{n} &amp; = Q_0 \cdot \mathbf{n} - O \cdot \mathbf{n}
&amp; \iff \\
t &amp; = \frac{Q_0 \cdot \mathbf{n} - O \cdot \mathbf{n}}{D \cdot
\mathbf{n}}
\end{aligned}
\]</span></p>
<p>Es decir, hemos obtenido el único valor de <span
class="math inline">\(t\)</span> para el cual el rayo toca la
superficie.</p>
<p>Debemos tener en cuenta el caso para el cual <span
class="math inline">\(D \cdot \mathbf{n} = 0\)</span>. Esto solo se da
si la dirección y el vector normal a la superficie son paralelos.</p>
<blockquote>
<p>TODO: dibujo de dos rayos con un plano: uno corta a la superficie,
mientras que el otro es paralelo.</p>
</blockquote>
<h3 id="superficies-paramétricas">Superficies paramétricas</h3>
<p>Otra forma de definir una superficie en el espacio es mediante un
subconjunto <span class="math inline">\(D \subset \mathbb{R}^2\)</span>
y una serie de funciones, <span class="math inline">\(f, g, h: D
\rightarrow \mathbb{R}^3\)</span>, de forma que</p>
<p><span class="math display">\[
(x, y, z) = \left( f(u, v), g(u, v), h(u, v) \right) \\
\]</span></p>
<blockquote>
<p>En informática gráfica, hacemos algo similar cuando mapeamos una
textura a una superficie. Se conoce como UV mapping</p>
</blockquote>
<p>Demos un par de ejemplos de superficies paramétricas: - El grafo de
una función <span class="math inline">\(f: D \rightarrow
\mathbb{R}^3\)</span>, <span class="math display">\[
G(f) = \left\{(x, y, f(x, y)) \,\middle|\,  (x, y) \in D\right\}
\]</span> define una superficie diferenciable siempre que <span
class="math inline">\(f\)</span> también lo sea. - Usando coordenadas
esféricas <span class="math inline">\((r, \theta, \phi)\)</span>,
podemos parametrizar la esfera como <span class="math inline">\((x, y,
z) = (\cos\phi\sin\theta, \sin\phi\sin\theta, \cos\theta)\)</span></p>
<blockquote>
<p>TODO añadir imagen de coordenadas esféricas. U otro capítulo con
coordenadas.</p>
<p>NOTE: estoy usando (radial, polar, azimuthal). <span
class="math inline">\(\theta\)</span> corresponde con la apertura con
respecto a la vertical</p>
</blockquote>
<p>El vector normal <span class="math inline">\(\mathbf{n}\)</span> a la
superficie en un punto <span class="math inline">\((u, v)\)</span> del
dominio viene dado por</p>
<p><span class="math display">\[
\mathbf{n}(u, v) =
        \left( \frac{\partial f}{\partial u}, \frac{\partial g}{\partial
u}, \frac{\partial h}{\partial u} \right)
                \times
        \left( \frac{\partial f}{\partial v}, \frac{\partial g}{\partial
v}, \frac{\partial h}{\partial v} \right)
\]</span></p>
<p>Encontrar el punto de intersección de una superficie paramétrica con
un rayo es sencillo. Basta con encontrar aquellos puntos <span
class="math inline">\((u, v)\)</span> y <span
class="math inline">\(t\)</span> para los que</p>
<p><span class="math display">\[
\begin{aligned}
O_x + tD_x &amp; = f(u, v) \\
O_y + tD_y &amp; = g(u, v) \\
O_z + tD_z &amp; = h(u, v) \\
\end{aligned}
\]</span></p>
<p>Es posible que el rayo no impacte en ningún punto. En ese caso, el
sistema de ecuaciones no tendría solución. Otra posibilidad es que
intersequen en varios puntos.</p>
<h3 id="intersecciones-con-esferas">Intersecciones con esferas</h3>
<p>Estudiemos ahora cómo intersecan una esfera con nuestro rayo. Una
esfera de centro <span class="math inline">\(C\)</span> y radio <span
class="math inline">\(r\)</span> viene dada por aquellos puntos <span
class="math inline">\(P = (x, y, z)\)</span> que cumplen</p>
<p><span class="math display">\[
(P - C) \cdot (P - C) = r^2
\]</span></p>
<p>Podemos reescribir esta ecuación en términos de sus coordenadas para
obtener</p>
<p><span class="math display">\[
(x - C_x)^2 + (y - C_y)^2 + (z - C_z)^2 = r^2
\]</span></p>
<p>Veamos para qué valores de <span class="math inline">\(t\)</span> de
nuestro rayo se cumple esa ecuación:</p>
<p><span class="math display">\[
\begin{aligned}
(P(t) - C) \cdot (P(t) - C) &amp; = r^2 &amp; \iff \\
(O + tD - C) \cdot (O + tD - C) &amp; = r^2 &amp; \iff \\
\end{aligned}
\]</span></p>
<p>Aplicando las propiedades del producto escalar de la conmutatividad
(<span class="math inline">\(a \cdot b = b \cdot a\)</span>) y la
distributiva (<span class="math inline">\(a \cdot (b + c) = a \cdot b +
a \cdot c\)</span>), podemos escribir</p>
<p><span class="math display">\[
\begin{aligned}
((O - C) + tD) \cdot ((O - C) + tD) &amp; = r^2 &amp; \iff \\
(O - C)^2 + 2 \cdot (O - C) \cdot tD + (tD)^2 &amp; = r^2 &amp; \iff \\
D^2t^2 + 2 D \cdot (O - C)t + (O - C)^2 - r^2 &amp; = 0 &amp; \iff \\
\end{aligned}
\]</span></p>
<p>Así que tenemos una ecuación de segundo grado. Resolviéndola, nos
salen nuestros puntos de intersección:</p>
<p><span class="math display">\[
t = \frac{
    - D \cdot (O - C) \pm \sqrt{(D \cdot (O - C))^2 - 4 (D^2)((O - C)^2
- r^2)}
}{
    2 D^2
}
\]</span></p>
<p>Debemos distinguir tres casos, atiendiendo al valor que toma el
discriminante <span class="math inline">\(\Delta = \small{(D \cdot (O -
C))^2 - 4 (D^2)((O - C)^2 - r^2)}\)</span>:</p>
<ol type="1">
<li>Si <span class="math inline">\(\Delta &lt; 0\)</span>, <span
class="math inline">\(\sqrt{\Delta} \notin \mathbb{R}\)</span>, y el
rayo no impacta con la esfera</li>
<li>Si <span class="math inline">\(\Delta = 0\)</span>, el rayo impacta
en un punto, que toma el valor <span class="math inline">\(t = \frac{-D
\cdot (O - C)}{2 D \cdot D}\)</span>. Digamos que <em>pegaría</em> justo
en el borde.</li>
<li>Si <span class="math inline">\(\Delta &gt; 0\)</span>, existen dos
soluciones. En ese caso, el rayo atraviesa la esfera.</li>
</ol>
<figure>
<img src="./img/01/Intersección%20rayo%20-%20esfera.png"
alt="Puntos de intersección con una esfera." />
<figcaption aria-hidden="true">Puntos de intersección con una
esfera.</figcaption>
</figure>
<p>Para estos dos últimos, si consideramos <span
class="math inline">\(t_0\)</span> cualquier solución válida, el vector
normal resultante viene dado por</p>
<p><span class="math display">\[
\mathbf{n} = 2 (P(t_0) - C)
\]</span></p>
<p>o, normalizando,</p>
<p><span class="math display">\[
\hat{\mathbf{n}} = \frac{(P(t_0) - C)}{r}
\]</span></p>
<h3 id="intersecciones-con-triángulos">Intersecciones con
triángulos</h3>
<p>Este tipo de intersecciones serán las más útiles en nuestro path
tracer. Generalmente, nuestras geometrías estarán compuestas por mallas
de triángulos, así que conocer dónde impacta nuestro rayo será clave.
Empecemos por la base:</p>
<p>Un triángulo viene dado por tres puntos, <span
class="math inline">\(A, B\)</span>, y <span
class="math inline">\(C\)</span>; correspondientes a sus vértices. Para
evitar casos absurdos, supongamos que estos puntos son afinmente
independientes; es decir, que no están alineados.</p>
<h4 id="coordenadas-baricéntricas">Coordenadas baricéntricas</h4>
<p>Podemos describir los puntos contenidos en el plano que forman estos
vertices mediante <strong>coordenadas baricéntricas</strong>. Este
sistema de coordenadas expresa cada punto del plano como una combinación
convexa de los vértices. Es decir, que para cada punto <span
class="math inline">\(P\)</span> del triángulo existen <span
class="math inline">\(\alpha, \beta\)</span> y <span
class="math inline">\(\gamma\)</span> tales que <span
class="math inline">\(\alpha + \beta + \gamma = 1\)</span> y</p>
<p><span class="math display">\[
P = \alpha A + \beta B + \gamma C
\]</span></p>
<blockquote>
<p>TODO: triángulo con coordenadas baricéntricas.</p>
</blockquote>
<p>Debemos destacar que existen dos grados de libertad debido a la
restricción de que las coordenadas sumen 1.</p>
<p>Una propiedad de estas coordenadas que nos puede resultar útil es que
un punto <span class="math inline">\(P\)</span> está contenido en el
triángulo si y solo si <span class="math inline">\(0 &lt; \alpha, \beta,
\gamma &lt; 1\)</span>.</p>
<p>Esta propiedad y la restricción de que sumen 1 nos da una cierta
intuición de cómo funcionan. Podemos ver las coordenadas baricéntricas
como la contribución de los vértices a un punto <span
class="math inline">\(P\)</span>. Por ejemplo, si <span
class="math inline">\(\alpha = 0\)</span>, eso significa que el punto
viene dado por <span class="math inline">\(\beta B + \gamma C\)</span>;
es decir, una combinación lineal de <span
class="math inline">\(B\)</span> y <span
class="math inline">\(C\)</span>. Se encuentra en la recta que
generan.</p>
<p>Por proponer otro ejemplo, si alguna de las coordenadas fuera mayor
que 1, eso significaría que el punto estaría más allá del triángulo.</p>
<blockquote>
<p>TODO: dibujo con explicación de cómo funciona (libreta Shinrin -
Yoku)</p>
</blockquote>
<h4 id="calculando-la-intersección">Calculando la intersección</h4>
<p>Podemos eliminar una de las varibales escribiendo <span
class="math inline">\(\alpha = 1 - \beta - \gamma\)</span>, lo que nos
dice</p>
<p><span class="math display">\[
\begin{aligned}
P &amp; = (1 - \beta - \gamma) A + \beta B + \gamma C \\
  &amp; = A + (B - A) \beta + (C - A) \gamma
\end{aligned}
\]</span></p>
<p>bajo la restricción</p>
<p><span id="eq:beta_gamma" class="eqnos"><span class="math display">\[
\begin{aligned}
\beta + \gamma &amp; &lt; 1 \\
0 &amp; &lt; \beta          \\
0 &amp; &lt; \gamma
\end{aligned}
\]</span><span class="eqnos-number">(1)</span></span> </p>
<p>Un rayo <span class="math inline">\(P(t) = O + tD\)</span> impactará
en un punto del triángulo si se cumple</p>
<p><span class="math display">\[
P(t) = O + tD = A + (B - A) \beta + (C - A) \gamma
\]</span></p>
<p>cumpliendo [<a href="#eq:beta_gamma">1</a>]. Podemos expandir la
ecuación anterior en sus coordenadas para obtener</p>
<p><span class="math display">\[
\begin{aligned}
O_x + tD_x &amp; = A_x + (B_x - A_x) \beta + (C_x - A_x) \gamma \\
O_y + tD_y &amp; = A_y + (B_y - A_y) \beta + (C_y - A_y) \gamma \\
O_z + tD_z &amp; = A_z + (B_z - A_z) \beta + (C_z - A_z) \gamma \\
\end{aligned}
\]</span></p>
<p>Reordenamos:</p>
<p><span class="math display">\[
\begin{aligned}
(A_x - B_x) \beta + (A_x - C_x) \gamma+ tD_x &amp; = A_x - O_x \\
(A_y - B_y) \beta + (A_y - C_y) \gamma+ tD_y &amp; = A_y - O_y \\
(A_z - B_z) \beta + (A_z - C_z) \gamma+ tD_z &amp; = A_z - O_z
\end{aligned}
\]</span></p>
<p>Lo que nos permite escribir el sistema en forma de ecuación:</p>
<p><span class="math display">\[
\begin{pmatrix}
        A_x - B_x &amp; A_x - C_x &amp; D_x \\
        A_y - B_y &amp; A_y - C_y &amp; D_y \\
        A_z - B_z &amp; A_z - C_z &amp; D_z
\end{pmatrix}
\begin{pmatrix}
        \beta \\ \gamma \\ t
\end{pmatrix}
=
\begin{pmatrix}
        A_x - O_x \\ A_y - O_y \\ A_z - O_z
\end{pmatrix}
\]</span></p>
<p>Calcular rápidamente la solución a un sistema de ecuaciones lineales
es un problema habitual. En <span class="citation"
data-cites="ShirleyRRT">(<a href="#ref-ShirleyRRT"
role="doc-biblioref">Shirley and Morley 2003</a>)</span> se utiliza la
regla de Cramer para hacerlo, esperando que el compilador optimice las
variables intermedias creadas. Nosotros no nos tendremos que preocupar
de esto en particular, ya que el punto de impacto lo calculará la GPU
gracias a las herramientras aportadas por KHR <span class="citation"
data-cites="the-khronos-vulkan-working-group-2022">(<a
href="#ref-the-khronos-vulkan-working-group-2022"
role="doc-biblioref">The Khronos® Vulkan Working Group
n.d.</a>)</span>.</p>
<p>Para obtener el vector normal, podemos hacer el producto vectorial de
dos vectores que se encuentren en el plano del triángulo. Como, por
convención, los vértices se guardan en sentido antihorario visto desde
fuera del objeto, entonces</p>
<p><span class="math display">\[
\mathbf{n} = (B - A) \times (C - A)
\]</span></p>
<hr>
<h2 class="unlisted unnumbered" id="referencias-1">Referencias</h2>
<p><span class="citation" data-cites="wikipedia-contributors-2022F">(<a
href="#ref-wikipedia-contributors-2022F" role="doc-biblioref">Wikipedia:
Implicit surface n.d.</a>)</span>, <span class="citation"
data-cites="wikipedia-contributors-2021A">(<a
href="#ref-wikipedia-contributors-2021A" role="doc-biblioref">Wikipedia:
Parametric surface n.d.</a>)</span>, <span class="citation"
data-cites="wikipedia-contributors-2022G">(<a
href="#ref-wikipedia-contributors-2022G" role="doc-biblioref">Wikipedia:
Barycentric coordinate system n.d.</a>)</span>, <span class="citation"
data-cites="wikipedia-contributors-2022O">(<a
href="#ref-wikipedia-contributors-2022O" role="doc-biblioref">Wikipedia:
Differential geometry of surfaces n.d.</a>)</span></p>
<h1 id="transporte-de-luz">Transporte de luz</h1>
<p>En este capítulo estudiaremos las bases de la radiometría. Esta área
de la óptica nos proporcionará una serie de herramientas con las cuales
podremos responder a la pregunta <em>cuánta luz existe en un
punto</em>.</p>
<h2 id="introducción-a-la-radiometría">Introducción a la
radiometría</h2>
<blockquote>
<p><strong>Nota</strong>: cuando usemos un paréntesis tras una ecuación,
dentro denotaremos sus unidades de medida.</p>
</blockquote>
<p>Antes de comenzar a trabajar, necesitamos conocer <em>qué
entendemos</em> por luz. Aunque hay muchas formas de trabajar con ella
(a fin de cuentas, todavía seguimos discutiendo sobre <em>qué es</em>
exactamente la luz <a href="#fn1" class="footnote-ref" id="fnref1"
role="doc-noteref"><sup>1</sup></a>), nosotros nos quedaremos con
algunas pinceladas de la cuántica. Nos será suficiente quedarnos con la
concepción de fotón. Una fuente de iluminación emite una serie de
fotones. Estos fotones tienen<sup><span class="citation"
data-cites="ShirleyRRT">(<a href="#ref-ShirleyRRT"
role="doc-biblioref">Shirley and Morley 2003</a>)</span></sup> una
posición, una dirección de propagación y una longitud de onda <span
class="math inline">\(\lambda\)</span>. Un fotón también tiene asociado
una velocidad <span class="math inline">\(c\)</span> que depende del
índice de refracción del medio, <span
class="math inline">\(n\)</span>.</p>
<p>La unidad de medida de <span class="math inline">\(\lambda\)</span>
es el nanómetro (<span class="math inline">\(\text{nm}\)</span>).
También nos vendrá bien definir una frecuencia, <span
class="math inline">\(f\)</span>. Su utilidad viene del hecho de que,
cuando la luz cambia de medio al propagarse, la frecuencia se mantiene
constante.</p>
<p><span class="math display">\[
f = \frac{c}{\lambda}
\]</span></p>
<p>Un fotón tiene asociada una carga de energía, denotada por <span
class="math inline">\(Q\)</span>:</p>
<p><span class="math display">\[
Q = hf = \frac{hc}{\lambda} (\text{J})
\]</span></p>
<p>donde <span class="math inline">\(h = 6.62607004 \times 10^{-34}
\text{J} \cdot \text{s}\)</span> es la constante de Plank y <span
class="math inline">\(c = 299 792 458 \text{m/s}\)</span> la velocidad
de la luz.</p>
<p>En realidad, <strong>todas estas cantidades deberían tener un
subíndice <span class="math inline">\(\lambda\)</span></strong>, puesto
que dependen de la longitud de onda. La energía de un fotón <span
class="math inline">\(Q\)</span>, por ejemplo, debería denotarse <span
class="math inline">\(Q_\lambda\)</span>. Sin embargo, en la literatura
de informática gráfica, <strong>se ha optado por omitirla</strong>.
¡Tenlo en cuenta a partir de aquí!</p>
<h3 id="potencia">Potencia</h3>
<p>A partir de la energía anterior, podemos estimar <em>la tasa de
producción de energía</em>. A esta tasa la llamaremos<sup><span
class="citation" data-cites="PBRT3e">(<a href="#ref-PBRT3e"
role="doc-biblioref">Pharr, Jakob, and Humphreys 2016</a>)</span></sup>
<strong>potencia</strong>, o <strong>flujo radiante</strong> <span
class="math inline">\(\Phi\)</span>. Esta medida nos resultará más útil
que la energía total, puesto que nos permite estimar la energía en un
instante:</p>
<p><span class="math display">\[
\Phi = \lim_{\Delta t \to 0}{\frac{\Delta Q}{\Delta t}} = \frac{dQ}{dt}
(J/s)
\]</span></p>
<p>Su unidad es julios por segundo, comúnmente denotado vatio
(<em>watts</em>, <span class="math inline">\(\text{W}\)</span>). También
se utiliza el lumen. Podemos encontrar la energía total en un periodo de
tiempo <span class="math inline">\([t_0, t_1]\)</span> integrando el
flujo radiante:</p>
<p><span class="math display">\[
Q = \int_{t_0}^{t_1}{\Phi(t)dt}
\]</span></p>
<h3 id="irradiancia">Irradiancia</h3>
<p>La <strong>irradiancia</strong> o <strong>radiancia emitida</strong>
es el flujo radiante que recibe una superficie. Dada un área <span
class="math inline">\(A\)</span>, se define como</p>
<p><span class="math display">\[
E = \frac{\Phi}{A} (\text{W/m}^2)
\]</span></p>
<figure>
<img src="./img/02/Irradiancia.png"
alt="La irradiancia es la potencia por metro cuadrado incidente en una superficie. Es proporcional al coseno del ángulo entre la dirección de la luz y la normal a la superficie." />
<figcaption aria-hidden="true">La irradiancia es la potencia por metro
cuadrado incidente en una superficie. Es proporcional al coseno del
ángulo entre la dirección de la luz y la normal a la
superficie.</figcaption>
</figure>
<p>Ahora que tenemos la potencia emitida en una cierta área, nos surge
una pregunta: <em>¿y en un cierto punto <span
class="math inline">\(p\)</span>?</em>. Tomando límites en la expresión
anterior, encontramos la respuesta:</p>
<p><span class="math display">\[
E(p) = \lim_{\Delta A \to 0}{\frac{\Delta \Phi}{\Delta A}} =
\frac{d\Phi}{dA} (\text{W/m}^2)
\]</span></p>
<p>De la misma manera que con la potencia, integrando <span
class="math inline">\(E(p)\)</span> podemos obtener el flujo
radiante:</p>
<p><span class="math display">\[
\Phi = \int_{A}{E(p)dp}
\]</span></p>
<p>El principal problema de la irradiancia es que <em>no nos dice nada
sobre las direcciones</em> desde las que ha llegado la luz.</p>
<h3 id="ángulos-sólidos">Ángulos sólidos</h3>
<p>Con estas tres unidades básicas, nos surge una pregunta muy natural:
<em>¿cómo mido cuánta luz llega a una superficie?</em></p>
<p>Para responder a esta pregunta, necesitaremos los <strong>ángulos
sólidos</strong>. Son la extensión de los <strong>ángulos
planares</strong>, en dos dimensiones.</p>
<p>Ilustremos el sentido de estos ángulos: imaginemos que tenemos un
cierto objeto en dos dimensiones delante de nosotros, a una distancia
desconocida. ¿Sabríamos cuál es su tamaño, solo con esta información? Es
más, si entrara otro objeto en la escena, ¿podríamos distinguir cuál de
ellos es más grande?</p>
<p>Parece difícil responder a estas preguntas. Sin embargo, sí que
podemos determinar <em>cómo de grandes nos parecen</em> desde nuestro
punto de vista. Para ello, describimos una circunferencia de radio <span
class="math inline">\(r\)</span> alrededor nuestra. Si trazamos un par
de líneas desde nuestra posición a las partes más alejadas de este
objeto, y las cortamos con nuestra circunferencia, obtendremos un par de
puntos inscritos en ella. Pues bien, al arco que encapsulan dichos
puntos le vamos a hacer corresponder un cierto ángulo: el ángulo
planar.</p>
<figure>
<img src="./img/02/Ángulo%20planar.png"
alt="La idea intuitiva de un ángulo planar" />
<figcaption aria-hidden="true">La idea intuitiva de un ángulo
planar</figcaption>
</figure>
<p>Llevando esta idea a las tres dimensiones es como conseguimos el
concepto de <strong>ángulo sólido</strong>. Si en dos dimensiones
teníamos una circunferencia, aquí tendremos una esfera. Cuando generemos
las rectas proyectantes hacia el volumen, a diferencia de los ángulos
planares, se inscribirá un área en la esfera. La razón entre dicha área
<span class="math inline">\(A\)</span> y el cuadrado del radio <span
class="math inline">\(r\)</span> nos dará un ángulo sólido:</p>
<p><span class="math display">\[
\omega = \frac{A}{r^2} \text{(sr)}
\]</span></p>
<figure>
<img src="./img/02/Ángulo%20sólido.png"
alt="Un ángulo sólido es la razón entre el área proyectada y el cuadrado del radio" />
<figcaption aria-hidden="true">Un ángulo sólido es la razón entre el
área proyectada y el cuadrado del radio</figcaption>
</figure>
<p>Los denotaremos por <span class="math inline">\(\omega\)</span>. En
física se suele usar <span class="math inline">\(\Omega\)</span>, pero
aquí optaremos por la minúscula. Su unidad de medida es el
estereorradián (<span class="math inline">\(\text{sr}\)</span>). Se
tiene que <span class="math inline">\(\omega \in [0, 4\pi]\)</span>. Si
<span class="math inline">\(2 \pi\)</span> radianes corresponden a la
circunferencia completa, para la esfera se tiene que <span
class="math inline">\(4 \pi\)</span> esteorradianes cubren toda la
superficie de esta. Se tiene también que <span
class="math inline">\(2\pi \text{sr}\)</span> cubren un hemisferio.
Además, un esteorradián corresponde a una superficie con área <span
class="math inline">\(r^2\)</span>: <span class="math inline">\(1
\text{sr} = \frac{r^2}{r^2}\)</span>.</p>
<p>De vez en cuando, usaremos <span
class="math inline">\(\omega\)</span> <strong>un vector dirección
unitario en la esfera</strong>.</p>
<figure>
<img src="./img/02/xkcd_1276.png"
alt="Como de costumbre, hay un xkcd relevante. (Fuente)" />
<figcaption aria-hidden="true">Como de costumbre, hay un xkcd relevante.
<a href="https://xkcd.com/1276/">(Fuente)</a></figcaption>
</figure>
<p>Usualmente emplearemos coordenadas esféricas cuando trabajemos con
ellos, dado que resulta más cómodo.</p>
<p><span class="math display">\[
\begin{aligned}
    \begin{cases}
        x = \sin\theta\cos\theta \\
        y = \sin\theta\sin\theta \\
        z = \cos\theta
    \end{cases}
\end{aligned}
\]</span></p>
<p>A <span class="math inline">\(\theta\)</span> se le denomina ángulo
polar, mientras que a <span class="math inline">\(\phi\)</span> se le
llama acimut. Imaginémonos un punto en la esfera de radio <span
class="math inline">\(r\)</span> ubicado en una posición <span
class="math inline">\((r, \theta, \phi)\)</span>. Queremos calcular un
área chiquitita <span class="math inline">\(dA_h\)</span>, de forma que
el ángulo sólido asociado a dicha área debe ser <span
class="math inline">\(d\omega\)</span>. Así, <span
class="math inline">\(d\omega = \frac{dA_h}{r^2}\)</span>. Si
proyectamos el área, obtenemos <span
class="math inline">\(d\theta\)</span> y <span
class="math inline">\(d\phi\)</span>: pequeños cambios en los ángulos
que nos generan nuestra pequeña área.</p>
<p><span class="math inline">\(dA_h\)</span> debe tener dos lados <span
class="math inline">\(lado_1\)</span> y <span
class="math inline">\(lado_2\)</span>. Podemos hallar <span
class="math inline">\(lado_1\)</span> si lo trasladamos al eje <span
class="math inline">\(z\)</span> de nuevo. Así, <span
class="math inline">\(lado_1 = r \sin d\theta\)</span>. De la misma
manera, <span class="math inline">\(lado_2 = r d\theta\)</span>.</p>
<blockquote>
<p>TODO: foto que explique todo esto, porque si no, no hay quien se
entere. Quizás me sirva la de
https://cs184.eecs.berkeley.edu/public/sp22/lectures/lec-11-radiometry-and-photometry/lec-11-radiometry-and-photometry.pdf,
p.16 siempre que adapte <span class="math inline">\(\phi\)</span>.</p>
</blockquote>
<p>Poniendo estos valores en <span
class="math inline">\(d\omega\)</span>:</p>
<p><span id="eq:d_omega" class="eqnos"><span class="math display">\[
\begin{aligned}
d\omega &amp; = \frac{dA_h}{r^2} = \frac{lado_1 lado_2}{r^2} = \\
        &amp; = \frac{r \sin\theta\ d\phi\ r\ d\theta}{r^2} = \\
        &amp; = \sin\theta\ d\theta\ d\phi
\end{aligned}
\]</span><span class="eqnos-number">(2)</span></span></p>
<p>¡Genial! Acabamos de añadir un recurso muy potente a nuestro
inventario. Esta expresión nos permitirá convertir integrales sobre
ángulos sólidos en integrales sobre ángulos esféricos.</p>
<h3 id="intensidad-radiante">Intensidad radiante</h3>
<p>Los ángulos sólidos nos proporcionan una variedad de herramientas
nuevas considerable. Gracias a ellos, podemos desarrollar algunos
conceptos nuevos. Uno de ellos es la <strong>intensidad
radiante</strong>.</p>
<p>Imaginémonos un pequeñito punto de luz encerrado en una esfera, el
cual emite fotones en todas direcciones. Nos gustaría medir cuánta
energía pasa por la esfera. Podríamos entonces definir</p>
<p><span class="math display">\[
I = \frac{\Phi}{4\pi} \text{(W/sr)}
\]</span></p>
<p>Otra unidad de medida es el lumen por esterorradián, <span
class="math inline">\(\text{(lm/sr)}\)</span>. La anterior definición
mide cuántos fotones pasan por toda la esfera. ¿Qué ocurre si
<em>cerramos</em> el ángulo, restringiéndonos así a un área muy pequeña
de la esfera?</p>
<p><span class="math display">\[
I = \lim_{\Delta\omega \to 0}{\frac{\Delta\Phi}{\Delta\omega}} =
\frac{d\Phi}{d\omega}
\]</span></p>
<p>De la misma manera que con los conceptos anteriores, podemos volver a
la potencia integrando sobre un conjunto de direcciones:</p>
<p><span class="math display">\[
\Phi = \int_{\Omega}{I(\omega)d\omega}
\]</span></p>
<h3 id="radiancia">Radiancia</h3>
<p>Finalmente, llegamos al concepto más importante. La <strong>radiancia
espectral</strong> (o radiancia a secas<a href="#fn2"
class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>) es
una extensión de la radiancia emitida teniendo en cuenta la
dirección:</p>
<p><span class="math display">\[
L(p, \omega) = \lim_{\Delta\omega \to 0}{\frac{\Delta
E_\omega(p)}{\Delta\omega}} = \frac{dE_\omega(p)}{d\omega}
\]</span></p>
<p>siendo <span class="math inline">\(E_\omega(p)\)</span> la radiancia
emitida a la superficie perpendicular a <span
class="math inline">\(\omega\)</span>.</p>
<blockquote>
<p>TODO: foto como la de
https://cs184.eecs.berkeley.edu/public/sp22/lectures/lec-11-radiometry-and-photometry/lec-11-radiometry-and-photometry.pdf,
página 10.</p>
</blockquote>
<p>Podemos dar otra expresión de la radiancia en términos del flujo:</p>
<p><span id="eq:radiancia_flujo" class="eqnos"><span
class="math display">\[
L(p, \omega) = \frac{d^2\Phi(p, \omega)}{d\omega\ dA^\bot} =
\frac{d^2\Phi(p, \omega)}{d\omega\ dA\ \cos\theta}
\]</span><span class="eqnos-number">(3)</span></span></p>
<p>donde <span class="math inline">\(dA^\bot\)</span> es el área
proyectada por <span class="math inline">\(dA\)</span> en una hipotética
superficie perpendicular a <span
class="math inline">\(\omega\)</span>:</p>
<blockquote>
<p>TODO: figura similar a pbr figura 5.10
https://www.pbr-book.org/3ed-2018/Color_and_Radiometry/Radiometry</p>
</blockquote>
<p>Cuando un rayo impacta en una superficie, <span
class="math inline">\(L\)</span> puede tomar valores muy diferentes en
un lado y otro de dicha superficie. Por ejemplo, si nos imaginamos un
espejo, el valor un poco por encima y un poco por debajo de un punto del
espejo es muy diferente. Para solucionarlo, podemos tomar límites para
distinguir a ambos lados:</p>
<p><span id="eq:L_limit" class="eqnos"><span class="math display">\[
\begin{aligned}
L^+(p, \omega) = \lim_{t \to 0^+}{L(p + t\mathbf{n_p}, \omega)} \\
L^-(p, \omega) = \lim_{t \to 0^-}{L(p + t\mathbf{n_p}, \omega)}
\end{aligned}
\]</span><span class="eqnos-number">(4)</span></span></p>
<p>donde <span class="math inline">\(\mathbf{n_p}\)</span> es la normal
en el punto <span class="math inline">\(p\)</span>.</p>
<p>Otra forma de solucionarlo (y preferible, puesto que simplifica
entender lo que ocurre) es distinguir entre la radiancia que llega a un
punto –la incidente–, y la saliente.</p>
<p>La primera se llamará <span class="math inline">\(L_i(p,
\omega)\)</span>, mientras que la segunda será <span
class="math inline">\(L_o(p, \omega)\)</span>. Es importante destacar
que <span class="math inline">\(\omega\)</span> apunta <em>hacia
fuera</em> de la superficie. Quizás es contraintuitivo en <span
class="math inline">\(L_i\)</span>, puesto que <span
class="math inline">\(-\omega\)</span> apunta <em>hacia</em> la
superficie. Depende del autor se utiliza una concepción u otra.</p>
<blockquote>
<p><strong>Nota</strong>(ción): a <span
class="math inline">\(L_o\)</span> también se le conoce como la
radiancia reflejada. Por eso, algunas veces aparece como <span
class="math inline">\(L_r\)</span> en algunas fuentes.</p>
</blockquote>
<p>Utilizando esta notación y usando [<a href="#eq:L_limit">4</a>],
podemos escribir <span class="math inline">\(L_i\)</span> y <span
class="math inline">\(L_o\)</span> como</p>
<p><span class="math display">\[
\begin{aligned}
    L_i(p, \omega) &amp; =
        \begin{cases}
            L^+(p, -\omega) &amp; \text{si } \omega \cdot \mathbf{n_p}
&gt; 0 \\
            L^-(p, -\omega) &amp; \text{si } \omega \cdot \mathbf{n_p}
&lt; 0
        \end{cases} \\
    L_o(p, \omega) &amp; =
        \begin{cases}
            L^+(p, \omega) &amp; \text{si } \omega \cdot \mathbf{n_p}
&gt; 0 \\
            L^-(p, \omega) &amp; \text{si } \omega \cdot \mathbf{n_p}
&lt; 0
        \end{cases}
\end{aligned}
\]</span></p>
<p>Hacemos esta distinción porque, a fin de cuentas, necesitamos
distinguir entre los fotones que llegan a la superficie y los que
salen.</p>
<blockquote>
<p>TODO:
https://cs184.eecs.berkeley.edu/public/sp22/lectures/lec-11-radiometry-and-photometry/lec-11-radiometry-and-photometry.pdf,
p.36</p>
</blockquote>
<p>Una propiedad a tener en cuenta es que, si cogemos un punto <span
class="math inline">\(p\)</span> del espacio donde no existe ninguna
superifcie, <span class="math inline">\(L_o(p, \omega) = L_i(p, -\omega)
= L(p, \omega)\)</span></p>
<p>La importancia de la radiancia se debe a un par de propiedades:</p>
<p>La primera de ellas es que, dado <span
class="math inline">\(L\)</span>, podemos calcular cualquier otra unidad
básica mediante integración. Además, <strong>su valor se mantiene
constante en rayos que viajan en el vacío en línea recta</strong> <span
class="citation" data-cites="Pellacini-Marschner-2017">(<a
href="#ref-Pellacini-Marschner-2017" role="doc-biblioref">Fabio
Pellacini n.d.</a>)</span>. Esto último hace que resulte muy natural
usarla en un ray tracer.</p>
<p>Veamos por qué ocurre esto:</p>
<blockquote>
<p>TODO:
https://pellacini.di.uniroma1.it/teaching/graphics17b/lectures/12_pathtracing.pdf,
página 18.</p>
</blockquote>
<p>Consideremos dos superficies ortogonales entre sí, <span
class="math inline">\(S_1\)</span> y <span
class="math inline">\(S_2\)</span> separadas una distancia <span
class="math inline">\(r\)</span>. Debido a la conservación de la
energía, cualquier fotón que salga de una superficie y se encuentre bajo
el ángulo sólido de la otra debe llegar impactar en dicha superficie
opuesta.</p>
<p>Por tanto:</p>
<p><span class="math display">\[
d^2\Phi_1 = d^2\Phi_2
\]</span></p>
<p>Sustituyendo en la expresión de la radiancia [<a
href="#eq:radiancia_flujo">3</a>], y teniendo en cuenta que son
ortogonales (lo que nos dice que <span class="math inline">\(\cos\theta
= 1\)</span>):</p>
<p><span class="math display">\[
L_1 d\omega_1 dA_1 = L_2 d\omega_2 dA_2
\]</span></p>
<p>Por construcción, podemos cambiar los ángulos sólidos:</p>
<p><span class="math display">\[
L_1 \frac{dA_2}{r^2} dA_1 = L_2 \frac{dA_1}{r^2} dA_2
\]</span></p>
<p>Lo que finalmente nos dice que <span class="math inline">\(L_1 =
L_2\)</span>, como queríamos ver.</p>
<h3 id="integrales-radiométricas">Integrales radiométricas</h3>
<p>En esta sección, vamos a explorar las nuevas herramientas que nos
proporciona la radiancia. Veremos también cómo integrar ángulos sólidos,
y cómo simplificar dichas integrales.</p>
<h4 id="una-nueva-expresión-de-la-irradiancia-y-el-flujo">Una nueva
expresión de la irradiancia y el flujo</h4>
<p>Como dijimos al final de <a href="#irradiancia">la sección de la
irradiancia</a>, esta medida no tiene en cuenta las direcciones desde
las que llegaba la luz. A diferencia de esta, la radiancia sí que las
utiliza. Dado que una de las ventajas de la radiancia es que nos permite
obtener el resto de medidas radiométricas, ¿por qué no desarrollamos una
nueva expresión de la irradiancia?</p>
<p>Para obtener cuánta luz llega a un punto, debemos acumular la
radiancia incidente que nos llega desde cualquier dirección.</p>
<blockquote>
<p>TODO: dibujo como el de la libreta roja. Me lo mandé por Telegram,
por si no lo encuentro</p>
</blockquote>
<p>Dado un punto <span class="math inline">\(p\)</span> que se encuentra
en una superficie con normal <span
class="math inline">\(\mathbf{n}\)</span> en dicho punto, la irradiancia
se puede expresar como</p>
<p><span id="eq:E_abs_cos" class="eqnos"><span class="math display">\[
E(p, \mathbf{n}) = \int_{\Omega}{L_i(p, \omega) \lvert cos\theta \rvert
d\omega}
\]</span><span class="eqnos-number">(5)</span></span></p>
<p>El término <span class="math inline">\(\cos\theta\)</span> aparece en
la integral debido a la derivada del área proyectada, <span
class="math inline">\(dA^\bot\)</span>. <span
class="math inline">\(\theta\)</span> es el ángulo entre la dirección
<span class="math inline">\(\omega\)</span> y la normal <span
class="math inline">\(\mathbf{n}\)</span>.</p>
<p>Generalmente, la irradiancia se calcula únicamente en el hemisferio
de direcciones asociado a la normal en el punto, <span
class="math inline">\(H^2(\mathbf{n})\)</span>.</p>
<p>Podemos eliminar el <span class="math inline">\(\cos\theta\)</span>
de la integral mediante una pequeña transformación: proyectando el
ángulo sólido sobre el disco alrededor del punto <span
class="math inline">\(p\)</span> con normal <span
class="math inline">\(\mathbf{n}\)</span>, obtenemos una expresión más
sencilla: como <span class="math inline">\(d\omega^\bot = \lvert
\cos\theta \rvert d\omega\)</span>, entonces</p>
<p><span class="math display">\[
\begin{aligned}
    E(p, \mathbf{n}) = \int_{H^2(\mathbf{n})}{L_i(p, \omega)
d\omega^\bot}
\end{aligned}
\]</span></p>
<p>Usando lo que aprendimos sobre la derivada de los ángulos sólidos [<a
href="#eq:d_omega">2</a>], se puede reescribir la ecuación anterior
como</p>
<p><span class="math display">\[
E(p, \mathbf{n}) = \int_{0}^{2\pi}\int_{0}^{\pi/2}{L_i(p, \theta, \phi)
\cos\theta\ \sin\theta\ d\theta\ d\phi}
\]</span></p>
<p>Haciendo el mismo juego con el flujo emitido de un cierto objeto al
hemisferio que encapsula la normal, conseguimos:</p>
<p><span class="math display">\[
\begin{aligned}
    \Phi &amp; = \int_{A}\int_{H^2(\mathbf{n})}{L_o(p, \omega)
\cos\theta\ d\omega dA} = \\
         &amp; = \int_{A}\int_{H^2(\mathbf{n})}{L_o(p, \omega)
d\omega^\bot dA}
\end{aligned}
\]</span></p>
<blockquote>
<p>TODO: a lo mejor merece la pena hacer un ejemplo sobre los diferentes
tipos de luz, como en
https://cs184.eecs.berkeley.edu/public/sp22/lectures/lec-11-radiometry-and-photometry/lec-11-radiometry-and-photometry.pdf
p.41? O a lo mejor un capítulo para hablar de luces en general.</p>
</blockquote>
<h4 id="integrando-sobre-área">Integrando sobre área</h4>
<p>Una herramienta más que nos vendrá bien será la capacidad de
convertir integrales sobre direcciones en integrales sobre área. Hemos
hecho algo similar en las secciones anteriores, así que no perdemos nada
por generalizarlo.</p>
<p>Considera un punto <span class="math inline">\(p\)</span> sobre una
superficie con normal en dicho punto <span
class="math inline">\(\mathbf{n}\)</span>. Supongamos que tenemos una
pequeña área <span class="math inline">\(dA\)</span> con normal <span
class="math inline">\(\mathbf{n_{dA}}\)</span>. Sea <span
class="math inline">\(\theta\)</span> el ángulo entre <span
class="math inline">\(\mathbf{n}\)</span> y <span
class="math inline">\(\mathbf{n_{dA}}\)</span>, y <span
class="math inline">\(r\)</span> la distancia entre <span
class="math inline">\(p\)</span> y <span
class="math inline">\(dA\)</span>.</p>
<p>Entonces, la relación entre la diferencial de un ángulo sólido y la
de un área es</p>
<p><span class="math display">\[
d\omega = \frac{dA\cos\theta}{r^2}
\]</span></p>
<blockquote>
<p>TODO: figura como la de pbr book 5.16.</p>
</blockquote>
<p>Esto nos permite, por ejemplo, expandir algunas expresiones como la
de la irradiancia [<a href="#eq:E_abs_cos">5</a>] si partimos de un
cuadrilátero <span class="math inline">\(dA\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
    E(p, \mathbf{n}) &amp; = \int_{\Omega}{L_i(p, \omega) \lvert
\cos\theta \rvert d\omega} = \\
                     &amp; = \int_{A}{L\cos\theta\
\frac{\cos\theta_o}{r^2}dA}
\end{aligned}
\]</span></p>
<p>siendo <span class="math inline">\(\theta_o\)</span> el ángulo de la
radiancia de salida de la superficie del cuadrilátero.</p>
<h3 id="fotometría-y-radiometría">Fotometría y radiometría</h3>
<blockquote>
<p>TODO: hablar sobre las diferencias. Hay información útil en
01_lights.pdf, p.43</p>
</blockquote>
<h2 id="dispersión-de-luz">Dispersión de luz</h2>
<p>Cuando una luz impacta en una superficie, ocurren un par de sucesos:
parte de los fotones se reflejan saliendo disparados hacia alguna
dirección, mientras que otros se absorben.</p>
<p>La forma en la que se comportan depende de cómo sea la superficie.
Específicamente, del material del que esté hecha.</p>
<p>En informática gráfica se consideran tres tipos principales de
dispersión de luz: <strong>dispersión en superficie</strong>
(<em>surface scattering</em>), <strong>dispersión volumétrica</strong>
(<em>volumetric scattering</em>) y <strong>dispersión bajo
superficie</strong> (<em>subsurface scattering</em>)</p>
<p>En este capítulo vamos a modelar la primera. Estudiaremos qué es lo
que ocurre cuando los fotones alcanzan una superficie, en qué dirección
se reflejan, y cómo cambia el comportamiento dependiendo de las
propiedades del material.</p>
<h3
id="la-función-de-distribución-de-reflectancia-bidireccional-brdf">La
función de distribución de reflectancia bidireccional (BRDF)</h3>
<p>La <strong>función de distribución de reflectancia
bidireccional</strong> (en inglés, <em>bidirectional reflectance
distribution function</em>, BRDF) describe cómo la luz se refleja en una
superficie opaca. Se encarga de informarnos sobre cuánta radiancia sale
en dirección <span class="math inline">\(\omega_o\)</span> debido a la
radiancia incidente desde la dirección <span
class="math inline">\(\omega_i\)</span>, partiendo de un punto <span
class="math inline">\(p\)</span> en una superficie con normal <span
class="math inline">\(\mathbf{n}\)</span>. Depende de la longitud de
onda <span class="math inline">\(\lambda\)</span>, pero, como de
costumbre, la omitiremos.</p>
<blockquote>
<p><strong>Intuición</strong>: <em>¿cuál es la probabilidad de que,
habiéndome llegado un fotón desde <span
class="math inline">\(\omega_i\)</span>, me salga disparado hacia <span
class="math inline">\(\omega_o\)</span>?</em></p>
</blockquote>
<blockquote>
<p>TODO: esquema como el de pbr fig. 5.18, o como
https://pellacini.di.uniroma1.it/teaching/graphics17b/lectures/12_pathtracing.pdf
p.20</p>
</blockquote>
<p>Si consideramos <span class="math inline">\(\omega_i\)</span> como un
cono diferencial de direcciones, la irradiancia diferencial en <span
class="math inline">\(p\)</span> viene dada por</p>
<p><span class="math display">\[
dE(p, \omega_i) = L_i(p, \omega_i) \cos\theta_i\ d\omega_i
\]</span></p>
<p>Debido a esta irradiancia, una pequeña parte de radiancia saldrá en
dirección <span class="math inline">\(\omega_o\)</span>, proporcional a
la irradiancia:</p>
<p><span class="math display">\[
dL_o(p, \omega_o) \propto dE(p, \omega_i)
\]</span></p>
<p>Si lo ponemos en forma de cociente, sabremos exactamente cuál es la
proporción de luz. A este cociente lo llamaremos <span
class="math inline">\(f_r(p, \omega_o \leftarrow \omega_i)\)</span>; la
función de distribución de reflectancia bidireccional:</p>
<p><span class="math display">\[
f_r(p, \omega_o \leftarrow \omega_i) = \frac{dL_o(p, \omega_o)}{dE(p,
\omega_i)} = \frac{dL_o(p, \omega_o)}{L_i(p, \omega_i) \cos\theta_i\
d\omega_i} \text{(1/sr)}
\]</span></p>
<blockquote>
<p><strong>Nota</strong>(ción): dependiendo de la fuente que estés
leyendo, es posible que te encuentres una integral algo diferente. Por
ejemplo, en tanto en Wikipedia como en <span class="citation"
data-cites="ShirleyRRT">(<a href="#ref-ShirleyRRT"
role="doc-biblioref">Shirley and Morley 2003</a>)</span> se integra con
respecto a los ángulos de salida <span
class="math inline">\(\omega_o\)</span>, en vez de los incidentes.</p>
<p>Aquí, usaremos la notación de integrar con respecto a los incidentes,
como se hace en <span class="citation" data-cites="PBRT3e">(<a
href="#ref-PBRT3e" role="doc-biblioref">Pharr, Jakob, and Humphreys
2016</a>)</span>.</p>
</blockquote>
<p>Las BRDF físicamente realistas tienen un par de propiedades
importantes:</p>
<ol type="1">
<li><strong>Reciprocidad</strong>: para cualquier par de direcciones
<span class="math inline">\(\omega_i\)</span>, <span
class="math inline">\(\omega_o\)</span>, se tiene que <span
class="math inline">\(f_r(p, \omega_i, \omega_o)=\ \)</span> <span
class="math inline">\(f_r(p, \omega_o \leftarrow
\omega_i)\)</span>.</li>
<li><strong>Conservación de la energía</strong>: La energía reflejada
tiene que ser menor o igual que la incidente:</li>
</ol>
<p><span class="math display">\[
\int_{H^2(\mathbf{n})}{f_r(p, \omega_o \leftarrow \omega_i)
\cos\theta_i\ d\omega_i} \leq 1
\]</span></p>
<h3
id="la-función-de-distribución-de-transmitancia-bidireccional-btdf">La
función de distribución de transmitancia bidireccional (BTDF)</h3>
<p>Si la BRDF describe cómo se refleja la luz, la <em>bidirectional
transmittance distribution function</em> (abreviada BTDF) nos informará
sobre la transmitancia; es decir, cómo se comporta la luz cuando
<em>entra</em> en un medio. Generalmente serán dos caras de la misma
moneda: cuando la luz impacta en una superficie, parte de ella, se
reflejará, y otra parte se transmitirá.</p>
<p>Puedes imaginarte la BTDF como una función de reflectancia del
hemisferio opuesto a donde se encuentra la normal de la superficie.</p>
<p>Denotaremos a la BTDF por</p>
<p><span class="math display">\[
f_t(p, \omega_o \leftarrow \omega_i)
\]</span></p>
<p>Al contrario que en la BRDF, <span
class="math inline">\(\omega_o\)</span> y <span
class="math inline">\(\omega_i\)</span> se encuentran en hemisferios
diferentes.</p>
<h3 id="la-función-de-distribución-de-dispersión-bidireccional-bsdf">La
función de distribución de dispersión bidireccional (BSDF)</h3>
<p>Convenientemente, podemos unir la BRDF y la BTDF en una sola
expresión, llamada <strong>la función de distribución de dispersión
bidireccional</strong> (<em>bidirectional scattering distribution
function</em>, BSDF). A la BSDF la denotaremos por</p>
<p><span class="math display">\[
f(p, \omega_o \leftarrow \omega_i)
\]</span></p>
<blockquote>
<p><strong>Nota</strong>(ción): también se suele utilizar BxDF en vez de
BSDF.</p>
</blockquote>
<p>Usando esta definición, podemos obtener</p>
<p><span class="math display">\[
dL_o(p, \omega_o) = f(p, \omega_o \leftarrow \omega_i) L_i(p, \omega_i)
\lvert \cos\theta_i \rvert d\omega_i
\]</span></p>
<p>Esto nos deja a punto de caramelo una nueva expresión de la
randiancia en términos de la randiancia incidente en un punto <span
class="math inline">\(p\)</span>. Integrando la expresión anterior,
obtenemos</p>
<p><span id="eq:scattering_equation" class="eqnos"><span
class="math display">\[
L_o(p, \omega_o) = \int_{\mathbb{S}^2}{f(p, \omega_o \leftarrow
\omega_i)L_i(p, \omega_i)\lvert \cos\theta_i \rvert d\omega_i}
\]</span><span class="eqnos-number">(6)</span></span></p>
<p>siendo <span class="math inline">\(\mathbb{S}^2\)</span> la
esfera.</p>
<blockquote>
<p><strong>Intuición:</strong> <em>la BSDF son todas las posibles
direcciones en las que puede salir disparada la luz.</em></p>
</blockquote>
<p>Esta forma de expresar la radiancia es muy importante. Generalmente
se le suele llamar la <em>ecuación de dispersión</em> (<em>scattering
equation</em>, en inglés). Dado que es una integral muy importante,
seguramente tengamos que evaluarla repetidamente. ¡Los métodos de Monte
Carlo nos vendrán de perlas! Más adelante hablaremos de ella.</p>
<p>Las BSDFs tienen unas propiedades interesantes:</p>
<ul>
<li><strong>Positividad</strong>: como los fotones no se pueden reflejar
“negativamente”, <span class="math inline">\(f(p, \omega_o \leftarrow
\omega_i) \ge 0\)</span>.</li>
<li><strong>Reciprocidad de Helmotz:</strong> se puede invertir la
dirección de un rayo: <span class="math inline">\(f(p, \omega_o
\leftarrow \omega_i) = f(p, \omega_i \leftarrow \omega_o)\)</span>.</li>
<li><strong>White furnace test</strong>: Toda la luz incidente debe ser
reflejada cuando la reflectividad de la superficie es 1.</li>
<li><strong>Conservación de la energía</strong>: todos los fotones que
llegan a la superficie deben ser reflejados o absorbidos. Es decir, no
se emite ningún fotón nuevo:</li>
</ul>
<p><span class="math display">\[
\int_{H^2(\mathbf{n})}{f(p, \omega_o \leftarrow \omega_i) \cos\theta_i\
d\omega_i} \le 1\ \forall \omega_o
\]</span></p>
<h3 id="reflectancia-hemisférica">Reflectancia hemisférica</h3>
<p>Puede ser útil tomar el comportamiento agregado de las BRDFs y las
BTDFs y reducirlo un cierto valor que describa su comportamiento general
de dispersión. Sería Algo así como un resumen de su distribución. Para
conseguirlo, vamos a introducir dos nuevas funciones:</p>
<p>La <strong>reflectancia hemisférica-direccional</strong>
(<em>hemispherical-directional reflectance</em>) describe la reflexión
total sobre un hemisferio debida a una fuente de luz que proviene desde
la dirección <span class="math inline">\(\omega_o\)</span>:</p>
<p><span class="math display">\[
\rho_{hd}(\omega_o) = \int_{H^2(n)}{f_r(p, \omega_o \leftarrow \omega_i)
\lvert \cos\theta_i \rvert\ d\omega_i}
\]</span></p>
<p>Por otra parte, la <strong>reflectancia
hemisférica-hemisférica</strong> (<em>hemispherical-hemispherical
reflectance</em>) es un valor espectral que nos proporciona el ratio de
luz incidente reflejada por una superficie, suponiendo que llega la
misma luz desde todas direcciones:</p>
<p><span class="math display">\[
\rho_{hh} = \frac{1}{\pi} \int_{H^2(n)} \int_{H^2(n)}{f_r(p, \omega_o
\leftarrow \omega_i) \lvert \cos\theta_o\ \cos\theta_i \rvert\
d\omega_o\ d\omega_i}
\]</span></p>
<h3 id="tipos-de-dispersión">Tipos de dispersión</h3>
<p>Una vez hemos definido las funciones de distribución bidireccionales,
debemos encargarnos de modelar el comportamiento explícitamente. Para
ello, veamos cómo los materiales modifican las distribuciones.</p>
<p>En esencia, los reflejos se pueden clasificar en cuatro grandes
tipos:</p>
<ul>
<li><strong>Difusos</strong> (<em>Diffuse</em>): esparcen la luz en
todas direcciones casi equiprobablemente. Por ejemplo, la tela y el
papel son materiales difusos.</li>
<li><strong>Especulares brillantes</strong> (<em>Glossy specular</em>):
la distribución de luz se asemeja a un cono. La chapa de un coche es un
material especular brillante.</li>
<li><strong>Especulares perfectos</strong> (<em>Perfect specular</em>):
en esencia, son espejos. El ángulo de salida de la luz es muy pequeño,
por lo que reflejan casi a la perfección lo que les llega.</li>
<li><strong>Retrorreflectores</strong> (<em>Retro reflective</em>): la
luz se refleja en dirección contraria a la de llegada. Esto es lo que
sucede a la luna.</li>
</ul>
<p>Ten en cuenta que es muy difícil encontrar objetos físicos que imiten
a la perfección un cierto modelo. Suelen recaer en un híbrido entre dos
o más modelos.</p>
<p>Fijado un cierto modelo, la función de distribución de reflectancia,
BRDF, puede ser <strong>isotrópica</strong> o
<strong>anisotrópica</strong>. Los materiales isotrópicos mantienen las
propiedades de reflectancia invariantes ante rotaciones; es decir, la
distribución de luz es la misma en todas direcciones. Por el contrario,
los anisotrópicos reflejan diferentes cantidades de luz dependiendo
desde dónde los miremos. Los ejemplos más habituales de materiales
anisotrópicos son las rocas y la madera.</p>
<h3 id="modelos-analíticos-de-shading">Modelos analíticos de
<em>shading</em></h3>
<blockquote>
<p>TODO: https://alain.xyz/blog/advances-in-material-models este señor
me acaba de solucionar la vida. Gracias por tanto.</p>
<p>TODO: Completar con lo siguiente, basado en
02_Rendering_Zsolnai_Ray_Tracing.</p>
<p>TODO: también estaría bien añadir el repositorio de Disney como
referencia https://github.com/wdas/brdf/tree/main/src/brdfs</p>
</blockquote>
<p>Los modelos analíticos de shading surgen como simplificaciones de las
BRDFs. En esta sección, vamos a ver algunos de los más famosos.</p>
<p>Usaremos <span class="math inline">\(L_o^d\)</span> para indicar la
radiancia obtenida por materiales difusos, y <span
class="math inline">\(L_o^s\)</span> para los especulares.</p>
<h4 id="lambertiano">Lambertiano</h4>
<p>Este es uno de los modelos más sencillos. Se asume que la superficie
es completamente difusa, lo cual implica que la luz se refleja en todas
direcciones equiprobablemente, independientemente del punto de vista del
observador.</p>
<p>Se describe como</p>
<p><span class="math display">\[
L_o^d(p, \omega_o \leftarrow \omega_i) = k_d \max\{0, \mathbf{n} \cdot
\mathbf{l}\}
\]</span></p>
<p>siendo <span class="math inline">\(k_d\)</span> el coeficiente de
reflectancia (que mide cuánta luz se absorbe por la superficie, conocido
como albedo), <span class="math inline">\(\mathbf{n}\)</span> la normal
al punto en la superficie, y <span
class="math inline">\(\mathbf{l}\)</span> la dirección de la luz.</p>
<p>El producto escalar de los vectores <span
class="math inline">\(\mathbf{n}\)</span> y <span
class="math inline">\(\mathbf{l}\)</span> hace que, cuando el ángulo de
incidencia de la luz es muy cerrado, la radiancia será prácticamente
0.</p>
<p>La implementación es muy sencilla:</p>
<pre><code class="language-glsl">float Lambertian_pdf(vec3 normal, vec3 light_dir) {
    return max(
        0.0,
        dot(normal, light_dir)
    ) * (1.0 / PI);
}

float Lambiertian_light(Superficie s, Luz light) {
    return s.albedo * Lambertian_pdf(s.normal, light.dir);
}</code></pre>
<p>Este modelo está muy limitado, pues en la vida real, los objetos
muestran algún tipo de interacción especular.</p>
<h4 id="phong">Phong</h4>
<p>El modelo de Phong se basa en la observación de que, cuando el punto
de vista se alinea con la dirección del vector de luz reflejado <span
class="math inline">\(r = 1 - 2(\mathbf{n} \cdot
\mathbf{l})\mathbf{n}\)</span>, aparecen puntos muy iluminados, lo que
se conoce como resaltado especular.</p>
<p>Esta idea se <em>refleja</em> considerando la componente especular
como</p>
<p><span class="math display">\[
L_o^s(p, \omega_o \leftarrow \omega_i) =
      k_\alpha
    + k_d L_o^d(p, \omega_o \leftarrow \omega_i)
    + k_s \max\{0, \omega \cdot \mathbf{r}\}^\alpha
\]</span></p>
<p>donde <span class="math inline">\(k_\alpha\)</span> es el coeficiente
de luz ambiental (con <span class="math inline">\(\alpha\)</span> el
índice de brillo) <span class="math inline">\(k_s\)</span> es la
constante de reflectancia especular (<em>specular-reflection</em>) que
define el ratio de luz reflejada, <span
class="math inline">\(k_d\)</span> el de radiancia difusa <span
class="math inline">\(L_o^d\)</span>. Usualmente, <span
class="math inline">\(k_s \vert k_d &lt; 1\)</span>.</p>
<p>Evidentemente, este modelo no es más que una aproximación físicamente
poco realista de la realidad; pero funciona lo suficientemente bien como
para usarlo en ciertas partes.</p>
<pre><code class="language-glsl">float Phong_specular(vec3 normal, vec3 light_dir, vec3 view_dir, float shininess) {
    return pow(
        max(
            0.0,
            dot(
                reflejar(normal, light_dir),
                view_dir
            )
        ),
        shininess
    );
}</code></pre>
<h4 id="blinn---phong">Blinn - Phong</h4>
<p>Este es una pequeña modificación al de Phong. En vez de usar el
vector reflejado de luz, se define un vector unitario entre el
observador y la luz, <span class="math inline">\(\mathbf{h} =
\frac{\omega + \mathbf{l}}{\lVert \omega + \mathbf{l} \rVert}\)</span>.
Resulta más fácil calcularlo. Además, este modelo es más realista.</p>
<p><span class="math display">\[
L_o^s(p, \omega_o \leftarrow \omega_i) =
      k_\alpha
    + k_d L_o^d(p, \omega_o \leftarrow \omega_i)
    + k_s \max\{0, \mathbf{h} \cdot \mathbf{n}\}^\alpha
\]</span></p>
<pre><code class="language-glsl">float BlingPhong_specular(vec3 normal, vec3 light_dir, vec3 view_dir, float shininess) {
    vec3 h = normalize(view_dir + light_dir);
    return pow(
        max(
            0.0,
            dot(h, normal)
        ),
        shininess
    );
}</code></pre>
<h4 id="schlick">Schlick</h4>
<blockquote>
<p>TODO: este está bastante cojo. Parece que está medianamente bien
descrito en RT IOW, así que podría revisarlo de ahí.</p>
</blockquote>
<p>El modelo de Schlick describe una aproximación de las ecuaciones de
Fresnel. Se utiliza con frecuencia, por ser físicamente realista y
fácilmente computable.</p>
<p>Se define como</p>
<p><span class="math display">\[
\begin{aligned}
    &amp; S_\lambda(u) = C_\lambda + (1 - C_\lambda)(1 - u)^5 \\
    &amp; R_\lambda(t, u, v, v&#39;, w) = S_\lambda(u) D(t, v, v&#39;,
w)
\end{aligned}
\]</span></p>
<pre><code class="language-glsl">float Fresnel(float f0, float u) {
    return f0 + (1.0 - f0) * pow(1.0 - u, 5.0);
}

vec3 BRDF(vec3 light_dir, vec3 view_dir, vec3 normal, vec3 X, vec3 Y) {
    vec3 halfway = normalize(light_dir + view_dir);

    float ior = 1   // Puede vale 1, 3, o 1.2
    float f0  = pow((ior - 1)/(ior + 1), 2);
    float F   = Fresnel(f0, dot(light_dir, halfway));

    return vec3(F);
}</code></pre>
<h4 id="oren---nayar">Oren - Nayar</h4>
<p>Este modelo intenta aproximar superficies difusas utilizando un ratio
de lambertiano, lo cual mejora el rendimiento el <em>white furnace
test</em>:</p>
<pre><code>float OrenNayar_diffuse(vec3 normal, vec3 light_dir, vec3 view_dir, material m) {
    float L_dot_V = dot(light_dir, view_dir);
    float N_dot_L = dot(light_dir, noral);
    float N_dot_V = dot(normal, view_dir);

    float s = L_dot_V - N_dot_L * N_dot_V;
    float t = mix(
        1.0,
        ma(N_dot_L, N_dot_V),
        step(0.0, s)
    );

    float sigma2 = m.roughness * m.roughness;
    float A = 1.0 + sigma2 * (m.albedo / (sigma2 + 0.13) + 0.5 / (sigma2 + 0.33));
    float B = 0.45 * sigma2 / (sigma2 + 0.09);

    return m.albedo * max(0.0, N_dot_L) * (A + B * s / t) / PI;
}</code></pre>
<h4 id="ggx">GGX</h4>
<p>El modelo Ground Glass Unknown es una BSDF analítica que se basa en
la distribución de microfacetas del material subyacente. Es una de las
técnicas más avanzadas y exploradas recientemente. Los motores modernos
como Unreal Engine 4 y Unity lo utilizan en sus pipelines físicamente
realistas.</p>
<p>A diferencia de los otros modelos, no entraremos en detalles de la
implementación.</p>
<h3 id="relejos">Relejos</h3>
<blockquote>
<p>TODO: IOW está bien documentado. Podría sacar esto de ahí.</p>
</blockquote>
<h4 id="ley-de-snell">Ley de Snell</h4>
<h4 id="ecuaciones-de-fresnel">Ecuaciones de Fresnel</h4>
<h3 id="reflejos-especulares-perfectos">Reflejos especulares
perfectos</h3>
<p>https://graphicscodex.courses.nvidia.com/app.html?page=_rn_matrls
mirror reflections</p>
<h2 id="la-rendering-equation">La rendering equation</h2>
<p>Y, finalmente, tras esta introducción de los principales conceptos
radiométricos, llegamos a la ecuación más importante de todo este
trabajo: la <strong>rendering equation</strong>; también llamada la
<strong>ecuación del transporte de luz</strong>.</p>
<blockquote>
<p><strong>Nota</strong>(ción): esta vez no traduciré el concepto. Es
cierto que afea un poco la escritura teniendo en cuenta que esto es un
texto en castellano. Sin embargo, la otra opción es inventarme una
traducción que nadie usa.</p>
</blockquote>
<p>Antes de comenzar, volvamos a plantear de nuevo la situación: nos
encontramos observando desde nuestra pantalla una escena virtual
mediante la cámara. Queremos saber qué color tomará un pixel específico.
Para conseguirlo, dispararemos rayos desde nuestro punto de vista hacia
el entorno, haciendo que reboten en los objetos. Cuando un rayo impacte
en una superficie, adquirirá parte de las propiedades del material del
objeto. Además, de este rayo surgirán otros nuevos (un rayo dispersado y
otro refractado), que a su vez repetirán el proceso. La información que
se obtiene a partir de estos caminos de rayos nos permitirá darle color
al píxel.</p>
<p>La <em>rendering equation</em> se va a encargar de describir
analíticamente cómo ocurre esto.</p>
<p>Un último concepto más: denotemos por <span
class="math inline">\(L_e(p, \omega_o)\)</span> a <strong>la radiancia
producida por los materiales emisivos</strong>. Por ejemplo, una luz
emite radiancia por sí misma.</p>
<p>Bien, partamos de la ecuación de para la radiancia reflejada:</p>
<p><span class="math display">\[
L_o(p, \omega_o) = \int_{H^2(\mathbf{n})}{f(p, \omega_o \leftarrow
\omega_i) L_i(p, \omega_i) \cos\theta_i\ d\omega_i}
\]</span></p>
<p>Vamos a buscar expresar la radiancia incidente en términos de la
radiancia reflejada. Para ello, usamos la propiedad de que la radiancia
a lo largo de un rayo no cambia.</p>
<p>Si a una superficie le llega un fotón desde alguna parte, debe ser
porque <em>“alguien”</em> ha tenido que emitirlo. El fotón
necesariamente ha llegado a partir de un rayo. La propiedad nos dice que
la radiancia no ha podido cambiar en el camino.</p>
<p>Pues bien, consideremos una función <span class="math inline">\(r:
\mathbb{R}^3 \times \Omega \to \mathbb{R}^3\)</span> tal que, dado un
punto <span class="math inline">\(p\)</span> y una dirección <span
class="math inline">\(\omega\)</span>, devuelve el siguiente punto de
impacto en una superficie. En esencia, es una función de <em>ray
casting</em>.</p>
<blockquote>
<p>TODO: foto como la de
https://pellacini.di.uniroma1.it/teaching/graphics17b/lectures/12_pathtracing.pdf,
p.29</p>
</blockquote>
<p>Esta función nos permite expresar el punto anterior de la siguiente
forma:</p>
<p><span class="math display">\[
L_i(p, \omega) = L_o(r(p, \omega), -\omega)
\]</span></p>
<p>Esto nos permite cambiar la expresión de <span
class="math inline">\(L_i\)</span> en la integral anterior:</p>
<p><span class="math display">\[
L_o(p, \omega_o) = \int_{H^2(\mathbf{n})}{f(p, \omega_o \leftarrow
\omega_i) L_o(r(p, \omega_i), -\omega_i) \cos\theta_i\ d\omega_i}
\]</span></p>
<p>Finalmente, la radiancia total vendrá dada por la suma de la
radiancia emitida y la reflejada:</p>
<p><span id="eq:rendering_equation" class="eqnos"><span
class="math display">\[
L(p, \omega_o) = L_e(p, \omega_o) + \int_{H^2(\mathbf{n})}{f(p, \omega_o
\leftarrow \omega_i) L_o(r(p, \omega_i), -\omega_i) \cos\theta_i\
d\omega_i}
\]</span><span class="eqnos-number">(7)</span></span></p>
<p>Y con esto, ¡hemos obtenido la <em>rendering equation</em>!</p>
<p>Si quieres ver gráficamente cómo funciona, te recomiendo pasarte por
<span class="citation" data-cites="arneback-2019">(<a
href="#ref-arneback-2019" role="doc-biblioref">Arnebäck
n.d.</a>)</span>. Es un vídeo muy intuitivo.</p>
<iframe width="784" height="441" src="https://www.youtube-nocookie.com/embed/eo_MTI-d28s" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
<p>Si nos paramos a pensar, la ecuación de reflexión es muy similar a la
de renderizado. Sin embargo, hay un par de matices que las hacen muy
diferentes:</p>
<ul>
<li>La ecuación de reflexión describe cómo se comporta la luz reflejada
en un cierto punto. Es decir, tiene un ámbito local. Además, para
calcular la radiancia reflejada, se necesita conocer la radiancia
incidente.</li>
<li>La <em>rendering equation</em> calcula las condiciones globales de
la luz. Además, no se conocen las radiancias de salida.</li>
</ul>
<p>Este último matiz es importante. Para renderizar una imagen, se
necesita calcular la radiancia de salida para aquellos puntos visibles
desde nuestra cámara.</p>
<hr>
<h2 class="unlisted unnumbered" id="referencias-2">Referencias</h2>
<p><span class="citation" data-cites="PBRT3e">(<a href="#ref-PBRT3e"
role="doc-biblioref">Pharr, Jakob, and Humphreys 2016</a>)</span>, <span
class="citation" data-cites="wikipedia-contributors-2021D">(<a
href="#ref-wikipedia-contributors-2021D" role="doc-biblioref">Wikipedia:
Radiometry n.d.</a>)</span>, <span class="citation"
data-cites="studysession-2021">(<a href="#ref-studysession-2021"
role="doc-biblioref">StudySession n.d.</a>)</span>, <span
class="citation" data-cites="berkeley-cs184">(<a
href="#ref-berkeley-cs184" role="doc-biblioref">Berkeley cs184 n.d.</a>,
Radiometry &amp; Photometry)</span>, <span class="citation"
data-cites="wikipedia-funcion-de-distribucion-de-reflectancia-bidireccional-2022">(<a
href="#ref-wikipedia-funcion-de-distribucion-de-reflectancia-bidireccional-2022"
role="doc-biblioref">Wikipedia: Función de distribución de reflectancia
bidireccional n.d.</a>)</span>, <span class="citation"
data-cites="wikipedia-transmittance-2021">(<a
href="#ref-wikipedia-transmittance-2021" role="doc-biblioref">Wikipedia:
Transmittance n.d.</a>)</span></p>
<ul>
<li>https://matmatch.com/learn/property/isotropy-anisotropy</li>
<li>https://pellacini.di.uniroma1.it/teaching/graphics17b/lectures/12_pathtracing.pdf</li>
<li>https://alain.xyz/blog/advances-in-material-models</li>
<li>https://github.com/Corralx/BRDFExplorer</li>
</ul>
<h1 id="integración-de-monte-carlo">Integración de Monte Carlo</h1>
<p>Como vimos en el capítulo anterior, la clave para conseguir una
imagen en nuestro ray tracer es calcular la cantidad de luz en un punto
de la escena. Para ello, necesitamos hallar la radiancia en dicha
posición mediante la <em>rendering equation</em>. Sin embargo, es
<em>muy</em> difícil resolverla; tanto computacional como
analíticamente. Por ello, debemos atacar el problema desde otro punto de
vista.</p>
<p>Las técnicas de Monte Carlo nos permitirán aproximar el valor que
toman las integrales mediante una estimación. Utilizando muestreo
aleatorio para evaluar puntos de una función, seremos capaces de obtener
un resultado suficientemente bueno.</p>
<p>Una de las propiedades que hacen interesantes a este tipo de métodos
es la <strong>independencia del ratio de convergencia y la
dimensionalidad del integrando</strong>. Sin embargo, conseguir un mejor
rendimiento tiene un precio a pagar. Dadas <span
class="math inline">\(n\)</span> muestras, la convergencia a la solución
correcta tiene un orden de <span
class="math inline">\(\mathcal{O}\left(n^{-1/2}\right) =
\mathcal{O}\left(\frac{1}{\sqrt{n}}\right)\)</span>. Es decir, para
reducir el error a la mitad, necesitaríamos 4 veces más muestras.</p>
<p>En este capítulo veremos los fundamentos de la integración de Monte
Carlo, cómo muestrear distribuciones específicas y métodos para afinar
el resultado final.</p>
<h2 id="repaso-de-probabilidad">Repaso de probabilidad</h2>
<p>Antes de comenzar a fondo, necesitaremos unas nociones de variable
aleatoria para poder entender la integración de Monte Carlo, por lo que
vamos a hacer un breve repaso.</p>
<p>Una <strong>variable aleatoria</strong> <span
class="math inline">\(X\)</span> (v.a.) es, esencialmente, una regla que
asigna un valor numérico a cada posibilidad de un proceso de azar.
Formalmente, es una función definida en un espacio de probabilidad <span
class="math inline">\((\Omega, \mathcal{A}, P)\)</span> asociado a un
experimento aleatorio:</p>
<p><span class="math display">\[
X: \Omega \rightarrow \mathbb{R}
\]</span></p>
<p>A <span class="math inline">\(\Omega\)</span> lo conocemos como
espacio muestral (conjunto de todas las posibilidades), <span
class="math inline">\(\mathcal{A}\)</span> es una <span
class="math inline">\(\sigma\)</span>-álgebra de subconjuntos de <span
class="math inline">\(\Omega\)</span> que refleja todas las
posibilidades de eventos aleatorios, y <span
class="math inline">\(P\)</span> es una función probabilidad, que asigna
a cada evento una probabilidad.</p>
<p>Una variable aleatoria <span class="math inline">\(X\)</span> puede
clasificarse atendiendo a cómo sea su rango <span
class="math inline">\(R_X = \left\{x \in \mathbb{R} \,\middle|\, \exists
\omega \in \Omega \text{ tal que } X(\omega) = x \right\}\)</span>: en
discreta o continua.</p>
<h3 id="variables-aleatorias-discretas">Variables aleatorias
discretas</h3>
<p>Las v.a. discretas son aquellas cuyo rango es un conjunto
discreto.</p>
<p>Para comprender mejor cómo funcionan, pongamos un ejemplo:
Consideremos un experimento en el que lanzamos dos dados, anotando lo
que sale en cada uno. Los posibles valores que toman serán</p>
<p><span class="math display">\[
\begin{aligned}
\Omega = \{ &amp; (1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6),  \\
   &amp; (2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (2, 6),  \\
   &amp; (3, 1), (3, 2), (3, 3), (3, 4), (3, 5), (3, 6),  \\
   &amp; (4, 1), (4, 2), (4, 3), (4, 4), (4, 5), (4, 6),  \\
   &amp; (5, 1), (5, 2), (5, 3), (5, 4), (5, 5), (5, 6),  \\
   &amp; (6, 1), (6, 2), (6, 3), (6, 4), (6, 5), (6, 6)   \}
\end{aligned}
\]</span></p>
<p>Cada resultado tiene la misma probabilidad de ocurrir (claro está, si
el dado no está trucado). Como hay <span
class="math inline">\(36\)</span> posibilidades, la probabilidad de
obtener un cierto valor es de <span
class="math inline">\(\frac{1}{36}\)</span>.</p>
<p>La v.a. <span class="math inline">\(X\)</span> denotará la suma de
los valores obtenidos en cada uno. Así, por ejemplo, si al lanzar los
dados hemos obtenido <span class="math inline">\((1, 3)\)</span>, <span
class="math inline">\(X\)</span> tomará el valor <span
class="math inline">\(4\)</span>. En total, <span
class="math inline">\(X\)</span> puede tomar todos los valores
comprendidos entre <span class="math inline">\(2\)</span> y <span
class="math inline">\(12\)</span>. Cada pareja no está asociada a un
único valor de <span class="math inline">\(X\)</span>. Por ejemplo,
<span class="math inline">\((1, 2)\)</span> suma lo mismo que <span
class="math inline">\((2, 1)\)</span>. Esto nos lleva a preguntarnos…
¿Cuál es la probabilidad de que <span class="math inline">\(X\)</span>
adquiera un cierto valor?</p>
<p>La <strong>función masa de probabilidad</strong> nos permite conocer
la probabilidad de que <span class="math inline">\(X\)</span> tome un
cierto valor <span class="math inline">\(x\)</span>. Se denota por <span
class="math inline">\(P(X = x)\)</span>.</p>
<p>También se suele usar <span class="math inline">\(p_X(x)\)</span> o,
directamente <span class="math inline">\(p(x)\)</span>, cuando no haya
lugar a dudas. Sin embargo, en este trabajo reservaremos este nombre a
otro tipo de funciones.</p>
<blockquote>
<p><strong>Nota</strong>(ción): Cuando <span
class="math inline">\(X\)</span> tenga una cierta función masa de
probabilidad, escribiremos <span class="math inline">\(X \sim
p_X\)</span></p>
</blockquote>
<p>En este ejemplo, la probabilidad de que <span
class="math inline">\(X\)</span> tome el valor <span
class="math inline">\(4\)</span> es</p>
<p><span class="math display">\[
\begin{aligned}
P(X = 4) &amp; = \sum{\small{\text{nº parejas que suman 4}} \cdot
\small{\text{probabilidad de que salga la pareja}}} \\
         &amp; = 3 \cdot \frac{1}{36} = \frac{1}{12}
\end{aligned}
\]</span></p>
<p>Las parejas serían <span class="math inline">\((1, 3), (2,
2)\)</span> y <span class="math inline">\((3, 1)\)</span>.</p>
<p>Por definición, si el conjunto de valores que puede tomar <span
class="math inline">\(X\)</span> es <span class="math inline">\(\{x_1,
\dots, x_n\}\)</span>, la función masa de probabilidad debe cumplir
que</p>
<p><span class="math display">\[
\sum_{i = 1}^{n}{P(X = x_i)} = 1
\]</span></p>
<p>Muchas veces nos interesará conocer la probabilidad de que <span
class="math inline">\(X\)</span> se quede por debajo de un cierto valor
<span class="math inline">\(x\)</span> (de hecho, podemos caracterizar
distribuciones aleatorias gracias a esto). Para ello, usamos la
<strong>función de distribución</strong>:</p>
<p><span class="math display">\[
F_X(x) = P(X \le x) = \sum_{\substack{k \in \mathbb{R} \\ k \le x}}{P(X
= k)}
\]</span></p>
<p>Es una función continua por la derecha y monótona no decreciente.
Además, se cumple que <span class="math inline">\(0 \le F_X \le
1(x)\)</span> y <span class="math inline">\(\lim_{x \to -\infty}{F_X} =
0\)</span>, <span class="math inline">\(\lim_{x \to \infty}{F_X} =
1\)</span>.</p>
<p>En nuestro ejemplo, si consideramos <span class="math inline">\(x =
3\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
F_X(x) &amp; = \sum_{i = 1}^{3}{P(X = i)} = P(X = 1) + P(X = 2) + P(X =
3) \\
       &amp; = \frac{1}{36} + \frac{2}{36} + \frac{3}{36} = \frac{1}{12}
\end{aligned}
\]</span></p>
<h3 id="variables-aleatorias-continuas">Variables aleatorias
continuas</h3>
<p>Este tipo de variables aleatorias tienen un rango no numerable; es
decir, el conjunto de valores que puede tomar abarca un intervalo de
números.</p>
<p>Un ejemplo podría ser la altura de una persona.</p>
<p>Si en las variables aleatorias discretas teníamos funciones masa de
probabilidad, aquí definiremos las <strong>funciones de densidad de
probabilidad</strong> (o simplemente, funciones de densidad). La idea es
la misma: nos permite conocer la probabilidad de que nuestra variable
aleatoria tome un cierto valor del espacio muestral.</p>
<p>Es importante mencionar que, aunque <em>la probabilidad de que la
variable aleatoria tome un valor específico</em> es <span
class="math inline">\(0\)</span>, ya que nos encontramos en un conjunto
no numerable, sí que podemos calcular la probabilidad de que se
encuentre entre dos valores. Por tanto, si la función de densidad es
<span class="math inline">\(f_X\)</span>, entonces</p>
<p><span class="math display">\[
P(a \le X \le b) = \int_{a}^{b}{f_X(x)dx}
\]</span></p>
<p>La función de densidad tiene dos características importantes:</p>
<ol type="1">
<li><span class="math inline">\(f_X\)</span> es no negativa; esto es,
<span class="math inline">\(f_X(x) \ge 0\ \forall x \in
\mathbb{R}\)</span></li>
<li><span class="math inline">\(f_X\)</span> integra uno en todo <span
class="math inline">\(\mathbb{R}\)</span>:</li>
</ol>
<p><span class="math display">\[
\int_{-\infty}^{\infty}{f_X(x)} = 1
\]</span></p>
<p>Estas dos propiedades caracterizan a una función de densidad; es
decir, toda función <span class="math inline">\(f: \mathbb{R}
\rightarrow \mathbb{R}\)</span> no negativa e integrable tal que <span
class="math inline">\(\int_{\infty}^{\infty}{f_X(x)} = 1\)</span> es la
función de densidad de alguna variable continua.</p>
<p>Intuitivamente, podemos ver esta última propiedad como <em>si
acumulamos todos los valores que puede tomar la variable aleatoria, la
probabilidad de que te encuentres en el conjunto debe ser 1</em>. Si
tratamos con un conjunto de números reales, podemos escribir la integral
como <span class="math inline">\(\int_{-\infty}^{\infty}{f_X(x)} =
1\)</span>.</p>
<p>Una de las variables aleatorias que más juego nos darán en el futuro
será la <strong>v.a. con distribución uniforme en <span
class="math inline">\([0, 1)\)</span></strong>. La denotaremos <span
class="math inline">\(\Xi \sim U\left([0, 1)\right)\)</span>. La
probabilidad de que <span class="math inline">\(\xi\)</span> tome un
valor es constante, por lo que podemos definir su función de densidad
como</p>
<p><span class="math display">\[
f_\Xi(\xi) = \left\{  \begin{array}{llc}
                  1 &amp; \text{si } \xi \in [0, 1) \\
                  0 &amp; \text{en otro caso.}
                  \end{array}
         \right.
\]</span></p>
<p>La probabilidad de <span class="math inline">\(\Xi\)</span> tome un
valor entre dos elementos <span class="math inline">\(a, b \in [0,
1)\)</span> es</p>
<p><span class="math display">\[
P(\Xi \in [a, b]) = \int_{a}^{b}{1dx} = b - a
\]</span></p>
<p>Como veremos más adelante, definiendo correctamente una función de
densidad conseguiremos mejorar el rendimiento del path tracer.</p>
<p>La función de distribución <span
class="math inline">\(F_X(x)\)</span> podemos definirla como:</p>
<p><span class="math display">\[
F_X(x) = P(X \le x) = \int_{-\infty}^{x}{f_X(t)dt}
\]</span></p>
<p>Es decir, dado un <span class="math inline">\(x\)</span>, ¿cuál sería
la probabilidad de que <span class="math inline">\(X\)</span> se quede
por debajo de <span class="math inline">\(x\)</span>?</p>
<p>El Teorema Fundamental del Cálculo nos permite relacionar función de
distribución y función de densidad directamente:</p>
<p><span class="math display">\[
f_X(x) = \frac{dF_X(x)}{dx}
\]</span></p>
<h3 id="esperanza-y-varianza-de-una-variable-aleatoria">Esperanza y
varianza de una variable aleatoria</h3>
<p>La <strong>esperanza de una variable aleatoria</strong>, denotada
<span class="math inline">\(E[X]\)</span>, es una generalización de la
media ponderada. Nos informa del <em>valor esperado</em> de dicha
variable aleatoria.</p>
<p>En el caso de las variables discretas, se define como</p>
<p><span class="math display">\[
E[X] = \sum_{i}{x_i p_i}
\]</span></p>
<p>donde <span class="math inline">\(x_i\)</span> son los posibles
valores que puede tomar la v.a., y <span
class="math inline">\(p_i\)</span> la probabilidad asociada a cada uno
de ellos; es decir, <span class="math inline">\(p_i = P[X =
x_i]\)</span></p>
<p>Para una variable aleatoria continua real, la esperanza viene dada
por</p>
<p><span class="math display">\[
E[X] = \int_{-\infty}^{\infty}{x f_X(x) dx}
\]</span></p>
<p>Pongamos un par de ejemplos del cálculo de la esperanza. En el <a
href="#variables-aleatorias-discretas">ejemplo de las variables
discretas</a>, la esperanza venía dada por</p>
<p><span class="math display">\[
E[X] = \sum_{i = 2}^{12}{i P[X = i]} = 2\frac{1}{36} + 3 \frac{2}{36} +
\dots + 12 \frac{1}{36} = 7
\]</span></p>
<p>Para variables aleatorias uniformes en <span
class="math inline">\((a, b)\)</span> (es decir, <span
class="math inline">\(X \sim U(a, b)\)</span>), la esperanza es</p>
<p><span class="math display">\[
E[X] = \int_{a}^{b}{x \frac{1}{b - a}dx} = \frac{a + b}{2}
\]</span></p>
<p>La esperanza tiene unas cuantas propiedades que nos resultarán muy
útiles. Estas son:</p>
<ul>
<li><strong>Linealidad</strong>:
<ul>
<li>Si <span class="math inline">\(X, Y\)</span> son dos v.a., <span
class="math inline">\(E[X + Y] = E[X] + E[Y]\)</span></li>
<li>Si <span class="math inline">\(a\)</span> es una constante, <span
class="math inline">\(X\)</span> una v.a., entonces <span
class="math inline">\(E[aX] = aE[X]\)</span></li>
<li>Análogamente, para ciertas <span class="math inline">\(X_1, \dots,
X_k\)</span>, <span class="math inline">\(E\left[\sum_{i =
1}^{k}{X_i}\right] = \sum_{i = 1}^{k}{E[X_i]}\)</span></li>
<li>Estas propiedades no necesitan que las variables aleatorias sean
independientes. Este hecho será clave para las técnicas de Monte
Carlo.</li>
</ul></li>
<li>La <strong>Ley del estadístico insconciente</strong> (<em>Law of the
unconscious statistician</em>, o LOTUS): dada una variable aleatoria
<span class="math inline">\(X\)</span> y una función medible <span
class="math inline">\(g\)</span>, la esperanza de <span
class="math inline">\(g(X)\)</span> se puede calcular como</li>
</ul>
<p><span class="math display">\[
E[g(X)] = \int_{-\infty}^{\infty}{g(x) f_X(x) dx}
\]</span></p>
<ul>
<li>La <strong>Ley (fuerte) de los grandes números</strong> nos dice que
dada una muestra de <span class="math inline">\(n\)</span> valores <span
class="math inline">\(X_1, \dots, X_N\)</span> de una variable aleatoria
<span class="math inline">\(X\)</span> con esperanza <span
class="math inline">\(E[X] = \mu\)</span>,</li>
</ul>
<p><span class="math display">\[
P\left[\lim_{N \to \infty}{\frac{1}{n} \sum_{i = 1}^{N}{X_i}} = \mu
\right] = 1
\]</span></p>
<p>Usando que <span class="math inline">\(\bar{X}_N = \frac{1}{N}
\sum_{i = 1}^{N}{X_i}\)</span>, esta ley se suele escribir como</p>
<p><span class="math display">\[
P\left[\lim_{N \to \infty}{\bar{X}_N} = \mu \right] = 1
\]</span></p>
<p>Estas dos últimas propiedades resultarán claves en el desarrollo.</p>
<p>Será habitual encontrarnos con el problema de que no conocemos la
distribución de una variable aleatoria <span
class="math inline">\(Y\)</span>. Sin embargo, si encontramos una
transformación medible de una variable aleatoria <span
class="math inline">\(X\)</span> de forma que obtengamos <span
class="math inline">\(Y\)</span> (esto es, <span
class="math inline">\(\exists g\)</span> función medible tal que <span
class="math inline">\(g(X) = Y\)</span>), entonces podemos calcular la
esperanza de <span class="math inline">\(Y\)</span> fácilmente. Esta
propiedad hará que las variables aleatorias con distribución uniforme
adquieran muchísima importancia. Generar números aleatorios en <span
class="math inline">\([0, 1)\)</span> es muy fácil, así <a
href="#método-de-la-transformada-inversa">que obtendremos otras vv.aa a
partir de <span class="math inline">\(\xi\)</span></a>.</p>
<p>Otra medida muy útil de una variable aleatoria es <strong>la
varianza</strong>. Nos permitirá medir cómo de dispersa es la
distribución con respecto a su media. La denotaremos como <span
class="math inline">\(Var[X]\)</span>, y se define como</p>
<p><span class="math display">\[
Var[X] = E\left[(X - E[X])^2\right]
\]</span></p>
<p>Si desarrollamos esta definición, podemos conseguir una expresión
algo más agradable:</p>
<p><span class="math display">\[
\begin{aligned}
   Var[X] &amp; = E\left[(X - E[X])^2\right] = \\
          &amp; = E\left[X^2 + E[X]^2 - 2XE[X]\right] = \\
          &amp; = E\left[X^2\right] + E[X]^2 - 2E[X]E[X] = \\
          &amp; = E\left[X^2\right] - E\left[X\right]^2
\end{aligned}
\]</span></p>
<p>Hemos usado que <span class="math inline">\(E[E[X]] = E[X]\)</span> y
la linealidad de la esperanza.</p>
<p>Enunciemos un par de propiedades que tiene, similares a la de la
esperanza:</p>
<ul>
<li>La varianza saca constantes al cuadrado: <span
class="math inline">\(Var[aX] = a^2Var[X]\)</span></li>
<li><span class="math inline">\(Var[X + Y] =\)</span> <span
class="math inline">\(Var[X] + Var[Y] + 2Cov[X, Y]\)</span>, donde <span
class="math inline">\(Cov[X, Y]\)</span> es la covarianza de <span
class="math inline">\(X\)</span> y <span
class="math inline">\(Y\)</span>.
<ul>
<li>En el caso en el que <span class="math inline">\(X\)</span> e <span
class="math inline">\(Y\)</span> sean incorreladas (es decir, la
covarianza es <span class="math inline">\(0\)</span>), <span
class="math inline">\(Var[X + Y] =\)</span> <span
class="math inline">\(Var[X] + Var[Y]\)</span>.</li>
</ul></li>
</ul>
<p>La varianza nos será útil a la hora de medir el error cometido por
una estimación de Monte Carlo.</p>
<h3 id="estimadores">Estimadores</h3>
<p>A veces, no podremos conocer de antemano el valor que toma un cierto
parámetro de una distribución. Sin embargo, conocemos el tipo de
distribución que nuestra variable aleatoria <span
class="math inline">\(X\)</span> sigue. Los estimadores nos
proporcionarán una forma de calcular el posible valor de esos parámetros
a partir de una muestra de <span class="math inline">\(X\)</span>.</p>
<p>Sea <span class="math inline">\(X\)</span> una variablea aleatoria
con distribución perteneciente a una familia de distribuciones
paramétricas <span class="math inline">\(X \sim F \in \left\{F(\theta)
\,\middle|\, \theta \in \Theta \right\}\)</span>. <span
class="math inline">\(\Theta\)</span> es el conjunto de valores que
puede tomar el parámetro. Buscamos una forma de determinar el valor de
<span class="math inline">\(\theta\)</span>.</p>
<p>Diremos que <span class="math inline">\(T(X_1, \dots, X_N)\)</span>
es <strong>un estimador de <span
class="math inline">\(\theta\)</span></strong> si <span
class="math inline">\(T\)</span> toma valores en <span
class="math inline">\(\Theta\)</span>.</p>
<p>A los estimadores de un parámetro los solemos denotar con <span
class="math inline">\(\hat{\theta}\)</span>.</p>
<p>Como vemos, la definición no es muy restrictiva. Únicamente le
estamos pidiendo a la función de la muestra que pueda tomar valores
viables para la distribución.</p>
<p>Se dice que un estimador <span class="math inline">\(T(X_1, \dots,
X_N)\)</span> es <strong>insesgado</strong> (o centrado en el parámetro
<span class="math inline">\(\theta\)</span>) si</p>
<p><span class="math display">\[
E[T(X_1, \dots, X_n)] = \theta\quad \forall \theta \in \Theta
\]</span></p>
<p>Naturalmente, decimos que un estimador <span
class="math inline">\(T(X_1, \dots, X_N)\)</span> está
<strong>sesgado</strong> si <span class="math inline">\(E[T(X_1, \dots,
X_N] \not = \theta\)</span>.</p>
<h2 id="el-estimador-de-monte-carlo">El estimador de Monte Carlo</h2>
<p>Tras este breve repaso de probabilidad, estamos en condiciones de
definir el estimador de Monte Carlo. Primero, vamos con su versión más
sencilla.</p>
<p>Los estimadores de Monte Carlo nos permiten hallar la esperanza de
una variable aleatoria, digamos, <span class="math inline">\(Y\)</span>,
sin necesidad de calcular explícitamente su valor. Para ello, tomamos
unas cuantas muestras <span class="math inline">\(Y_1, \dots,
Y_N\)</span> que sigan la misma distribución que <span
class="math inline">\(Y\)</span> con media <span
class="math inline">\(\mu\)</span>. Entonces, consideramos el estimador
de <span class="math inline">\(\mu\)</span> <span class="citation"
data-cites="mcbook">(<a href="#ref-mcbook" role="doc-biblioref">Owen
2013</a>)</span>:</p>
<p><span id="eq:mc_simple" class="eqnos"><span class="math display">\[
\hat\mu_N = \frac{1}{N} \sum_{i = 1}^{N}{Y_i}
\]</span><span class="eqnos-number">(8)</span></span></p>
<p>Haciendo la esperanza de este estimador, vemos que</p>
<p><span class="math display">\[
\begin{aligned}
E[\hat\mu_N] &amp; = E\left[\frac{1}{N} \sum_{i = 1}^{N}{Y_i}\right] =
\frac{1}{N} E\left[\sum_{i = 1}^{N}{Y_i}\right] \\
             &amp; = \frac{1}{N} \sum_{i = 1}^{N}{E\left[Y_i\right]} =
\frac{1}{N} \sum_{i = 1}^{N}{\mu} = \\
             &amp; = \mu
\end{aligned}
\]</span></p>
<p>Por lo que el estimador es insesgado.</p>
<p>Generalmente nos encontraremos en la situación en la que <span
class="math inline">\(Y = f(X)\)</span>, donde <span
class="math inline">\(X\)</span> sigue una distribución con función de
densidad <span class="math inline">\(p_X(x)\)</span>, y <span
class="math inline">\(f: S \rightarrow \mathbb{R}\)</span>. En ese caso,
sabemos que la esperanza de <span class="math inline">\(Y\)</span> se
puede calcular como</p>
<p><span class="math display">\[
\mu = E[Y] = E[f(X)] = \int_{S}{f(x)p_X(x)dx}
\]</span></p>
<p>Lo que estamos buscando es calcular <span
class="math inline">\(\int_{S}{f(x)dx}\)</span>. Entonces, ¿qué ocurre
si intentamos compensar en [<a href="#eq:mc_simple">8</a>] con la
función de densidad?</p>
<p><span class="math display">\[
\begin{aligned}
&amp; E\left[\frac{1}{N} \sum_{i =
1}^{N}{\frac{f(X_i)}{p_X(X_i)}}\right] = \frac{1}{N} \sum_{i =
1}^{N}{E\left[\frac{f(X_i)}{p_X(X_i)}\right]} = \\
&amp; = \frac{1}{N} \sum_{i =
1}^{N}{\left(\int_{S}{\frac{f(x)}{p_X(x)}p_X(x)dx}\right)} = \\
&amp; = \frac{1}{N} N \int_{S}{f(x)dx} = \\
&amp; = \int_{S}{f(x)dx}
\end{aligned}
\]</span></p>
<p>¡Genial! Esto nos da una forma de calcular la integral de una función
usando muestras de variables aleatorias con cierta distribución.
Llamaremos al estimador de Monte Carlo</p>
<p><span id="eq:mc_integral" class="eqnos"><span class="math display">\[
\hat{I}_N = \frac{1}{N} \sum_{i = 1}^{N}{\frac{f(X_i)}{p_X(X_i)}}
\]</span><span class="eqnos-number">(9)</span></span></p>
<p>Es importante mencionar que <span
class="math inline">\(p_X(x)\)</span> debe ser distinto de 0 cuando
<span class="math inline">\(f\)</span> también lo sea.</p>
<blockquote>
<p><strong>Nota</strong>(ción): si te preguntas por qué lo llamamos
<span class="math inline">\(\hat{I}_N\)</span>, piensa que queremos
calcular la intergal <span class="math inline">\(I =
\int_{S}{f(x)dx}\)</span>. Para ello, usamos el estimador <span
class="math inline">\(\hat{I}\)</span>, y marcamos explícitamente que
usamos <span class="math inline">\(N\)</span> muestras.</p>
</blockquote>
<p>Podemos particularizar el caso en el que nuestras muestras <span
class="math inline">\(X_i\)</span> sigan una distribución uniforme en
<span class="math inline">\([a, b]\)</span>. Si eso ocurre, su función
de densidad es <span class="math inline">\(p_X(x) = \frac{1}{b -
a}\)</span>, así que podemos simplificar un poco [<a
href="#eq:mc_integral">9</a>]:</p>
<p><span class="math display">\[
\hat{I}_N = \frac{b - a}{N} \sum_{i = 1}^{N}{f(X_i)}
\]</span></p>
<p>Elegir correctamente la función de densidad <span
class="math inline">\(p_X\)</span> será clave. Si conseguimos escogerla
debidamente, reduciremos en gran medida el error que genera el
estimador. Esto es lo que se conoce como <a
href="#importance-sampling"><em>importance sampling</em></a>.</p>
<p>Puesto que la varianza del estimador nos dará información sobre el
error que genera, vamos a calcular <span
class="math inline">\(Var[\hat{I}_N]\)</span>. Para ello, usamos las
propiedades que vimos en la <a
href="#esperanza-y-varianza-de-una-variable-aleatoria">sección
anterior</a>:</p>
<p><span id="eq:mc_varianza" class="eqnos"><span class="math display">\[
\begin{aligned}
  Var[\hat{I}_N]
    &amp; = Var\left[\frac{1}{N} \sum_{i =
1}^{N}{\frac{f(X_i)}{p_X(X_i)}}\right] = \\
    &amp; = \frac{1}{N^2} Var\left[ \sum_{i =
1}^{N}{\frac{f(X_i)}{p_X(X_i)}} \right] = \\
    &amp; = \frac{1}{N^2} N Var\left[\frac{f(X)}{p_X(X)}\right] = \\
    &amp; = \frac{1}{N} Var\left[\frac{f(X)}{p_X(X)}\right]
\end{aligned}
\]</span><span class="eqnos-number">(10)</span></span></p>
<p>es decir, la varianza del estimador es inversamente proporcional al
número de muestras <span class="math inline">\(N\)</span>.</p>
<p>La desviación estándar es <span class="math display">\[
\sqrt{Var[\hat{I}_N]} =
\frac{\sqrt{Var\left[\frac{f(X)}{p_X(X)}\right]}}{\sqrt{N}}
\]</span></p>
<p>así que, como adelantamos al inicio del capítulo, la estimación tiene
un error del orden <span
class="math inline">\(\mathcal{O}(N^{-1/2})\)</span>. Esto nos dice que,
para reducir el error a la mitad, debemos tomar 4 veces más
muestras.</p>
<p>Pongamos un ejemplo de estimador de Monte Carlo para una caja de
dimensiones <span class="math inline">\(\small{[x_0, x_1] \times [y_0,
y_1] \times [z_0, z_1]}\)</span>. Si queremos estimar la integral de la
función <span class="math inline">\(f: \mathbb{R}^3 \rightarrow
\mathbb{R}\)</span></p>
<p><span class="math display">\[
\int_{x_0}^{x_1} \int_{y_0}^{y_1} \int_{z_0}^{z_1}{f(x, y, z)dx dy dz}
\]</span></p>
<p>mediante una variable aleatoria <span class="math inline">\(X \sim
U(\small{[x_0, x_1] \times [y_0, y_1] \times [z_0, z_1]})\)</span> con
función de densidad <span class="math inline">\(p(x, y, z) =
\frac{1}{x_1 - x_0} \frac{1}{y_1 - y_0} \frac{1}{z_1 - z_0}\)</span>,
tomamos el estimador</p>
<p><span class="math display">\[
\hat{I}_N = \frac{1}{(x_1 - x_0) \cdot (y_1 - y_0) \cdot (z_1 - z_0)}
\sum_{i = 1}^{N}{f(X_i)}
\]</span></p>
<p>Otro ejemplo clásico de estimador de Monte Carlo es calcular el valor
de <span class="math inline">\(\pi\)</span>. Se puede hallar integrando
una función que valga <span class="math inline">\(1\)</span> en el
interior de la circunferencia de radio unidad y <span
class="math inline">\(0\)</span> en el exterior:</p>
<p><span class="math display">\[
\begin{aligned}
f = \begin{cases}
      1 &amp; \text{si } x^2 + y^2 \le 1 \\
      0 &amp; \text{en otro caso}
    \end{cases} \Longrightarrow \pi = \int_{-1}^{1} \int_{-1}^{1}{f(x,
y)}\ dxdy
\end{aligned}
\]</span></p>
<p>Para usar el estimador de [<a href="#eq:mc_integral">9</a>],
necesitamos saber la probabilidad de obtener un punto dentro de la
circunferencia.</p>
<p>Bien, consideremos que una circunferencia de radio <span
class="math inline">\(r\)</span> se encuentra inscrita en un cuadrado.
El área de la circunferencia es <span class="math inline">\(\pi
r^2\)</span>, mientras que la del cuadrado es <span
class="math inline">\((2r)^2 = 4r^2\)</span>. Por tanto, la probabilidad
de obtener un punto dentro de la circunferencia es <span
class="math inline">\(\frac{\pi r^2}{4r^2} = \frac{\pi}{4}\)</span>.
Podemos tomar <span class="math inline">\(p(x, y) =
\frac{1}{4}\)</span>, de forma que</p>
<p><span class="math display">\[
\pi \approx \frac{4}{N} \sum_{i = 1}^{N}{f(x_i, y_i)}, \text{  con }
(x_i, y_i) \sim U(\small{[-1, 1] \times [-1, 1]})
\]</span></p>
<h2 id="importance-sampling">Importance sampling</h2>
<p>Si recordamos la varianza del estimador de Monte Carlo [<a
href="#eq:mc_varianza">10</a>],</p>
<p><span class="math display">\[
Var[\hat{I}_N] = \frac{1}{N} Var\left[\frac{f(X)}{p_X(X)}\right]
\]</span></p>
<p>podemos ver que depende de dos factores: el número de muestras <span
class="math inline">\(N\)</span> y la varianza de <span
class="math inline">\(Var\left[\frac{f(X)}{p_X(X)}\right]\)</span>.
Aumentar el número de muestras haría que la varianza decrezca. Sin
embargo, alcanzaríamos un punto de retornos reducidos. Por tanto, vamos
a centrarnos ahora en el segundo término.</p>
<p>En esencia, la varianza de <span
class="math inline">\(Var\left[\frac{f(X)}{p_X(X)}\right]\)</span>
decrecerá cuanto más cercana sea la función de probabilidad <span
class="math inline">\(p_X\)</span> a la función <span
class="math inline">\(f(X)\)</span>.</p>
<p>Supongamos que <span class="math inline">\(f\)</span> es proporcional
a <span class="math inline">\(p_X\)</span>. Esto es, existe un <span
class="math inline">\(s\)</span> tal que <span
class="math inline">\(f(x) = s p_X(x)\)</span>. Como <span
class="math inline">\(p_X\)</span> debe integrar uno, podemos calcular
el valor de <span class="math inline">\(s\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
  \int_{S}{p_X(x)dx} &amp; = \int_{S}{sf(x)dx} = 1 \quad \iff \\
  s &amp; = \frac{1}{\int_{S}{f(x)dx}}
\end{aligned}
\]</span></p>
<p>Y entonces, se tendría que</p>
<p><span class="math display">\[
\begin{aligned}
  Var\left[\frac{f(X)}{p_X(X)}\right] &amp; =
Var\left[\frac{f(X)}{sf(X)}\right] = \\
  &amp; = Var\left[\frac{1}{s}\right] = \\
  &amp; = 0
\end{aligned}
\]</span></p>
<p>En la práctica, esto es inviable. El problema que queremos resolver
es calcular la integral de <span class="math inline">\(f\)</span>. Y
para sacar <span class="math inline">\(s\)</span>, necesitaríamos el
valor de la integral de <span class="math inline">\(f\)</span>. ¡Estamos
dando vueltas!</p>
<p>Por fortuna, hay algoritmos que son capaces de proporcionar la
constante <span class="math inline">\(s\)</span> sin necesidad de
calcular la integral. Uno de los más conocidos es
<strong>Metropolis-Hastings</strong>, el cual se basa en cadenas de
Markov de Monte Carlo.</p>
<p>En este trabajo nos centraremos en buscar funciones de densidad <span
class="math inline">\(p_X\)</span> que se aproximen a <span
class="math inline">\(f\)</span> lo más fielmente posible, dentro del
contexto del transporte de luz.</p>
<h2 id="multiple-importance-sampling">Multiple importance sampling</h2>
<p>https://graphics.stanford.edu/courses/cs348b-03/papers/veach-chapter9.pdf</p>
<h2 id="escogiendo-puntos-aleatorios">Escogiendo puntos aleatorios</h2>
<p>Una de las partes clave del estimador de Monte Carlo [<a
href="#eq:mc_integral">9</a>] es saber escoger la función de densidad
<span class="math inline">\(p_X\)</span> correctamente. En esta sección,
veremos algunos métodos para conseguir distribuciones específicas
partiendo de funciones de densidad sencillas, así como formas de elegir
funciones de densidad próximas a <span
class="math inline">\(f\)</span>.</p>
<h3 id="método-de-la-transformada-inversa">Método de la transformada
inversa</h3>
<blockquote>
<p><strong>En resumen</strong>: Para conseguir una muestra de una
distribución específica <span class="math inline">\(F_X\)</span>:</p>
<ol type="1">
<li>Generar un número aleatorio <span class="math inline">\(\xi \sim
U(0, 1)\)</span>.</li>
<li>Hallar la inversa de la función de distribución deseada <span
class="math inline">\(F_X\)</span>, denotada <span
class="math inline">\(F_X^{-1}(x)\)</span>.</li>
<li>Calcular <span class="math inline">\(F_X^{-1}(\xi) =
X\)</span>.</li>
</ol>
</blockquote>
<p>Este método nos permite conseguir muestras de cualquier distribución
continua a partir de variables aleatorias uniformes, siempre que se
conozca la inversa de la función de distribución.</p>
<p>Sea <span class="math inline">\(X\)</span> una variable aleatoria con
función de distribución <span class="math inline">\(F_X\)</span><a
href="#fn3" class="footnote-ref" id="fnref3"
role="doc-noteref"><sup>3</sup></a>. Queremos buscar una transformación
<span class="math inline">\(T: [0, 1] \rightarrow \mathbb{R}\)</span>
tal que <span class="math inline">\(T(\xi) \stackrel{\text{\small d}}{=}
X\)</span>, siendo <span class="math inline">\(\xi\)</span> una v.a.
uniformemente distribuida. Para que esto se cumpla, se debe dar</p>
<p><span class="math display">\[
\begin{aligned}
F_X(x) &amp; = P[X &lt; x] = \\
       &amp; = P[T(\xi) &lt; x] = \\
       &amp; = P(\xi &lt; T^{-1}(x)) = \\
       &amp; = T^{-1}(x)
\end{aligned}
\]</span></p>
<p>Este último paso se debe a que, como <span
class="math inline">\(\xi\)</span> es uniforme en <span
class="math inline">\((0, 1)\)</span>, <span class="math inline">\(P[\xi
&lt; x] = x\)</span>. Es decir, hemos obtenido que <span
class="math inline">\(F_X\)</span> es la inversa de <span
class="math inline">\(T\)</span>.</p>
<blockquote>
<p>TODO: dibujo similar a <a
href="https://cs184.eecs.berkeley.edu/public/sp22/lectures/lec-12-monte-carlo-integration/lec-12-monte-carlo-integration.pdf">este:
p.52</a></p>
</blockquote>
<p>Como ejemplo, vamos a muestrear la función <span
class="math inline">\(f(x) = x^2,\ x \in [0, 2]\)</span>.</p>
<p>Primero, normalizamos esta función para obtener una función de
densidad <span class="math inline">\(p_X(x)\)</span>. Es decir, buscamos
<span class="math inline">\(p_X(x) = c f(x)\)</span> tal que</p>
<p><span class="math display">\[
\begin{aligned}
1 &amp; = \int_{0}^{2}{p_X(x)dx} = \int_{0}^{2}{c f(x)dx} = c
\int_{0}^{2}{f(x)dx} = \\
  &amp; = \left.\frac{cx^3}{3}\right\rvert_{2}^{3} = \frac{8c}{3} \\
  &amp; \Rightarrow c = \frac{3}{8} \\
  &amp; \Rightarrow p_X(x) = \frac{3x^2}{8}
\end{aligned}
\]</span></p>
<p>A continuación, integramos la función de densidad para obtener la de
distribución <span class="math inline">\(F_X\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
F_X(x) = \int_{0}^{x}{p_X(x)dx} = \int_{0}^{x}{\frac{3x^2}{8}} =
\frac{x^3}{8}
\end{aligned}
\]</span></p>
<p>Solo nos queda conseguir la muestra. Para ello,</p>
<p><span class="math display">\[
\begin{aligned}
\xi &amp; = F_X(x)  = \frac{x^3}{8} \quad \iff \\
x &amp; = \sqrt[3]{8 \xi}
\end{aligned}
\]</span></p>
<p>Sacando un número aleatorio <span class="math inline">\(\xi\)</span>,
y pasándolo por la función obtenida, conseguimos un elemento con
distribución <span class="math inline">\(F(x)\)</span>.</p>
<h3 id="método-del-rechazo">Método del rechazo</h3>
<blockquote>
<p><strong>En resumen</strong>: Para conseguir una muestra de una
variable aleatoria <span class="math inline">\(X\)</span> con función de
densidad <span class="math inline">\(p_X\)</span>:</p>
<ol type="1">
<li>Obtener una muestra <span class="math inline">\(y\)</span> de <span
class="math inline">\(Y\)</span> , y otra <span
class="math inline">\(\xi\)</span> de <span class="math inline">\(U(0,
1)\)</span>.</li>
<li>Comprobar si <span class="math inline">\(\xi &lt;
\frac{p_X(y)}{Mp_Y(y)}\)</span>. Si es así, aceptarla. Si no, sacar otra
muestra.</li>
</ol>
</blockquote>
<p>El método anterior presenta principalmente dos problemas:</p>
<ol type="1">
<li>No siempre es posible integrar una función para hallar su función de
densidad.</li>
<li>La inversa de la función de distribución, <span
class="math inline">\(F_X^{-1}\)</span> no tiene por qué existir.</li>
</ol>
<p>Como alternativa, podemos usar este método (en inglés, <em>rejection
method</em>). Para ello, necesitamos una variable aleatoria <span
class="math inline">\(Y\)</span> con función de densidad <span
class="math inline">\(p_Y(y)\)</span>. El objetivo es conseguir una
muestra de <span class="math inline">\(X\)</span> con función de
densidad <span class="math inline">\(p_X(x)\)</span>.</p>
<p>La idea principal es aceptar una muestra de <span
class="math inline">\(Y\)</span> con probabilidad <span
class="math inline">\(p_X/Mp_Y\)</span>, con <span
class="math inline">\(1 &lt; M &lt; \infty\)</span>. En esencia, estamos
jugando a los dardos: si la muestra de <span
class="math inline">\(y\)</span> que hemos obtenido se queda por debajo
de la gráfica de la función <span class="math inline">\(Mp_Y &lt;
p_X\)</span>, estaremos obteniendo una de <span
class="math inline">\(p_X\)</span>.</p>
<blockquote>
<p>TODO dibujo de la gráfica <span
class="math inline">\(\frac{p_X(y)}{Mp_Y(y)}\)</span>.</p>
<p>¿Quizás haga falta una demostración también? No estoy satisfecho con
este apartado ahora mismo. Necesita trabajo.</p>
</blockquote>
<p>El algoritmo consiste en:</p>
<ol type="1">
<li>Obtener una muestra de <span class="math inline">\(Y\)</span>,
denotada <span class="math inline">\(y\)</span>, y otra de <span
class="math inline">\(U(0, 1)\)</span>, llamada <span
class="math inline">\(\xi\)</span>.</li>
<li>Comprobar si <span class="math inline">\(\xi &lt;
\frac{p_X(y)}{Mp_Y(y)}\)</span>.
<ol type="1">
<li>Si se cumple, se acepta <span class="math inline">\(y\)</span> como
muestra de <span class="math inline">\(p_X\)</span></li>
<li>En caso contrario, se rechaza <span class="math inline">\(y\)</span>
y se vuelve al paso 1.</li>
</ol></li>
</ol>
<h3 id="distribuciones-unidimensionales">Distribuciones
unidimensionales</h3>
<h4 id="uniformente">Uniformente</h4>
<h4 id="linear">Linear</h4>
<h4 id="gausiana">Gausiana</h4>
<h4 id="basada-en-texturas">Basada en texturas</h4>
<h3 id="distribuciones-bidimensionales">Distribuciones
bidimensionales</h3>
<h4 id="uniformemente">Uniformemente</h4>
<h4 id="basada-en-mappeo-de-superficies-a-un-hemisferio">Basada en
mappeo de superficies a un hemisferio</h4>
<h4 id="brdf">BRDF</h4>
<h4 id="basada-en-el-coseno">Basada en el coseno</h4>
<h2 id="técnicas-de-reducción-de-varianza-basadas-en-muestras">Técnicas
de reducción de varianza basadas en muestras</h2>
<h3 id="ruleta-rusa">Ruleta rusa</h3>
<blockquote>
<p><strong>Idea</strong>: A random chance that if the luminance of a ray
is less than a given <span class="math inline">\(\varepsilon\)</span>
the path will be discarded. Reduces variance by accepting stronger rays
more often.</p>
</blockquote>
<h3 id="next-event-estimation">Next Event Estimation</h3>
<blockquote>
<p><strong>Idea</strong>: Tracing shadow rays to the light source on
each bounce to see if you can terminate the current path. This involves
shooting a shadow ray towards light sources, if it’s occluded, terminate
the ray.</p>
</blockquote>
<h3 id="blue-noise">Blue noise</h3>
<ul>
<li>https://blog.demofox.org/2020/05/16/using-blue-noise-for-raytraced-soft-shadows/</li>
<li>https://alain.xyz/blog/ray-tracing-filtering</li>
</ul>
<h3 id="forced-random-sampling">Forced random sampling</h3>
<p>http://drivenbynostalgia.com/ (ctrl + f -&gt; forced random
sampling)</p>
<h3 id="sampling-importance-resampling">Sampling importance
resampling</h3>
<ul>
<li>https://blog.demofox.org/2022/03/02/sampling-importance-resampling/</li>
<li>https://research.nvidia.com/sites/default/files/pubs/2020-07_Spatiotemporal-reservoir-resampling/ReSTIR.pdf</li>
</ul>
<h3 id="low-discrepancy-sampling">Low discrepancy sampling</h3>
<hr>
<h2 class="unlisted unnumbered" id="referencias-3">Referencias</h2>
<p><span class="citation" data-cites="ShirleyRRT">(<a
href="#ref-ShirleyRRT" role="doc-biblioref">Shirley and Morley
2003</a>)</span>, <span class="citation" data-cites="PBRT3e">(<a
href="#ref-PBRT3e" role="doc-biblioref">Pharr, Jakob, and Humphreys
2016</a>)</span>, <span class="citation" data-cites="mcbook">(<a
href="#ref-mcbook" role="doc-biblioref">Owen 2013</a>)</span>, <span
class="citation" data-cites="berkeley-cs184">(<a
href="#ref-berkeley-cs184" role="doc-biblioref">Berkeley cs184 n.d.</a>,
Monte Carlo Integration)</span>, <span class="citation"
data-cites="wikipedia-contributors-2021B">(<a
href="#ref-wikipedia-contributors-2021B" role="doc-biblioref">Wikipedia:
Rendering equation n.d.</a>)</span>, <span class="citation"
data-cites="wikipedia-contributors-2021C">(<a
href="#ref-wikipedia-contributors-2021C" role="doc-biblioref">Wikipedia:
Variable aleatoria n.d.</a>)</span>, <span class="citation"
data-cites="wikipedia-contributors-2022H">(<a
href="#ref-wikipedia-contributors-2022H" role="doc-biblioref">Wikipedia:
Distribución de probabilidad n.d.</a>)</span>, <span class="citation"
data-cites="wikipedia-contributors-2022I">(<a
href="#ref-wikipedia-contributors-2022I" role="doc-biblioref">Wikipedia:
Función de probabilidad n.d.</a>)</span>, <span class="citation"
data-cites="wikipedia-contributors-2022J">(<a
href="#ref-wikipedia-contributors-2022J" role="doc-biblioref">Wikipedia:
Expected value n.d.</a>)</span>, <span class="citation"
data-cites="galvin-no-date">(<a href="#ref-galvin-no-date"
role="doc-biblioref">Galvin n.d.</a>)</span>, <span class="citation"
data-cites="wikipedia-contributors-2022K">(<a
href="#ref-wikipedia-contributors-2022K" role="doc-biblioref">Wikipedia:
Probability density function n.d.</a>)</span>, <span class="citation"
data-cites="wikipedia-contributors-2022L">(<a
href="#ref-wikipedia-contributors-2022L" role="doc-biblioref">Wikipedia:
Estimador n.d.</a>)</span>, <span class="citation"
data-cites="wikipedia-contributors-2022M">(<a
href="#ref-wikipedia-contributors-2022M" role="doc-biblioref">Wikipedia:
Método de la transformada inversa n.d.</a>)</span>, <span
class="citation" data-cites="wikipedia-contributors-2022N">(<a
href="#ref-wikipedia-contributors-2022N" role="doc-biblioref">Wikipedia:
Rejection sampling n.d.</a>)</span>,</p>
<ul>
<li><em>(berkeley-cs184)</em>
https://cs184.eecs.berkeley.edu/public/sp22/lectures/lec-12-monte-carlo-integration/lec-12-monte-carlo-integration.pdf</li>
<li>Gems I, p.284.</li>
<li>https://pellacini.di.uniroma1.it/teaching/graphics17b/lectures/12_pathtracing.pdf</li>
<li>Apuntes de inferencia estadística (cómo cito este tipo de
fuentes??)</li>
<li>https://www.wikiwand.com/en/Metropolis%E2%80%93Hastings_algorithm</li>
</ul>
<h1 id="construyamos-un-path-tracer">¡Construyamos un path tracer!</h1>
<p>Ahora que hemos introducido toda la teoría necesaria, es hora de
ponernos manos a la obra. En este capítulo, vamos a escoger una serie de
herramientas y haremos una pequeña implementación de un motor de path
tracing en tiempo real.</p>
<p>La implementación estará basada en Vulkan, junto al pequeño framework
de nvpro-samples. El motor mantendrá el mismo espíritu que la serie de
<span class="citation" data-cites="Shirley2020RTW1">(<a
href="#ref-Shirley2020RTW1" role="doc-biblioref">Shirley
2020a</a>)</span>, Ray Tracing In One Weekend.</p>
<p>Le pondremos especial atención a los conceptos claves. Vulkan tiende
a crear código muy verboso, por lo que se documentarán únicamente las
partes más importantes.</p>
<h2 id="requisitos-de-ray-tracing-en-tiempo-real">Requisitos de ray
tracing en tiempo real</h2>
<p>Como es natural, el tiempo es una limitación enorme para cualquier
programa en tiempo real. Mientras que en un <em>offline renderer</em>
disponemos de un tiempo muy considerable por frame (hablamos de varios
segundos), en un programa en tiempo real necesitamos que un frame salga
en 16 milisegundos o menos. Este concepto se suele denominar <em>frame
budget</em>: la cantidad de tiempo que disponemos para un frame.</p>
<blockquote>
<p><strong>Nota</strong>: cuando hablamos del tiempo disponible para un
frame, solemos hablar en milisegundos (ms) o frames por segundo (FPS).
Para que un motor vaya suficientemente fluido, necesitaremos que el
motor corra a un mínimo de 30 FPS (que equivalen a 33 ms por frame). Hoy
en día, debido al avance del área en campos como los videosjuegos, el
estándar se está convirtiendo en 60 FPS (16 ms/frame).</p>
</blockquote>
<p>Las nociones anteriores no distinguen entre un motor en tiempo real y
<em>offline</em>. Como es natural, necesitaremos introducir unos pocos
conceptos más para llevarlo a tiempo real. Además, existe una serie de
requisitos hardware que debemos cumplir para que un motor en tiempo real
con ray tracing funcione.</p>
<h3 id="arquitecturas-de-gráficas">Arquitecturas de gráficas</h3>
<blockquote>
<p>NOTE: sería interesante enlazarlo con la sección de rendimiento.</p>
<p>TODO: Esta <a
href="https://alain.xyz/blog/comparison-of-modern-graphics-apis">página</a>
es maravillosa <em>chef kiss</em></p>
</blockquote>
<p>El requisito más importante de todos es la gráfica. Para ser capaces
de realizar cálculos de ray tracing en tiempo real, necesitaremos una
arquitectura moderna con núcleos dedicados a este tipo de cáclulos <a
href="#fn4" class="footnote-ref" id="fnref4"
role="doc-noteref"><sup>4</sup></a>.</p>
<p>A día 17 de abril de 2022, para correr ray tracing en tiempo real, se
necesita alguna de las siguientes tarjetas gráficas:</p>
<table>
<colgroup>
<col style="width: 20%" />
<col style="width: 10%" />
<col style="width: 68%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Arquitectura</strong></th>
<th style="text-align: left;"><strong>Fabricante</strong></th>
<th><strong>Modelos de gráficas</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Turing</strong></td>
<td style="text-align: left;">Nvidia</td>
<td>RTX 2060, RTX 2060 Super, RTX 2070, RTX 2070 Super, RTX 2080, RTX
2080 Super, RTX 2080 Ti, RTX Titan</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Ampere</strong></td>
<td style="text-align: left;">Nvidia</td>
<td>RTX 3050, RTX 3060, RTX 3060 Ti, RTX 3070, RTX 3070 Ti, RTX 3080,
RTX 3080 Ti, RTX 3090, RTX 3090 Ti</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>RDNA2</strong> (Navi 2X, Big
Navi)</td>
<td style="text-align: left;">AMD</td>
<td>RX 6400, RX 6500 XT, RX 6600, RX 6600 XT, RX 6700 XT, RX 6800, RX
6800 XT, RX 6900 XT</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Arc Alchemist</strong></td>
<td style="text-align: left;">Intel</td>
<td><em>No reveleado aún</em></td>
</tr>
</tbody>
</table>
<p>Solo se han incluido las gráficas de escritorio de consumidor.</p>
<p>Para este trabajo se ha utilizado una <strong>RTX 2070
Super</strong>. En el capítulo de análisis del rendimiento se hablará
con mayor profundidad de este apartado.</p>
<h3 id="frameworks-y-api-de-ray-tracing-en-tiempo-real">Frameworks y API
de ray tracing en tiempo real</h3>
<p>Una vez hemos cumplido los requisitos de hardware, es hora de escoger
los frameworks de trabajo.</p>
<p>Las API de gráficos están empezando a adaptarse a los requisitos del
tiempo real, por lo que cambian frecuentemente. La mayoría adquirieron
las directivas necesarias muy recientemente. Aun así, son lo
suficientemente sólidas para que se pueda usar en aplicaciones
empresariales de gran embergadura.</p>
<p>Esta es una lista de las API disponibles con capacidades de Ray
Tracing disponibles para, al menos, la arquitectura Turing:</p>
<ul>
<li>Vulkan (los bindings de ray tracing se denominan KHR).</li>
<li>Microsoft DirectX Ray Tracing (DXR), una extensión de DirectX
12.</li>
<li>Nvidia OptiX.</li>
</ul>
<p>De momento, no hay mucho donde elegir.</p>
<p>OptiX es la API más vieja de todas. Su primera versión salió en 2009,
mientras que la última estable es de 2021. Tradicionalmente se ha usado
para offline renderers, y no tiene un especial interés para este trabajo
estando las otras dos disponibles.</p>
<p>Tanto DXR como Vulkan son los candidatos más sólidos. DXR salió en
2018, con la llegada de Turing. Es un par de años más reciente que
Vulkan KHR. Cualquiera de las dos cumpliría su cometido de forma
exitosa. Sin embargo, para este trabajo, <strong>hemos escogido
Vulkan</strong> por los siguientes motivos:</p>
<ul>
<li>DirectX 12 está destinado principalmente a plataformas de Microsoft.
Es decir, está pensado para sistemas operativos Windows 10 o mayor <a
href="#fn5" class="footnote-ref" id="fnref5"
role="doc-noteref"><sup>5</sup></a>.</li>
<li>Vulkan está apoyado principalmente por AMD. Esto sigue las línas de
la su política de empresa de apoyar el código abierto. Además, resulta
más sencillo exportarlo a otros sistemas operativos.</li>
</ul>
<p>Ambas API se comportan de manera muy similar, y no existe una gran
diferencia entre ellas; tanto en rendimiento como en complejidad de
desarrollo. Actualmente el proyecto solo compila en Windows 10 o mayor,
por lo que estos dos puntos no resultan especialmente relevantes para el
trabajo.</p>
<h2 id="setup-del-proyecto">Setup del proyecto</h2>
<p>Un proyecto de Vulkan necesita una cantidad de código inicial
considerable. Para acelerar este trámite y partir de una base más
sólida, se ha decidido usar un pequeño framework de trabajo de Nvidia
llamado <a
href="https://github.com/nvpro-samples">nvpro-samples</a>.</p>
<p>Esta serie de repositorios contienen proyectos de ray tracing de
Nvidia con fines didácticos. Nosotros usaremos <a
href="https://github.com/nvpro-samples/vk_raytracing_tutorial_KHR">vk_raytracing_tutorial_KHR</a>,
pues ejemplifica cómo añadir ray tracing en tiempo real a un proyecto de
Vulkan.</p>
<p>Estos frameworks contienen asimismo otras utilidades menores.
Destacan GLFW (gestión de ventanas en C++), imgui (interfaz de usuario)
y tinyobjloader (carga de <code>.obj</code> y <code>.mtl</code>).</p>
<p>Nuestro repositorio utiliza las herramientas citadas anteriormente
para compilar su proyecto. El Makefile es una modificación del que se
usa para ejecutar los ejemplos de Nvidia. Por defecto, ejecuta una
aplicación muy simple que muestra un cubo mediante rasterización, la
cual modificaremos hasta añadir ray tracing en tiempo real.</p>
<h3 id="vistazo-general-a-la-estructura">Vistazo general a la
estructura</h3>
<p>La estructura final del proyecto (es decir, la carpeta
<code>application</code>) es la siguiente:</p>
<ul>
<li>La carpeta <strong>./build</strong> contiene todo lo relacionado con
CMake y el ejecutable final.</li>
<li>En <strong>./media</strong> se encuentran todos los archivos
<code>.obj</code>, <code>.mtl</code> y las texturas.</li>
<li>La subcarpeta <strong>./src</strong> contiene el código fuente de la
propia aplicación.
<ul>
<li>Toda la implementación relacionada con el motor (y por tanto,
Vulkan), se halla en <code>engine.h/cpp</code>. Una de las desventajas
de seguir un framework “de juguete” es que el acoplamiento es
considerablemente alto. Más adelante comentaremos los motivos.</li>
<li>Los parámetros de la aplicación (como tamaño de pantalla y otras
estructuras comunes) se encuetran en <code>globals.hpp</code>.</li>
<li>La carga de escenas y los objetos se gestionan en
<code>scene.hpp</code>.</li>
<li>En <code>main.cpp</code> se gestiona tanto el punto de entrada de la
aplicación como la actualización de la interfaz gráfica.</li>
<li>La carpeta <code>./src/shaders</code> contiene todos los shaders;
tanto de rasterización, como de ray tracing.
<ul>
<li>Para ray tracing, se utilizan los <code>raytrace.*</code>,
<code>pathtrace.glsl</code> (que contiene el grueso del path
tracer).</li>
<li>En rasterización se usan principalmente
<code>frag_shader.frag</code>, <code>passthrough.vert</code>,
<code>post.frag</code>, <code>vert_shader.vert</code>.</li>
<li>El resto de shaders son archivos comunes a ambos o utilidades
varias.</li>
</ul></li>
<li>Finalmente, la carpeta <code>./src/spv</code> contiene los shaders
compilados a SPIR-V.</li>
</ul></li>
</ul>
<h2 id="compilación">Compilación</h2>
<p>Las dependencias necesarias para compilarlo son:</p>
<ol type="1">
<li>CMake.</li>
<li>Un driver de Nvidia compatible con la extensión
<code>VK_KHR_ray_tracing_pipeline</code>.</li>
<li>El SDK de Vulkan, versión 1.2.161 o mayor.</li>
</ol>
<p>La parte inicial del desarrollo consiste en adaptar Vulkan para usar
la extensión de ray tracing, extrayendo la información de la gráfica y
cargando correspondientemente el dispositivo.</p>
<p>Para compilarlo, ejecuta los siguientes comandos:</p>
<pre class="sh"><code>git clone --recursive --shallow-submodules https://github.com/Asmilex/Raytracing.git
cd .\Raytracing\application\vulkan_ray_tracing\
mkdir build
cd build
cmake ..
cmake --build .</code></pre>
<p>Si todo funciona correctamente, debería generarse un binario en
<code>./application/bin_x64/Debug</code> llamado
<code>asmiray.exe</code>. Desde la carpeta en la que estás, puedes
ejecutarlo con <code>..\..\bin_x64\Debug\asmiray.exe</code>.</p>
<h2 id="estructuras-de-aceleración">Estructuras de aceleración</h2>
<p>El principal coste de ray tracing es el cálculo de las intersecciones
con objetos; hasta un 95% del tiempo de ejecución total <span
class="citation" data-cites="scratchapixel-2019">(<a
href="#ref-scratchapixel-2019" role="doc-biblioref">Scratchapixel
n.d.</a>)</span>. Reducir el número de test de intersección es
clave.</p>
<p>Las <strong>estructuras de aceleración</strong> son una forma de
representar la geometría de la escena. Aunque hay varios tipos
diferentes, en esencia, engloban a un objeto o varios en una estructura
con la que resulta más eficiente hacer test de intersección. Son
similares a los grafos de escena de un rasterizador.</p>
<p>Uno de los tipos más comunes es la <strong>Bounding Volume Hierarchy
(BVH)</strong>. Fue una técnica desarrollada por Kay y Kajilla en 1986.
En esencia, este método encierra un objeto en una caja (lo que se
denomina una <strong>bounding box</strong>), de forma que el test de
intersección principal se hace con la caja y no con la geometría. Si un
rayo impacta en la <em>bounding box</em>, entonces se pasa a testear la
geometría.</p>
<p>Se puede repetir esta idea repetidamente, de forma que agrupemos
varias <em>bounding boxes</em>. Así, creamos una jerarquía de objetos
–como si nodos de un árbol se trataran–. A esta jerarquía es a la que
llamamos BVH.</p>
<p>Es importante crear buenas divisiones de los objetos en la BVH.
Cuanto más compacta sea una BVH, más eficiente será el test de
intersección.</p>
<p>Una forma habitual de crear la BVH es mediante la división del
espacio en una rejilla. Esta técnica se llama <strong>Axis-Aligned
Bounding Box (AABB)</strong>. Usualmente se usa el método del
<em>slab</em> (también introducido por Kay y Kajilla). Se divide el
espacio en una caja n-dimensional alineada con los ejes, de forma que
podemos verla como <span class="math inline">\([x_0, x_1]
\times\)</span> <span class="math inline">\([y_0, y_1] \times\)</span>
<span class="math inline">\([z_0, z_1] \times \dots\)</span> De esta
forma, comprobar si un rayo impacta en una bounding box es tan sencillo
como comprobar que está dentro del intervalo. Este es el método que se
ha usado en Ray Tracing in One Weekend.</p>
<p>Vulkan gestiona las estructuras de aceleración diviéndolas en dos
partes: <strong>Top-Level Acceleration Structure</strong> (TLAS) y
<strong>Bottom-Level Acceleration Structure</strong> (BLAS).</p>
<figure>
<img src="./img/04/Acceleration%20structure.png"
alt="La TLAS guarda información de las instancias de un objeto, así como una referencia a BLAS que contiene la geometría correspondiente. Fuente: Nvidia" />
<figcaption aria-hidden="true">La TLAS guarda información de las
instancias de un objeto, así como una referencia a BLAS que contiene la
geometría correspondiente. Fuente: Nvidia</figcaption>
</figure>
<blockquote>
<p>TODO: Deberíamos cambiar esa foto por otra propia.</p>
</blockquote>
<h3 id="botom-level-acceleration-structure-blas">Botom-Level
Acceleration Structure (BLAS)</h3>
<p>Las Bottom-Level Acceleration Structure almacenan la geometría de un
objeto individual; esto es, los vértices y los índices de los
triángulos, además de una AABB que la encapsula.</p>
<p>Pueden almacenar varios modelos, puesto que alojan uno o más buffers
de vértices junto a sus matrices de transformación. Si un modelo se
instancia varias veces <em>dentro de la misma BLAS</em>, la geometría se
duplica. Esto se hace para mejorar el rendimiento.</p>
<p>Como regla general, cuantas menos BLAS, mejor.</p>
<p>El código correspondiente a la creación de la BLAS en el programa es
el siguiente:</p>
<pre><code class="language-cpp">void Engine::createBottomLevelAS() {
    // BLAS - guardar cada primitiva en una geometría

    std::vector&lt;nvvk::RaytracingBuilderKHR::BlasInput&gt; allBlas;
    allBlas.reserve(m_objModel.size());

    for (const auto&amp; obj: m_objModel) {
        auto blas = objectToVkGeometryKHR(obj);

        // Podríamos añadir más geometrías en cada BLAS.
        // De momento, solo una.
        allBlas.emplace_back(blas);
    }

    m_rtBuilder.buildBlas(
        allBlas,
        VK_BUILD_ACCELERATION_STRUCTURE_PREFER_FAST_TRACE_BIT_KHR
    );
}</code></pre>
<h3 id="top-level-acceleration-structure-tlas">Top-Level Acceleration
Structure (TLAS)</h3>
<p>Las Top-Level Acceleration Structures almacenan las instancias de los
objetos, cada una con su matriz de transformación y referencia a la BLAS
correspondiente.</p>
<p>Además, guardan información sobre el <em>shading</em>. Así, los
shaders pueden relacionar la geometría intersecada y el material de
dicho objeto. En esta última parte jugará un papel fundamental la <a
href="#shader-binding-table">Shader Binding Table</a>.</p>
<p>En el programa hacemos lo siguiente para construir la TLAS:</p>
<pre><code class="language-cpp">void Engine::createTopLevelAS() {
    std::vector&lt;VkAccelerationStructureInstanceKHR&gt; tlas;
    tlas.reserve(m_instances.size());

    for (const HelloVulkan::ObjInstance&amp; inst: m_instances) {
        VkAccelerationStructureInstanceKHR rayInst{};

        // Posición de la instancia
        rayInst.transform = nvvk::toTransformMatrixKHR(inst.transform);

        rayInst.instanceCustomIndex = inst.objIndex;

        // returns the acceleration structure device address of the blasId. The id correspond to the created BLAS in buildBlas.
        rayInst.accelerationStructureReference = m_rtBuilder.getBlasDeviceAddress(inst.objIndex);

        rayInst.flags = VK_GEOMETRY_INSTANCE_TRIANGLE_FACING_CULL_DISABLE_BIT_KHR;
        rayInst.mask  = 0xFF; // Solo registramos hit si rayMask &amp; instance.mask != 0
        rayInst.instanceShaderBindingTableRecordOffset = 0; // Usaremos el mismo hit group para todos los objetos

        tlas.emplace_back(rayInst);
    }

    m_rtBuilder.buildTlas(
        tlas,
        VK_BUILD_ACCELERATION_STRUCTURE_PREFER_FAST_TRACE_BIT_KHR
    );
}</code></pre>
<h2 id="la-ray-tracing-pipeline">La ray tracing pipeline</h2>
<h3 id="descriptores-y-conceptos-básicos">Descriptores y conceptos
básicos</h3>
<p>Primero, debemos introducir unas nociones básicas de Vulkan sobre
cómo gestiona la información que se pasa a los shaders.</p>
<p>Un <strong><em>resource descriptor</em></strong> (usualmente lo
abreviaremos como descriptor) es una forma de cargar recursos como
buffers o imágenes para que la tarjeta gráfica los pueda utilizar;
concretamente, los shaders. El <strong><em>descriptor
layout</em></strong> especifica el tipo de recurso que va a ser
accedido. Finalmente, el <strong><em>descriptor set</em></strong>
determina el buffer o imagen que se va a asociar al descriptor. Este set
es el que se utiliza en los <strong>drawing commands</strong>. Un
<strong>pipeline</strong> es una secuencia de operaciones que reciben
una geometría y sus texturas, y la transforma en unos pixels.</p>
<p>Si necesitas más información, todos estos conceptos aparecen
desarrollados extensamente en <span class="citation"
data-cites="overvoorde-2022">(<a href="#ref-overvoorde-2022"
role="doc-biblioref">Overvoorde n.d.</a>)</span></p>
<p>Tradicionalmente, en rasterización se utiliza un descriptor set por
tipo de material, y consecuentemente, un pipeline por cada tipo. En ray
tracing esto no es posible, puesto que no se sabe qué material se va a
usar: un rayo puede impactar <em>cualquier</em> material presente en la
escena, lo cual invocaría un shader específico. Debido a esto,
empaquetaremos todos los recursos en un único set de descriptores.</p>
<h3 id="la-shader-binding-table">La Shader binding table</h3>
<p>Para solucionar esto, vamos a crear la <strong>Shader Binding
Table</strong> (SBT). Esta estructura permitirá cargar el shader
correspondiente dependiendo de dónde impacte un rayo.</p>
<p>Para cargar esta estructura, se debe hacer lo siguiente:</p>
<ol type="1">
<li>Cargar y compilar cada shader en un
<code>VkShaderModule</code>.</li>
<li>Juntar los cada <code>VkShaderModule</code> en un array
<code>VkPipelineShaderStageCreateInfo</code>.</li>
<li>Crear un array de <code>VkRayTracingShaderGroupCreateInfoKHR</code>.
Cada elemento se convertirá al final en una entrada de la Shader Binding
Table.</li>
<li>Compilar los dos arrays anteriores más un pipeline layout para
generar un <code>vkCreateRayTracingPipelineKHR</code>.</li>
<li>Conseguir los <em>handlers</em> de los shaders usando
<code>vkGetRayTracingShaderGroupHandlesKHR</code>.</li>
<li>Alojar un buffer con el bit
<code>VK_BUFFER_USAGE_SHADER_BINDING_TABLE_BIT_KHR</code> y copiar los
<em>handlers</em>.</li>
</ol>
<figure>
<img src="./img/04/Pipeline.png"
alt="La Shader Binding Table permite selccionar un tipo de shader dependiendo del objeto en el que se impacte. Para ello, se genera un rayo desde el shader raygen, el cual viaja a través de la Acceleration Structure. Dependiendo de dónde impacte, se utiliza un closest hit, any hit, o miss shaders. Fuente: Nvidia" />
<figcaption aria-hidden="true">La Shader Binding Table permite
selccionar un tipo de shader dependiendo del objeto en el que se
impacte. Para ello, se genera un rayo desde el shader
<code>raygen</code>, el cual viaja a través de la Acceleration
Structure. Dependiendo de dónde impacte, se utiliza un
<code>closest hit</code>, <code>any hit</code>, o <code>miss</code>
shaders. Fuente: Nvidia</figcaption>
</figure>
<p>Cada entrada de la SBT contiene un handler y una serie de parámetros
embebidos. A esto se le conoce como <strong>Shader Record</strong>.
Estos records se clasifican en:</p>
<ul>
<li><strong>Ray generation record</strong>: contiene el handler del ray
generation shader.</li>
<li><strong>Hit group record</strong>: se encargan de los handlers del
closest hit, anyhit (opcional), e intersection (opcional).</li>
<li><strong>Miss group record</strong>: se encarga del miss shader.</li>
<li><strong>Callable group record</strong>.</li>
</ul>
<p>Una de las partes más difíciles de la SBT es saber cómo se relacionan
record y geometría. Es decir, cuando un rayo impacta en una geometría,
¿a qué record de la SBT llamamos? Esto se determina mediante los
parámetros de la instancia, la llamada a <em>trace rays</em>, y el orden
de la geometría en la BLAS.</p>
<figure>
<img src="./img/04/SBT.png" alt="Fuente: https://www.willusher.io/" />
<figcaption aria-hidden="true">Fuente:
https://www.willusher.io/</figcaption>
</figure>
<h4 id="cálculo-de-la-entrada-de-la-sbt">Cálculo de la entrada de la
SBT</h4>
<p>El principal problema es el cálculo del índice en los hit groups.</p>
<p>Llamemos al índice de cada instancia de una geometría en la BLAS
<span class="math inline">\(\mathbb{G}_{\text{ID}}\)</span>. A cada
instancia se le puede asignar un desplazamiento con respecto a la SBT
(<span class="math inline">\(\mathbb{I}_{\text{offset}}\)</span>) desde
donde empieza la subtabla de hit groups.</p>
<blockquote>
<p>TODO: esto se deja temporal de momento. No sé hasta qué punto me
convence poner todo esto. Lo veo importante, pero no sé si <em>tan</em>
importante. El recurso que estoy usando es
https://www.willusher.io/graphics/2019/11/20/the-sbt-three-ways, por si
al final decidimos escribirlo.</p>
</blockquote>
<h3 id="tipos-de-shaders">Tipos de shaders</h3>
<p>El pipeline soporta varios tipos de shaders diferentes que cubren la
funcionalidad esencial de un ray tracer:</p>
<ul>
<li><strong>Ray generation shader</strong>: es el punto de inicio del
viaje de un rayo. Calcula punto de inicio y procesa el resultado final.
Idealmente, solo se invocan rayos desde aquí. Se suele invocar</li>
<li><strong>Closest hit shader</strong>: este shader se ejecuta cuando
un rayo impacta en una geometría por primera vez. Se pueden trazar rayos
recursivamente desde aquí (por ejemplo, para calcular oclusión
ambiental).</li>
<li><strong>Any-hit shader</strong>: similar al closest hit, pero
invocado en cada intersección del camino del rayo que cumpla <span
class="math inline">\(t \in [t_{min}, t_{max})\)</span>. Es comúnmente
utilizado en los cálculos de transparencias
(<em>alpha-testing</em>).</li>
<li><strong>Miss shader</strong>: si el rayo no choca con ninguna
geometría –pega con el infinito–, se ejecuta este shader. Normalmente,
añade una pequeña contribución ambiental al rayo.</li>
<li><strong>Intersection shader</strong>: este shader es algo diferente
al resto. Su función es calcular el punto de impacto de un rayo con una
geometría. Por defecto se utiliza un test triángulo - rayo. En nuestro
path tracer lo dejaremos por defecto, pero podríamos definir algún
método como los que vimos en la sección <a
href="#intersecciones-rayo---objeto">intersecciones rayo -
objeto</a>.</li>
</ul>
<p>Este es el código de los shaders del path tracer: <a
href="https://github.com/Asmilex/Raytracing/blob/main/application/vulkan_ray_tracing/src/shaders/raytrace.rgen">Raygen</a>,
<a
href="https://github.com/Asmilex/Raytracing/blob/main/application/vulkan_ray_tracing/src/shaders/raytrace.rchit">Closest
hit</a>, <a
href="https://github.com/Asmilex/Raytracing/blob/main/application/vulkan_ray_tracing/src/shaders/raytrace.rmiss">Miss</a>,
<a
href="https://github.com/Asmilex/Raytracing/blob/main/application/vulkan_ray_tracing/src/shaders/raytrace_rahit.glsl">Any-hit</a>.</p>
<p>Existe otro tipo de shader adicional denominado <strong>callable
shader</strong>. Este es un shader que se invoca desde otro shader. Por
ejemplo, un shader de intersección puede invocar a un shader de
oclusión. Otro ejemplo sería un closest hit que reemplaza un bloque
if-else por un shader para hacer cálculos de iluminación. Este tipo de
shaders no se han implementado en el path tracer, pero se podrían añadir
con un poco de trabajo.</p>
<h3 id="traspaso-de-información-entre-shaders">Traspaso de información
entre shaders</h3>
<p>En ray tracing, los shaders por sí solos no pueden realizar todos los
cálculos necesarios. Por ello, necesitaremos enviar información de uno a
otro. Tenemos diferentes mecanismos para conseguirlo:</p>
<p>El primero de ellos son las <strong>push constansts</strong>. Estas
son variables que se pueden enviar a los shaders (es decir, de CPU a
GPU), pero que no se pueden modificar. Únicamente podemos mandar un
pequeño número de variables, el cual se puede consultar mediante
<code>VkPhysicalDeviceLimits.maxPushConstantSize</code>.</p>
<p>Nuestro path tracer tiene implementado actualmente (19 de abril de
2022) las siguientes constantes:</p>
<pre><code class="language-cpp">struct PushConstantRay {
    vec4  clearColor;     // Color ambiental
    vec3  lightPosition;
    float lightIntensity;
    int   lightType;
    int   maxDepth;       // Cuántos rebotes máximos permitimos
    int   nb_samples;     // Para antialiasing
    int   frame;          // Para acumulación temporal
};</code></pre>
<p>Las push constants son, como dice su nombre, constantes. ¿Y si
queremos pasar información mutable entre shaders?.</p>
<p>Para eso están los <strong>payloads</strong>. Específicamente, cada
rayo puede llevar información adicional. Como una pequeña mochila. Esto
resulta <em>muy</em> útil, por ejemplo, a la hora de calcular la
radiancia de un camino. Se crean mediante la estructura
<code>rayPayloadEXT</code>, y se reciben en otro shader mediante
<code>rayPayloadInEXT</code>. Es importante controlar que el tamaño de
la carga no sea excesivamente grande.</p>
<h3 id="creación-de-la-ray-tracing-pipeline">Creación de la ray tracing
pipeline</h3>
<p>El código de la creación de la pipeline lo encapsula la función
<code>createRtPipeline()</code>, que se puede consultar <a
href="https://github.com/Asmilex/Raytracing/blob/6409feb628cc048186f6279b921ebe24e9337b6a/application/vulkan_ray_tracing/src/hello_vulkan.cpp#L763">aquí</a></p>
<h2 id="transporte-de-luz-en-la-práctica">Transporte de luz en la
práctica</h2>
<h3 id="estimando-la-rendering-equation-con-monte-carlo">Estimando la
rendering equation con Monte Carlo</h3>
<p>Hemos llegado a una de las partes más importantes de este trabajo. Es
el momento de poner en concordancia todo lo que hemos visto a lo largo
de los capítulos anteriores.</p>
<p>Empecemos por la dispersión. ¿Recuerdas la ecuación de dispersión [<a
href="#eq:scattering_equation">6</a>]?</p>
<p><span class="math display">\[
L_o(p, \omega_o \leftarrow \omega_i) = \int_{\mathbb{S}^2}{f(p, \omega_o
\leftarrow \omega_i)L_i(p, \omega_i)\cos\theta_i} d\omega_i
\]</span></p>
<p>Añadamos el término de radiancia emitida:</p>
<p><span class="math display">\[
L_o(p, \omega_o \leftarrow \omega_i) = L_e(p, \omega_o) +
\int_{\mathbb{S}^2}{f(p, \omega_o \leftarrow \omega_i)L_i(p,
\omega_i)\cos\theta_i} d\omega_i
\]</span></p>
<p>Podemos aproximar el valor de la integral utilizando Monte Carlo:</p>
<p><span class="math display">\[
\begin{aligned}
L_o(p, \omega_o \leftarrow \omega_i) &amp; = \int_{\mathbb{S}^2}{f(p,
\omega_o \leftarrow \omega_i)L_i(p, \omega_i)\cos\theta_i} d\omega_i \\
                 &amp; \approx \frac{1}{N} \sum_{j = 1}^{N}{\frac{f(p,
\omega_o \leftarrow \omega_j) L_i(p, \omega_j)
\cos\theta_j}{p(\omega_j)}}
\end{aligned}
\]</span></p>
<p>Fijémonos en el denominador. Lo que estamos haciendo es tomar una
muestra de un vector en la esfera. Si trabajamos con una BRDF en vez de
una BSDF, usaríamos un hemisferio en vez de la esfera.</p>
<p>En el caso de la componente difusa, sabemos que la BRDF es <span
class="math inline">\(f_r(p, \omega_o \leftarrow \omega_i) =
\frac{\rho}{\pi}\)</span>, así que</p>
<p><span class="math display">\[
\frac{1}{N} \sum_{j = 1}^{N}{\frac{(\rho / \pi) L_i(p, \omega_j)
\cos\theta_j}{p(\omega_j)}}
\]</span></p>
<p>¿Recuerdas la sección de <a href="#importance-sampling">importance
sampling</a>? La idea es buscar una función proporcional a <span
class="math inline">\(f\)</span> para reducir el error. Podemos usar
<span class="math inline">\(p(\omega) = \frac{\cos\theta}{\pi}\)</span>,
de forma que</p>
<p><span id="eq:rendering_eq_lambertian" class="eqnos"><span
class="math display">\[
\frac{1}{N} \sum_{j = 1}^{N}{\frac{(\rho / \pi) L_i(p, \omega_j)
\cos\theta_j}{(\cos\theta_j / \pi)}} = \frac{1}{N} \sum_{j =
1}^{N}{L_i(p, \omega_j) \rho}
\]</span><span class="eqnos-number">(11)</span></span></p>
<p>Lo cual nos proporciona una expresión muy agradable para los
materiales difusos. Solo es cuestión de calcular la radiancia del
punto.</p>
<h3 id="pseudocódigo-de-un-path-tracer">Pseudocódigo de un path
tracer</h3>
<p>Con lo que conocemos hasta ahora, podemos empezar a programar los
shaders. Una primera implementación basada en [<a
href="#eq:rendering_eq_lambertian">11</a>] sería similar a lo
siguiente:</p>
<pre><code class="language-cpp">vec3 path_trace(Rayo r, profundidad) {
    if (profundidad == profundidad_maxima) {
        return contribucion_ambiental;
    }

    r.trazar_rayo()

    if (!r.ha_impactado()) {
        return contribucion_ambiental;
    }

    hit_info = r.hit_info
    material = hit_info.material
    emission = material.emission

    nuevo_rayo = Rayo(
        origen = hit_info.punto_impacto,
        direccion = aleatorio_en_hemisferio(hit_info.normal)
    // ¡Importante! ^^^^^^^^^^^^^^^^^^^^^^^
    )

    // Usando reflexión lambertiana, tendríamos
    cos_theta = dot(nuevo_rayo.direccion, hit_info.normal)
    BRDF = material.reflectancia * cos_theta
    reflejado = path_trace(nuevo_rayo, profundidad + 1)

    return emission + BRDF * reflejado
}</code></pre>
<p>El término <code>emission</code> corresponde a <span
class="math inline">\(L_e(p, \omega_o)\)</span>. Siempre lo añadimos,
pues en caso de que el objeto no emita luz, la contribución de este
término sería 0.</p>
<h3
id="antialiasing-mediante-jittering-y-acumulación-temporal">Antialiasing
mediante jittering y acumulación temporal</h3>
<p>Normalmente, mandamos los rayos desde el centro de un pixel. Podemos
conseguir una mejora sustancial de la calidad con un pequeño truco: en
vez de generarlos siempre desde el mismo sitio, le aplicamos una pequeña
perturbación (<em>jittering</em>). Así, tendremos diferentes colores
para un mismo pixel, por lo que podemos hacer una ponderación del color
que se obtiene (a lo que llamamos <em>acumulación temporal</em>).</p>
<p>Es importante destacar que el efecto de esta técnica solo es válido
cuando la <strong>cámara se queda estática</strong>.</p>
<p>La implementación es muy sencilla. Debemos modificar tanto el motor
como los shaders para llevar una cuenta de ciertos frames, definiendo un
máximo de frames que se pueden acumular:</p>
<pre><code class="language-cpp">// engine.h
class Engine {
    //...
    int m_maxAcumFrames {20};
}</code></pre>
<p>Las push constant deberán llevar un registro del frame en el que se
encuentran, así como un número máximo de muestras a acumular para un
pixel:</p>
<pre><code class="language-cpp">// host_device.h
struct PushConstantRay {
    //...
    int   frame;
    int   nb_samples
}</code></pre>
<p>El número de frame se reseteará cuando la cámara se mueva, la ventana
se reescale, o se produzca algún efecto similar en la aplicación.</p>
<p>Finalmente, en los shaders podemos implementar lo siguiente:</p>
<pre><code class="language-glsl">// raytrace.rgen
vec3 pixel_color = vec3(0);

for (int smpl = 0; smpl &lt; pcRay.nb_samples; smpl++) {
    pixel_color += sample_pixel(image_coords, image_res);
}

pixel_color = pixel_color / pcRay.nb_samples;

if (!primer_frame) {
    guardar una mezcla de las anteriores imágenes junto con la actual
}
else {
    guardar la imagen directamente
}
</code></pre>
<pre><code class="language-glsl">// pathtrace.glsl
vec3 sample_pixel() {
    float r1 = rnd(prd.seed);
    float r2 = rnd(prd.seed);

    // Subpixel jitter: mandar el rayo desde una pequeña perturbación del pixel para aplicar antialiasing
    vec2 subpixel_jitter = pcRay.frame == 0
        ? vec2(0.5f, 0.5f)
        : vec2(r1, r2);

    const vec2 pixelCenter = vec2(image_coords.xy) + subpixel_jitter;

    // ...

    vec3 radiance = pathtrace(rayo);
}</code></pre>
<h3 id="materiales-y-objetos">Materiales y objetos</h3>
<blockquote>
<p>NOTE: esto son notas para el Andrés del futuro. Sí, lo sé, está
bastante claro solo con leerlo (⊙_⊙;)</p>
</blockquote>
<p>Si quiero meter las BxDFs en los materiales tal y como tenía pensado
(es decir, unas cuantas flags que me indiquen la BxDF que tengo que
usar), tengo que…</p>
<ol type="1">
<li>Modificar <code>common/obj_loader.h/MaterialObj</code> para meterle
las flags necesarias.</li>
<li>Modificar acordemente
<code>shaders/host_device.h/WaveFronMaterial</code>.</li>
<li>Secuestrar <code>ObjLoader::loadModel()</code> para indicarle los
parámetros nuevos.</li>
<li>(<em>Creo que no hace falta tocar <code>HelloVk::loadModel()</code>
de esta manera</em>)</li>
<li>Toquetear los shaders para que me saque las flags.</li>
</ol>
<p>CREO que de esta manera no me va a hacer falta tocar framebuffers.
Simplemente, todo dependerá de mi material y ya.</p>
<p><em>Creo</em>.</p>
<h2 id="fuentes-de-luz">Fuentes de luz</h2>
<blockquote>
<p>TODO: point lights, area lights, ambient lights…</p>
<p>TODO: estas son notas muy puntuales (<em>como las luces, jeje</em>).
Ya las revisaré más adelante.</p>
<p>TODO: 01_lights.pdf tiene información útil sobre muestreo directo de
fuents de luces.</p>
</blockquote>
<p>La interfaz se encuentra en <code>host_device.h</code>. Describe cómo
comunicarse con la GPU.</p>
<p>Ahora mismo, tenemos 3 constantes: tipo de luz:</p>
<pre><code class="language-glsl">vec3  lightPosition;    // (x, y, z)
float lightIntensity;   // (Intensidad)
int   lightType;        // (0 =&gt; point light, 1 =&gt; area light)</code></pre>
<p>Sería interesante añadir algunas constantes para controlar el tamaño
(radio, posición, normal para las de área…)</p>
<h3 id="point-lights-spotlights">Point lights + spotlights</h3>
<blockquote>
<p>pbr-book, point lights: <em>“Strictly speaking, it is incorrect to
describe the light arriving at a point due to a point light source using
units of radiance. Radiant intensity is instead the proper unit for
describing emission from a point light source, as explained in Section
5.4. In the light source interfaces here, however, we will abuse
terminology and use Sample_Li() methods to report the illumination
arriving at a point for all types of light sources, dividing radiant
intensity by the squared distance to the point p to convert units.
Section 14.2 revisits the details of this issue in its discussion of how
delta distributions affect evaluation of the integral in the scattering
equation. In the end, the correctness of the computation does not suffer
from this fudge, and it makes the implementation of light transport
algorithms more straightforward by not requiring them to use different
interfaces for different types of lights.”</em></p>
</blockquote>
<pre><code class="language-cpp">// https://github.com/mmp/pbrt-v3/blob/master/src/lights/point.h
// https://github.com/mmp/pbrt-v3/blob/master/src/lights/point.cpp

Spectrum sample_light(interaccion, vec2 u, vec3 wi, float pdf, visibility_tester) {
    wi = normalize(posicion_luz - interraccion.p);
    pdf = 1.f;
    // testeo de visibilidad. Opcional, I guess.

    return intensidad / distancia_al_cuadrado(posicion_luz, interraccion.p);
}</code></pre>
<p>La potencia total emitida por la luz puede calcularse integrando la
intensidad desprendida en toda su superficie. Asumiendo la intensidad
constante:</p>
<p><span class="math display">\[
\Phi = \int_{\mathbb{S}^2}{I d\omega} = I \int_{\mathbb{S}^2}{d\omega} =
4 \pi I
\]</span></p>
<p>Las spotlights son variaciones de las point lights iluminando en un
cono.</p>
<h3 id="fuentes-de-área">Fuentes de área</h3>
<p>Para simplificar la implementación, podemos asumir que son
rectangulares.</p>
<p>Nos van a hacer falta técnicas de Monte Carlo para solucionar el
problema de calcular integrales a lo largo de su superficie.</p>
<p>Primero, lo mejor es asumir un cuadrado, y después, extender la
interfaz para meter otras formas (es decir, rectángulos. Porque lo otro
sería mucha parafernalia innecesaria).</p>
<p><a
href="https://github.com/mmp/pbrt-v3/blob/aaa552a4b9cbf9dccb71450f47b268e0ed6370e2/src/core/light.h">Código
fuente</a></p>
<hr>
<h2 class="unlisted unnumbered" id="referencias-4">Referencias</h2>
<ul>
<li>https://github.com/dannyfritz/awesome-ray-tracing</li>
<li>https://www.wikiwand.com/en/Radeon</li>
<li>https://www.wikiwand.com/en/List_of_Nvidia_graphics_processing_units#/GeForce_30_series</li>
<li>https://www.eurogamer.net/digitalfoundry-2021-the-big-intel-interview-how-intel-alchemist-gpus-and-xess-upscaling-will-change-the-market</li>
<li>https://www.intel.com/content/www/us/en/products/docs/arc-discrete-graphics/overview.html</li>
<li>https://www.khronos.org/registry/vulkan/specs/1.2-khr-extensions/html/chap1.html</li>
<li>https://www.wikiwand.com/en/OptiX</li>
<li>https://www.wikiwand.com/en/DirectX_Raytracing</li>
<li>https://www.wikiwand.com/es/Valve_Corporation</li>
<li>https://www.phoronix.com/scan.php?page=news_item&amp;px=VKD3D-Proton-2.5</li>
<li>https://github.com/ValveSoftware/Proton</li>
<li>https://nvpro-samples.github.io/vk_raytracing_tutorial_KHR/</li>
<li>https://www.scratchapixel.com/lessons/3d-basic-rendering/introduction-acceleration-structure</li>
<li>https://www.khronos.org/blog/vulkan-ray-tracing-best-practices-for-hybrid-rendering</li>
<li>https://raytracing.github.io/books/RayTracingTheNextWeek.html#boundingvolumehierarchies</li>
<li>https://vulkan-tutorial.com/en/Uniform_buffers/Descriptor_layout_and_buffer</li>
<li>Ray tracing gems II, p.241.</li>
<li>https://www.willusher.io/graphics/2019/11/20/the-sbt-three-ways</li>
</ul>
<h1 id="análisis-de-rendimiento">Análisis de rendimiento</h1>
<blockquote>
<p>TODO: para completar esta parte, necesitamos ambas implementaciones
(en CPU y GPU) listas. Hasta entonces, esto se queda vacío. NOTE: Podría
preguntarle a Kako que tire benchmark en su 2060, y a Manu con su 3060
(¿ti?)</p>
</blockquote>
<hr>
<h2 class="unlisted unnumbered" id="referencias-5">Referencias</h2>
<h1 id="el-presente-y-futuro-de-rt">El presente y futuro de RT</h1>
<blockquote>
<p>TODO: de momento, se queda como está. Es un capítulo bastante fácil
de escribir, así que en unos tres días como muchísimo podría estar todo
listo.</p>
</blockquote>
<h2 id="denoising">Denoising</h2>
<p>https://alain.xyz/blog/ray-tracing-denoising</p>
<h2 id="filtering">Filtering</h2>
<p>https://alain.xyz/blog/ray-tracing-filtering</p>
<h2 id="offline-renderers">Offline renderers</h2>
<h2 id="la-industria-del-videojuego">La industria del videojuego</h2>
<h3 id="ray-tracing-híbrido">Ray tracing híbrido</h3>
<p>https://www.khronos.org/blog/vulkan-ray-tracing-best-practices-for-hybrid-rendering</p>
<h3 id="productos-comerciales">Productos comerciales</h3>
<ul>
<li>Control
<ul>
<li>Híbrido</li>
<li>https://alain.xyz/blog/frame-analysis-control</li>
<li>https://www.youtube.com/watch?v=blbu0g9DAGA</li>
</ul></li>
<li>Minecraft RTX
<ul>
<li>Path tracing</li>
<li>https://alain.xyz/blog/frame-analysis-minecraftrtx</li>
<li>https://www.youtube.com/watch?v=s_eeWr622Ss</li>
<li>https://www.youtube.com/watch?v=TVtSsJf86_Y</li>
</ul></li>
<li>Cyberpunk 2077
<ul>
<li>Híbrido</li>
</ul></li>
<li>Quake II RTX
<ul>
<li>Path tracing</li>
</ul></li>
<li>Doom RTX
<ul>
<li>Path tracing</li>
</ul></li>
<li>Metro Exodus</li>
</ul>
<h3 id="unreal-engine-5">Unreal Engine 5</h3>
<h3 id="la-última-generación-de-consolas">La última generación de
consolas</h3>
<h2 id="posibles-mejoras-del-trabajo">Posibles mejoras del trabajo</h2>
<ul>
<li>Test Driven Development
<ul>
<li><em>White furnace test</em> (01_lights.pdf, p.61)</li>
</ul></li>
<li>Debugging https://alain.xyz/blog/graphics-debugging</li>
<li>Materiales
<ul>
<li>Physically based materials</li>
<li>Diferentes tipos de materiales</li>
<li>Subsurface scattering</li>
</ul></li>
<li>HDR</li>
<li>Cámara
<ul>
<li>Diferentes tipos de cámaras</li>
<li>Focal lenght, depth of field, motion blur</li>
</ul></li>
</ul>
<hr>
<h2 class="unlisted unnumbered" id="referencias-6">Referencias</h2>

<h1 id="metodología-de-trabajo">Metodología de trabajo</h1>
<p>Cualquier proyecto de una envergadura considerable necesita ser
planificado con antelación. En este capítulo vamos a hablar de cómo se
ha realizado este trabajo: mostraremos las herramientas usadas, los
ciclos de desarrollo, integración entre documentación y path tracer, y
otras influencias que han afectado al producto final.</p>
<h2 id="influencias">Influencias</h2>
<p>Antes de comenzar con la labor, primero uno se debe hacer una simple
pregunta:</p>
<blockquote>
<p><em>“Y esto, ¿por qué me importa?”</em></p>
</blockquote>
<p>Dar una respuesta contundente a este tipo de cuestiones nunca es
fácil. Sin embargo, sí que puedo proporcionar motivos por los que he
querido escribir sobre ray tracing.</p>
<p>Una de las principales influencias ha sido <a
href="https://www.youtube.com/user/DigitalFoundry">Digital Foundry</a>.
Este grupo de divulgación se dedica al estudio de las técnicas
utilizadas en el mundo de los videojuegos. El inicio de la era del ray
tracing en tiempo real les llevó a dedicar una serie de vídeos y
artículos a esta tecnología, y a las diferentes maneras en las que se ha
implementado. Se puede ver un ejemplo en <span class="citation"
data-cites="digital-foundry-2020">(<a href="#ref-digital-foundry-2020"
role="doc-biblioref">Digital Foundry n.d.</a>)</span>.</p>
<p>Dado que esta área combina tanto informática, matemáticas y una
visión artística, ¿por qué no explorarlo a fondo?</p>
<p>Ahora que se ha decidido el tema, es hora de ver cómo atacarlo.</p>
<p>Soy un fiel creyente del aprendizaje mediante el juego. Páginas como
<a href="https://explorabl.es/">Explorable Explanations</a>, el <a
href="https://ciechanow.ski/lights-and-shadows/">blog de Bartosz
Ciechanowski</a>, el proyecto <a
href="https://web.evanchen.cc/napkin.html"><em>The napkin</em></a> o el
divulgador <a href="https://www.3blue1brown.com/">3Blue1Brown</a>
repercuten inevitablemente en la manera en la que te planteas cómo
comunicar textos científicos. Por ello, aunque esto a fin de cuentas es
un trabajo de fin de grado de una carrera, quería ver hasta dónde era
capaz de llevarlo.</p>
<p>Otro punto importante es la <em>manera</em> de escribir. No me gusta
especialmente la escritura formal. Prefiero ser distendido. Por suerte,
parece que el mundo científico se está volviendo más informal <span
class="citation" data-cites="nature-2016">(<a href="#ref-nature-2016"
role="doc-biblioref">Nature n.d.</a>)</span>, así que no soy el único
que aprueba esta tendencia. Además, la estructura clásica de un escrito
matemático de “teorema, lema, demostración, corolario” no me agrada
especialmente. He intentado preservar su estructura, pero sin ser tan
explícito. Estos dos puntos, en conjunto, suponen un balance entre
formalidad y distensión difícil de mantener.</p>
<h2 id="ciclos-de-desarrollo">Ciclos de desarrollo</h2>
<p>Este proyecto está compuesto por 2 grandes pilares: documentación –lo
que estás leyendo, ya sea en PDF o en la web– y software.</p>
<p>La metodología que se ha seguido es, en esencia, una versión de Agile
muy laxa <span class="citation" data-cites="beck2001agile">(<a
href="#ref-beck2001agile" role="doc-biblioref">Beck et al.
2001</a>)</span>.</p>
<p>Para empezar, se implementaron los tres libros de Shirley de la
“serie In One Weekend”: In One Weekend <span class="citation"
data-cites="Shirley2020RTW1">(<a href="#ref-Shirley2020RTW1"
role="doc-biblioref">Shirley 2020a</a>)</span>, The Next Week <span
class="citation" data-cites="Shirley2020RTW2">(<a
href="#ref-Shirley2020RTW2" role="doc-biblioref">Shirley
2020b</a>)</span>, y The Rest of your Life <span class="citation"
data-cites="Shirley2020RTW3">(<a href="#ref-Shirley2020RTW3"
role="doc-biblioref">Shirley 2020c</a>)</span>.</p>
<p>Tras esto, comenzó a <a href="#setup-del-proyecto">desarrollarse</a>
el motor por GPU. Cuando se consiguió una base sólida (que se puede ver
en <a href="https://github.com/Asmilex/Raytracing/issues/25">este issue
del repositorio</a>), se empezó a alternar entre escritura de
documentación y desarrollo del software. A fin de cuentas, no tiene
sentido implementar algo que no se conoce.</p>
<p>Para apoyar el desarrollo, se ha utilizado <a
href="#github">Github</a>. Más adelante hablaremos de cómo esta
plataforma ha facilitado el trabajo.</p>
<h2 id="presupuesto">Presupuesto</h2>
<blockquote>
<p>TODO: ahora mismo, no tengo ni idea de cómo empezar esto.</p>
</blockquote>
<h2 id="arquitectura-del-software">Arquitectura del software</h2>
<blockquote>
<p>TODO: especificaciones de los requerimientos y metodología de
desarrollo, así como los planos del proyecto que contendrán las
historias de usuario o casos de uso, diagrama conceptual, de iteración,
de diseño, esquema arquitectónico y bocetos de las interfaces de
usuario. Se describirán las estructuras de datos no fundamentales y
algoritmos no triviales</p>
<p>(Odio los diagramas de clases. Me gustaría evitar como sea
incluirlos.)</p>
</blockquote>
<h2 id="diseño">Diseño</h2>
<blockquote>
<p>TODO: hablar de paleta de colores, tipografía…</p>
</blockquote>
<p>El diseño juega un papel fundamental en este proyecto. Todos los
elementos visuales han sido escogidos con cuidado, de forma que se
preserve la estética.</p>
<p>Se ha creado <strong>un diseño que preserve el equilibrio entre la
profesionalidad y la distensión</strong>.</p>
<h3 id="bases-del-diseño">Bases del diseño</h3>
<p>Para la documentación en versión PDF, usamos como base la
<em>template</em> <a
href="https://github.com/Wandmalfarbe/pandoc-latex-template">Eisvogel</a>.
Esta es una elegante plantilla fácil de usar para LaTeX. Uno de sus
puntos fuertes es la personalización, la cual aprovecharemos para darle
un toque diferente.</p>
<p>La web utiliza como base el estilo generado por Pandoc, el
microframework de css <a
href="https://github.com/rilwis/bamboo">Bamboo</a> y unas modificaciones
personales.</p>
<h3 id="tipografías">Tipografías</h3>
<p>Un apartado al que se le debe prestar especial énfasis es a la
combinación de tipografías. A fin de cuentas, esto es un libro; así que
escoger un tipo de letra correcto facilitará al lector comprender los
conceptos. Puede parecer trivial a priori, pero es importante.</p>
<p>Para este trabajo, se han escogido las siguientes tipografías:</p>
<ul>
<li><a href="https://fonts.google.com/specimen/Crimson+Pro">Crimson
Pro</a>: una tipografía serif clara, legible y contemporánea. Funciona
muy bien en densidades más bajas, como 11pt. Es ideal para la versión en
PDF. Además, liga estupendamente con Source Sans Pro, utilizada para los
títulos en la plantilla Eisvogel.</li>
<li><a href="https://fonts.google.com/specimen/Fraunces">Fraunces</a>:
de lejos, la fuente más interesante de todo este proyecto. Es una
soft-serif <em>old style</em>, pensada para títulos y similares (lo que
se conoce como <em>display</em>). Es usada en los títulos de la web. Una
de sus propiedades más curiosas es que modifica activamente los glifos
dependiendo del valor del <em>optical size axis</em>, el peso y
similares. Recomiendo echarle un ojo a su <a
href="https://github.com/undercasetype/Fraunces">repositorio de
Github</a>.</li>
<li><a href="https://fonts.google.com/specimen/Rubik">Rubik</a>: La
elección de Rubik es peculiar. Por sí sola, no casa con el proyecto. Sin
embargo, combinada con Fraunces, proporcionan un punto de elegancia y
familiaridad a la web. Su principal fuerte es la facilidad para la
comprensión lectora en pantallas, algo que buscamos para la página
web.</li>
<li><a href="https://juliamono.netlify.app/">Julia Mono</a>:
monoespaciada, pensada para computación científica. Llevo usándola
bastante tiempo, y combia bien con Crimson Pro.</li>
<li><a href="https://www.jetbrains.com/es-es/lp/mono/">Jetbrains
Mono</a>: otra tipografía monoespaciada open source muy sólida,
producida por la compañía Jetbrains. Se utiliza en la web para los
bloques de código.</li>
</ul>
<p>Todas estas fuentes permiten un uso no comercial gratuito.</p>
<blockquote>
<p>TODO: Añadir imagen comparativa con las fuentes</p>
</blockquote>
<h3 id="paleta-de-colores">Paleta de colores</h3>
<p>A fin de mantener consistencia, se ha creado una paleta de colores
específica.</p>
<figure>
<img src="./img/07/Paleta%20de%20colores.png" width="400"
alt="La paleta de colores del proyecto" />
<figcaption aria-hidden="true">La paleta de colores del
proyecto</figcaption>
</figure>
<p>El principal objetivo es <strong>transmitir tranquilidad</strong>,
pero a la misma vez, <strong>profesionalidad</strong>. De nuevo,
buscamos la idea de profesionalidad distendida que ya hemos repetido un
par de veces.</p>
<p>Partiendo del rojo que traía Eisvogel (lo que para nosotros sería el
rojo primario), se han creado el resto. En principio, con 5 tonalidades
diferentes nos basta. Todas ellas vienen acompañadas de sus respectivas
variaciones oscuras, muy oscuras, claras y muy claras. Corresponderían a
los <code>color-100, color-300, color-500, color-700, color-900</code>
que estamos acostumbrados en diseño web. Para la escala de grises, se
han escogido 7 colores en vez de 9. Son más que suficientes para lo que
necesitamos. Puedes encontrar las definiciones en el <a
href="https://github.com/Asmilex/Raytracing/blob/main/docs/headers/style.css">fichero
de estilos</a>.</p>
<p>Todos los colores que puedes ver en este documento se han extraído de
la paleta. ¡La consistencia es clave!</p>
<h2 id="flujo-de-trabajo-y-herramientas">Flujo de trabajo y
herramientas</h2>
<p>Encontrar una herramienta que se adapte a un <em>workflow</em> es
complicado. Aunque hay muchos programas maravilosos, debemos hacerlos
funcionar en conjunto. En este apartado, vamos a describir cuáles son
las que hemos usado.</p>
<p>Principalmente destacan tres de ellas: <strong>Github</strong>,
<strong>Pandoc</strong> y <strong>Figma</strong>. La primera tendrá <a
href="#github">su propia sección</a>, así que hablaremos de las
otras.</p>
<blockquote>
<p>TODO: foto del workflow.</p>
</blockquote>
<h3 id="pandoc">Pandoc</h3>
<p><a href="https://pandoc.org/">Pandoc</a> es una estupendísima de
conversión de documentos. Se puede usar para convertir un tipo de
archivo a otro. En este caso, se usa para convertir una serie de
ficheros Markdown (los capítulos) a un fichero HTML (la web) y a PDF. Su
punto más fuerte es que permite escribir LaTeX de forma simplificada,
como si se tratara de <em>sugar syntax</em>. Combina la simplicidad de
Markdown y la correctitud de LaTeX.</p>
<p>Su funcionamiento en este proyecto es el siguiente: Primero, recoge
los capítulos que se encuentra en <code>docs/chapters</code>, usando una
serie de cabeceras en YAML que especifican ciertos parámetros (como
autor, fecha, título, etc.), así como scripts de Lua. Estas caceberas se
encuentran en <code>docs/headers</code>. En particular:</p>
<ol type="1">
<li><code>meta.md</code> recoge los parámetros base del trabajo.</li>
<li><code>pdf.md</code> y <code>web.md</code> contienen algunas
definiciones específicas de sus respectivos formatos. Por ejemplo, el
YAML del PDF asigna las variables disponibles de la plantilla Eisvogel;
mientras que para la web se incluyen las referencias a algunas
bibliotecas de Javascript necesarias o los estilos
(<code>docs/headers/style.css</code>, usando como base <a
href="https://github.com/rilwis/bamboo">Bamboo.css</a>).</li>
<li><code>math.md</code> contiene las definiciones de LaTeX.</li>
<li>Se utilizan algunos filtros específicos de Lua para simplificar la
escritura. En específico, <code>standard-code.lua</code> formatea
correctamente los bloques de código para la web.</li>
</ol>
<p>Un fichero Makefile (<code>docs/Makefile</code>) contiene varias
órdenes para generar ambos formatos. Tienen varios parámetros
adicionales de por sí, como puede ser la bibliografía
(<code>docs/chapters/bibliography.bib</code>).</p>
<h3 id="figma">Figma</h3>
<p><a href="https://www.figma.com/">Figma</a> es otro de esos programas
que te hace preguntarte por qué es gratis. Es una aplicación en la web
usada para diseño gráfico. Es muy potente, intuitiva, y genera unos
resultados muy buenos en poco tiempo. Todos los diseños de este trabajo
se han hecho con esta herramienta.</p>
<figure>
<img src="./img/07/Figma.png"
alt="Tablón principal del proyecto de Figma, a día 15 de abril de 2022" />
<figcaption aria-hidden="true">Tablón principal del proyecto de Figma, a
día 15 de abril de 2022</figcaption>
</figure>
<p>Una de las características más útiles es poder exportar rápidamente
la imagen. Esto permite hacer cambios rápidos y registrarlos en el
repositorio fácilmente. Además, permite instalar plugins. Uno de ellos
ha resultado especialmente útil: <a
href="https://www.figma.com/community/plugin/793023817364007801/LaTeX-Complete">Latex
Complete</a>. Esto nos permite incrustar código LaTeX en el documento en
forma de SVG.</p>
<h3 id="otros-programas">Otros programas</h3>
<p>Como es normal, hay muchos otros programas que han intervenido en el
desarrollo. Estos son algunos de ellos:</p>
<ul>
<li>El editor por excelencia <a
href="https://code.visualstudio.com/">VSCode</a>. Ha facilitado en gran
medida el desarrollo de la aplicación y la documentación. En particular,
se ha usado una extensión denominada <a
href="https://marketplace.visualstudio.com/items?itemName=Gruntfuggly.triggertaskonsave">Trigger
task on save</a> que compila la documentación HTML automáticamente al
guardar un fichero. ¡Muy útil y rápido!</li>
<li><a href="https://www.vectary.com/">Vectary</a> para hacer los
diseños en 3D fácilmente. Permite exportar una escena rápidamente a png
para editarla en Figma.</li>
<li>Como veremos más adelante, la documentación se compila en el
repositorio usando un contenedor de <a
href="https://www.docker.com/">Docker</a>.</li>
<li>Cualquier proyecto informático debería usar <code>git</code>. Este
no es una excepción.</li>
</ul>
<h2 id="github">Github</h2>
<p>La página <a href="https://github.com">Github</a> ha alojado
prácticamente todo el contenido del trabajo; desde el programa, hasta la
documentación online. El repositorio se puede consultar en <a
href="https://github.com/Asmilex/Raytracing">Github.com/Asmilex/Raytracing</a>.</p>
<p>Se ha escogido Github en vez de sus competidores por los siguientes
motivos:</p>
<ol type="1">
<li>Llevo usándola toda la carrera. Es mi página de hosting de
repositorios favorita.</li>
<li>Los repositorios de Nvidia se encontraban en Github, por lo que
resulta más fácil sincronizarlos.</li>
<li>La documentación se puede desplegar usando Github Pages.</li>
<li>Las Github Actions son particularmente cómodas y sencillas de
usar.</li>
</ol>
<p>Entremos en detalle en algunos de los puntos anteriores:</p>
<h3
id="integración-continua-con-github-actions-y-github-pages">Integración
continua con Github Actions y Github Pages</h3>
<p>Cuando hablamos de <strong>integración continua</strong>, nos
referimos a ciertos programas que corren en un repositorio y se encargan
de hacer ciertas transformaciones al código, de forma que este se
prepare para su presentación final. En esencia, automatizan algunas
tareas habituales de un desarrollo de software.</p>
<p>En este trabajo lo usaremos para compilar la documentación. De esta
forma, no necesitamos lidiar con “proyecto final”, “proyecto final
definitivo”, “proyecto final final v2”, etc. Simplemente, cuando
registremos un cambio en los ficheros Markdown (lo que se conoce en git
como un <code>commit</code>), y lo subamos a Github (acción de
<code>push</code>), se ejecutará un denominado <code>Action</code> que
operará sobre nuestros archivos.</p>
<p>Tendremos dos tipos de <code>Actions</code>: uno que se encarga de
compilar la web, y otro el PDF. En esencia, operan de la siguiente
manera:</p>
<ol type="1">
<li>Comprueba si se ha modificado algún fichero <code>.md</code> en el
último commit subido. Si no es el caso, para.</li>
<li>Si sí se ha modificado, accede a la carpeta del repositorio y
compila la documentación mediante <code>pandoc</code>.
<ol type="1">
<li>La web se genera en <code>docs/index.html</code>. Publica la web a
Github Pages.</li>
<li>El PDF se crea en <code>docs/TFG.pdf</code></li>
</ol></li>
<li>Commitea los archivos y termina.</li>
</ol>
<figure>
<img src="./img/07/Github%20Actions.png"
alt="La pestaña de Github Actions permite controlar con facilidad el resultado de un workflow y cuánto tarda en ejecutarse" />
<figcaption aria-hidden="true">La pestaña de Github Actions permite
controlar con facilidad el resultado de un workflow y cuánto tarda en
ejecutarse</figcaption>
</figure>
<p>El workflow de la web corre automáticamente, mientras que para
generar el PDF hace falta activación manual. Aunque no es <em>del
todo</em> correcto almacenar ficheros binarios en un repositorio de git,
no me resulta molesto personalmente. Así que, cuando considero que es el
momento oportuno, lo hago manualmente. Además, también se activa por
cada <em>release</em> que se crea.</p>
<p>Volviendo a la web, Github permite alojar páginas web para un
repositorio. Activando el parámetro correcto en las opciones del
repositorio, y configurándolo debidamente, conseguimos que lea el
archivo <code>index.html</code> generado por el Action y lo despliegue.
Esto es potentísimo: con solo editar una línea de código y subir los
cambios, conseguimos que la web se actualice al instante.</p>
<p>Para generar los archivos nos hace falta una distribución de LaTeX,
Pandoc, y todas las dependencias (como filtros). Como no encontré ningún
contenedor que sirviera mi propósito, decidí crear uno. Se encuentra en
el <a href="https://hub.docker.com/r/asmilex/raytracing">repositorio de
Dockerhub</a>. Esta imagen está basada en <a
href="https://hub.docker.com/r/dockershelf/latex">dockershelf/latex:full</a>.
Por desgracia, es <em>muy</em> pesada para ser un contenedor.
Desafortunadamente, una instalación de LaTeX ocupa una cantidad de
espacio considerable; y para compilar el PDF necesitamos una muy
completa, por lo que debemos lidiar con este <em>overhead</em>. Puedes
encontrar el Dockerfile <a
href="https://github.com/Asmilex/Raytracing/blob/main/Dockerfile">aquí</a>.</p>
<h3 id="issues-y-github-projects">Issues y Github Projects</h3>
<p>Las tareas pendientes se gestionan mediante issues. Cada vez que se
tenga un objetivo particular para el desarrollo, se anota un issue.
Cuando se genere un commit que avance dicha tarea, se etiqueta con el
número correspondiente al issue. De esta forma, todas las confirmaciones
relacionadas con la tarea quedan recogidas en la página web. Puedes ver
un ejemplo en el <a
href="https://github.com/Asmilex/Raytracing/issues/22">issue número
22</a>.</p>
<p>Esto permite una gestión muy eficiente de los principales problemas y
objetivos pendientes de la aplicación.</p>
<figure>
<img src="./img/07/Issues.png"
alt="Pestaña de issues, día 16 de abril de 2022" />
<figcaption aria-hidden="true">Pestaña de issues, día 16 de abril de
2022</figcaption>
</figure>
<p>Los issues se agrupan en <em>milestones</em>, o productos mínimamente
viables. Estos issues suelen estar relacionados con algún apartado
importante del desarrollo.</p>
<figure>
<img src="./img/07/Milestones.png"
alt="Los milestones agrupan una serie de issues relacionados con un punto clave del desarrollo" />
<figcaption aria-hidden="true">Los <em>milestones</em> agrupan una serie
de issues relacionados con un punto clave del desarrollo</figcaption>
</figure>
<p>De esta forma, podemos ver todo lo que queda pendiente para la fecha
de entrega.</p>
<p>Para añadir mayor granularidad a la gestión de tareas y proporcionar
una vista informativa, se utiliza Github Projects. En esencia, esta
aplicación es un acompañante del repositorio estilo Asana.</p>
<figure>
<img src="./img/07/Projects.png"
alt="Projects agrupa los issues y les asigna prioridades" />
<figcaption aria-hidden="true">Projects agrupa los issues y les asigna
prioridades</figcaption>
</figure>
<p>Una de las alternativas que se planteó al inicio fue <a
href="https://linear.app/">Linear</a>, una aplicación de gestión de
issues similar a Projects. Sin embargo, la conveniencia de tener
Projects integrado en Github supuso un punto a favor para este gestor.
De todas formas, el equipo de desarrollo se compone de una persona, así
que no hace falta complicar excesivamente el workflow.</p>
<p>El desarrollo general de la documentación no ha seguido este sistema
de issues, pues está sujeta a cambios constantes y cada commit está
marcado con <code>[:notebook:]</code>. No obstante, ciertos problemas
relacionados con ella, como puede ser el formato de entrega, sí que
quedan recogidos como un issue.</p>
<p>Finalmente, cuando se produce un cambio significativo en la
aplicación (como puede ser una refactorización, una implementación
considerablemente más compleja…) se genera una nueva rama. Cuando se ha
cumplido el objetivo, se <em>mergea</em> la rama con la principal
<code>main</code> mediante un <em>pull request</em>. Esto proporciona un
mecanismo de robustez ante cambios complejos.</p>
<h3 id="estilo-de-commits">Estilo de commits</h3>
<p>Una de los detalles que has podido apreciar si has entrado al
repositorio es un estilo de commit un tanto inusual. Aunque parece un
detalle de lo más insustancial, añadir emojis a los mensajes de commits
añade un toque particular al repositorio, y permite identificar
rápidamente el tipo de cambio.</p>
<p>Cada uno tiene un significado particular. En esta tabla se recogen
sus significados:</p>
<figure>
<img src="./img/07/Commits.png"
alt="Los emojis permiten reconocer el objetivo de cada commit. Esta tabla recoge el significado de cada uno" />
<figcaption aria-hidden="true">Los emojis permiten reconocer el objetivo
de cada commit. Esta tabla recoge el significado de cada
uno</figcaption>
</figure>
<hr>
<h2 class="unlisted unnumbered" id="referencias-7">Referencias</h2>
<p><span class="citation" data-cites="digital-foundry-2020">(<a
href="#ref-digital-foundry-2020" role="doc-biblioref">Digital Foundry
n.d.</a>)</span>, <span class="citation" data-cites="nature-2016">(<a
href="#ref-nature-2016" role="doc-biblioref">Nature n.d.</a>)</span>,
<span class="citation" data-cites="beck2001agile">(<a
href="#ref-beck2001agile" role="doc-biblioref">Beck et al.
2001</a>)</span>, <span class="citation" data-cites="merelo-2021">(<a
href="#ref-merelo-2021" role="doc-biblioref">Merelo n.d.</a>)</span></p>
<h1 id="glosario-de-términos">Glosario de términos</h1>
<blockquote>
<p><em>It’s dangerous to go alone, take this.</em></p>
</blockquote>
<p>Tener en mente <em>todos</em> los conceptos y sus expresiones que
aparecen en un libro como este es prácticamente imposible. Tampoco hay
necesidad de ello, realmente. ¡Vaya desperdicio de cabeza! Por eso, aquí
tienes recopilada una lista con todos los elementos importantes y un
enlace a sus secciones correspondientes.</p>
<h2 id="notación">Notación</h2>
<table>
<colgroup>
<col style="width: 28%" />
<col style="width: 71%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Concepto</strong></th>
<th style="text-align: left;"><strong>Notación</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Puntos</strong></td>
<td style="text-align: left;">Letras mayúsculas: <span
class="math inline">\(P, Q, \dots\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Escalares</strong></td>
<td style="text-align: left;">Letras minúsculas: <span
class="math inline">\(a, b, c, k, \dots\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Vectores</strong></td>
<td style="text-align: left;">Letras minúsculas en negrita: <span
class="math inline">\(\mathbf{v, w, n}, \dots\)</span>. Si están
normalizados, se les pone gorrito (por ejemplo, <span
class="math inline">\(\hat{\mathbf{n}}\)</span>)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Matrices</strong></td>
<td style="text-align: left;">Letras mayúsculas en negrita: <span
class="math inline">\(\mathbf{M}\)</span>. Por columnas.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Producto escalar</strong></td>
<td style="text-align: left;"><span class="math inline">\(\mathbf{v}
\cdot \mathbf{w}\)</span>. Si es el producto escalar de un vector
consigo mismo, a veces pondremos <span
class="math inline">\(\mathbf{v}^2\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Producto vectorial</strong></td>
<td style="text-align: left;"><span class="math inline">\(\mathbf{v}
\times \mathbf{w}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><a
href="#repaso-de-probabilidad"><strong>Variables
aleatorias</strong></a></td>
<td style="text-align: left;"><span class="math inline">\(X, Y\)</span>.
<span class="math inline">\(\xi\)</span> representa una variable
aleatoria con distribución uniforme en <span class="math inline">\([0,
1)\)</span>.</td>
</tr>
</tbody>
</table>
<h2 id="radiometría"><a href="#radiometría">Radiometría</a></h2>
<table>
<colgroup>
<col style="width: 17%" />
<col style="width: 82%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Concepto</strong></th>
<th style="text-align: left;"><strong>Expresiones</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><a href="#ángulos-sólidos"><strong>Ángulo
sólido</strong></a>, derivada [<a href="#eq:d_omega">2</a>]</td>
<td style="text-align: left;"><span
class="math inline">\(\begin{aligned} &amp;\omega = \frac{A}{r^2} \\
&amp; d\omega = \sin\theta\ d\theta\ d\phi\end{aligned}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Hemisferio de direcciones
alrededor de un vector</strong></td>
<td style="text-align: left;"><span
class="math inline">\(H^2(\mathbf{n})\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><a href="#unidades-básicas"><strong>Carga
de energía</strong></a></td>
<td style="text-align: left;"><span class="math inline">\(Q = hf =
\frac{hc}{\lambda}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong><a href="#potencia">Flujo
radiante, potencia</a></strong></td>
<td style="text-align: left;"><span
class="math inline">\(\begin{aligned}&amp; \Phi = \frac{dQ}{dt} \\ &amp;
\Phi = \int_{A}\int_{H^2(\mathbf{n})}{L_o(p, \omega) d\omega^\bot
dA}\end{aligned}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong><a
href="#irradiancia">Irradiancia, radiancia emitida</a></strong></td>
<td style="text-align: left;"><span
class="math inline">\(\begin{aligned} &amp; E = \frac{\Phi}{A} \\ &amp;
E(p) = \frac{d\Phi}{dA} \\ &amp; E(p, \mathbf{n}) = \int_{\Omega}{L_i(p,
\omega) \lvert cos\theta \rvert d\omega} \\ &amp; E(p, \mathbf{n}) =
\int_{0}^{2\pi}\int_{0}^{\pi/2}{L_i(p, \theta, \phi) \cos\theta\
\sin\theta\ d\theta\ d\phi} \\ &amp; E(p, \mathbf{n}) =
\int_{A}{L\cos\theta\ \frac{\cos\theta_o}{r^2}dA}
\end{aligned}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong><a
href="#intensidad_radiante">Intensidad radiante</a></strong></td>
<td style="text-align: left;"><span
class="math inline">\(\begin{aligned}I =
\frac{d\Phi}{d\omega}\end{aligned}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong><a
href="#radiancia">Radiancia</a></strong></td>
<td style="text-align: left;"><span
class="math inline">\(\begin{aligned} &amp; L(p, \omega) =
\frac{dE_\omega(p)}{d\omega} \\ &amp; L(p, \omega) = \frac{d^2\Phi(p,
\omega)}{d\omega\ dA^\bot} = \frac{d^2\Phi(p, \omega)}{d\omega\ dA\
\cos\theta} \\ &amp; L^+(p, \omega) = \lim_{t \to 0^+}{L(p +
t\mathbf{n_p}, \omega)} \\ &amp; L^-(p, \omega) = \lim_{t \to 0^-}{L(p +
t\mathbf{n_p}, \omega)} \end{aligned}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong><a href="#radiancia">Radiancia
incidente</a></strong></td>
<td style="text-align: left;"><span
class="math inline">\(\begin{aligned} L_i(p, \omega) = \begin{cases}
L^+(p, -\omega) &amp; \text{si } \omega \cdot \mathbf{n_p} &gt; 0 \\
L^-(p, -\omega) &amp; \text{si } \omega \cdot \mathbf{n_p} &lt; 0
\end{cases}\end{aligned}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong><a href="#radiancia">Radiancia
reflejada, radiancia de salida</a></strong></td>
<td style="text-align: left;"><span
class="math inline">\(\begin{aligned} &amp; L_o(p, \omega) =
\begin{cases} L^+(p, \omega) &amp; \text{si } \omega \cdot \mathbf{n_p}
&gt; 0 \\ L^-(p, \omega) &amp; \text{si } \omega \cdot \mathbf{n_p} &lt;
0 \end{cases} \end{aligned}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong><a href="">Ecuación de
dispersión</a></strong></td>
<td style="text-align: left;"><span
class="math inline">\(\begin{aligned}L_o(p, \omega_o) =
\int_{\mathbb{S}^2}{f(p, \omega_o \leftarrow \omega_i)L_i(p,
\omega_i)\lvert \cos\theta_i \rvert
d\omega_i}\end{aligned}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong><a
href="#la-función-de-distribución-de-reflectancia-bidireccional-brdf">BRDF</a></strong></td>
<td style="text-align: left;"><span
class="math inline">\(\begin{aligned}&amp; f_r(p, \omega_o \leftarrow
\omega_i) = \frac{dL_o(p, \omega_o)}{dE(p, \omega_i)} = \frac{dL_o(p,
\omega_o)}{L_i(p, \omega_i) \cos\theta_i\
d\omega_i}\end{aligned}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong><a
href="#la-función-de-distribución-de-transmitancia-bidireccional-btdf">BTDF</a></strong></td>
<td style="text-align: left;"><span class="math inline">\(f_t(p,
\omega_o \leftarrow \omega_i)\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong><a
href="#juntando-la-brdf-y-la-btdf">BSDF</a></strong></td>
<td style="text-align: left;"><span class="math inline">\(f(p, \omega_o
\leftarrow \omega_i)\)</span></td>
</tr>
</tbody>
</table>
<h1 class="unnumbered" id="bibliografía">Bibliografía</h1>
<div id="refs" class="references csl-bib-body hanging-indent"
role="doc-bibliography">
<div id="ref-Marrs2021" class="csl-entry" role="doc-biblioentry">
Adam Marrs, Peter Shirley, and Ingo Wald, eds. 2021. <span>“Ray Tracing
Gems II.”</span> Apress. 2021. <a
href="http://raytracinggems.com/rtg2">http://raytracinggems.com/rtg2</a>.
</div>
<div id="ref-arneback-2019" class="csl-entry" role="doc-biblioentry">
Arnebäck. n.d. <span>“An Explanation of the Rendering Equation.”</span>
Accessed April 9, 2022. <a
href="https://www.youtube.com/watch?v=eo_MTI-d28s">https://www.youtube.com/watch?v=eo_MTI-d28s</a>.
</div>
<div id="ref-beck2001agile" class="csl-entry" role="doc-biblioentry">
Beck, Kent, Mike Beedle, Arie van Bennekum, Alistair Cockburn, Ward
Cunningham, Martin Fowler, James Grenning, et al. 2001. <span>“Manifesto
for Agile Software Development.”</span> <a
href="http://www.agilemanifesto.org/">http://www.agilemanifesto.org/</a>.
</div>
<div id="ref-berkeley-cs184" class="csl-entry" role="doc-biblioentry">
Berkeley cs184. n.d. <span>“Monte Carlo Integration Cs184/284a.”</span>
Accessed March 20, 2022. <a
href="https://cs184.eecs.berkeley.edu/sp22">https://cs184.eecs.berkeley.edu/sp22</a>.
</div>
<div id="ref-caulfield-2020" class="csl-entry" role="doc-biblioentry">
Caulfield, Brian. n.d. <span>“What’s the Difference Between Ray Tracing,
Rasterization?”</span> Accessed April 22, 2022. <a
href="https://blogs.nvidia.com/blog/2018/03/19/whats-difference-between-ray-tracing-rasterization/">https://blogs.nvidia.com/blog/2018/03/19/whats-difference-between-ray-tracing-rasterization/</a>.
</div>
<div id="ref-crytek-2020" class="csl-entry" role="doc-biblioentry">
Crytek. n.d. <span>“Crysis Remastered Brings Ray Tracing to Current-Gen
Consoles.”</span> Accessed April 17, 2022. <a
href="https://www.cryengine.com/news/view/crysis-remastered-brings-ray-tracing-to-current-gen-consoles">https://www.cryengine.com/news/view/crysis-remastered-brings-ray-tracing-to-current-gen-consoles</a>.
</div>
<div id="ref-digital-foundry-2020" class="csl-entry"
role="doc-biblioentry">
Digital Foundry. n.d. <span>“Cyberpunk 2077 PC: What Does Ray Tracing
Deliver... And Is It Worth It?”</span> Accessed April 10, 2022. <a
href="https://www.youtube.com/watch?v=6bqA8F6B6NQ">https://www.youtube.com/watch?v=6bqA8F6B6NQ</a>.
</div>
<div id="ref-Pellacini-Marschner-2017" class="csl-entry"
role="doc-biblioentry">
Fabio Pellacini, Steve Marschner. n.d. <span>“Fundamentals of Computer
Graphics.”</span> Accessed April 9, 2022. <a
href="https://pellacini.di.uniroma1.it/teaching/graphics17b/">https://pellacini.di.uniroma1.it/teaching/graphics17b/</a>.
</div>
<div id="ref-galvin-no-date" class="csl-entry" role="doc-biblioentry">
Galvin. n.d. <span>“Random Variables.”</span> Accessed March 20, 2022.
<a
href="https://www3.nd.edu/~dgalvin1/10120/10120_S16/Topic17_8p4_Galvin_class.pdf">https://www3.nd.edu/~dgalvin1/10120/10120_S16/Topic17_8p4_Galvin_class.pdf</a>.
</div>
<div id="ref-Haines2019" class="csl-entry" role="doc-biblioentry">
Haines, Eric, and Tomas Akenine-Möller, eds. 2019. <span>“Ray Tracing
Gems.”</span> Apress. 2019. <a
href="http://raytracinggems.com">http://raytracinggems.com</a>.
</div>
<div id="ref-merelo-2021" class="csl-entry" role="doc-biblioentry">
Merelo. n.d. <span>“Infraestructura Virtual.”</span> Accessed April 16,
2022. <a
href="http://jj.github.io/IV/documentos/temas/Integracion_continua">http://jj.github.io/IV/documentos/temas/Integracion_continua</a>.
</div>
<div id="ref-nature-2016" class="csl-entry" role="doc-biblioentry">
Nature. n.d. <span>“Scientific Language Is Becoming More
Informal.”</span> Accessed April 10, 2022. <a
href="https://doi.org/10.1038/539140a">https://doi.org/10.1038/539140a</a>.
</div>
<div id="ref-overvoorde-2022" class="csl-entry" role="doc-biblioentry">
Overvoorde, Alexander. n.d. <span>“Introduction - Vulkan
Tutorial.”</span> Accessed April 18, 2022. <a
href="https://vulkan-tutorial.com/">https://vulkan-tutorial.com/</a>.
</div>
<div id="ref-mcbook" class="csl-entry" role="doc-biblioentry">
Owen, Art B. 2013. <em>Monte Carlo Theory, Methods and Examples</em>. <a
href="https://artowen.su.domains/mc/">https://artowen.su.domains/mc/</a>.
</div>
<div id="ref-PBRT3e" class="csl-entry" role="doc-biblioentry">
Pharr, Matt, Wenzel Jakob, and Greg Humphreys. 2016. <span>“Physically
Based Rendering: From Theory to Implementation (3rd Ed.).”</span> San
Francisco, CA, USA: Morgan Kaufmann Publishers Inc. November 2016. <a
href="https://www.pbr-book.org/3ed-2018/contents">https://www.pbr-book.org/3ed-2018/contents</a>.
</div>
<div id="ref-quantumfracture-2021" class="csl-entry"
role="doc-biblioentry">
QuantumFracture. n.d. <span>“Ya, En Serio, ¿Qué Es La Luz?”</span>
Accessed April 22, 2022. <a
href="https://www.youtube.com/watch?v=DkcEAz09Buo">https://www.youtube.com/watch?v=DkcEAz09Buo</a>.
</div>
<div id="ref-unknown-author-no-date" class="csl-entry"
role="doc-biblioentry">
<span>“Rendering.”</span> n.d. Accessed March 20, 2022. <a
href="https://sciencebehindpixar.org/pipeline/rendering#:%7E:text=They%20said%20it%20takes%20at,to%20render%20that%20many%20frames">https://sciencebehindpixar.org/pipeline/rendering#:%7E:text=They%20said%20it%20takes%20at,to%20render%20that%20many%20frames</a>.
</div>
<div id="ref-scratchapixel-2019" class="csl-entry"
role="doc-biblioentry">
Scratchapixel. n.d. <span>“Learn Computer Graphics from Scratch!”</span>
Accessed April 17, 2022. <a
href="https://www.scratchapixel.com/index.php?redirect">https://www.scratchapixel.com/index.php?redirect</a>.
</div>
<div id="ref-Shirley2020RTW1" class="csl-entry" role="doc-biblioentry">
Shirley, Peter. 2020a. <span>“Ray Tracing in One Weekend.”</span> 2020.
<a
href="https://raytracing.github.io/books/RayTracingInOneWeekend.html">https://raytracing.github.io/books/RayTracingInOneWeekend.html</a>.
</div>
<div id="ref-Shirley2020RTW2" class="csl-entry" role="doc-biblioentry">
———. 2020b. <span>“Ray Tracing: The Next Week.”</span> 2020. <a
href="https://raytracing.github.io/books/RayTracingTheNextWeek.html">https://raytracing.github.io/books/RayTracingTheNextWeek.html</a>.
</div>
<div id="ref-Shirley2020RTW3" class="csl-entry" role="doc-biblioentry">
———. 2020c. <span>“Ray Tracing: The Rest of Your Life.”</span> 2020. <a
href="https://raytracing.github.io/books/RayTracingTheRestOfYourLife.html">https://raytracing.github.io/books/RayTracingTheRestOfYourLife.html</a>.
</div>
<div id="ref-ShirleyRRT" class="csl-entry" role="doc-biblioentry">
Shirley, Peter, and R. Keith Morley. 2003. <em>Realistic Ray
Tracing</em>. 2nd ed. USA: A. K. Peters, Ltd. <a
href="https://www.taylorfrancis.com/books/mono/10.1201/9780429294891/realistic-ray-tracing-peter-shirley-keith-morley">https://www.taylorfrancis.com/books/mono/10.1201/9780429294891/realistic-ray-tracing-peter-shirley-keith-morley</a>.
</div>
<div id="ref-studysession-2021" class="csl-entry"
role="doc-biblioentry">
StudySession. n.d. <span>“Solid Angle Derivation &amp;
Intuition.”</span> Accessed April 22, 2022. <a
href="https://www.youtube.com/watch?v=WrKsgBElPWA">https://www.youtube.com/watch?v=WrKsgBElPWA</a>.
</div>
<div id="ref-the-khronos-vulkan-working-group-2022" class="csl-entry"
role="doc-biblioentry">
The Khronos® Vulkan Working Group. n.d. <span>“Vulkan® 1.2.210 - KHR
Extensions: 33. Ray Intersection.”</span> Accessed April 1, 2022. <a
href="https://www.khronos.org/registry/vulkan/specs/1.2-khr-extensions/html/chap33.html#ray-intersection-candidate-determination">https://www.khronos.org/registry/vulkan/specs/1.2-khr-extensions/html/chap33.html#ray-intersection-candidate-determination</a>.
</div>
<div id="ref-wikipedia-contributors-2022E" class="csl-entry"
role="doc-biblioentry">
tracing, Wikipedia: Ray. n.d. <span>“Ray Tracing (Graphics).”</span>
Accessed April 22, 2022. <a
href="https://en.wikipedia.org/wiki/Ray_tracing_(graphics)">https://en.wikipedia.org/wiki/Ray_tracing_(graphics)</a>.
</div>
<div id="ref-wikipedia-contributors-2022G" class="csl-entry"
role="doc-biblioentry">
Wikipedia: Barycentric coordinate system. n.d. <span>“Barycentric
Coordinate System.”</span> Accessed April 22, 2022. <a
href="https://en.wikipedia.org/wiki/Barycentric_coordinate_system">https://en.wikipedia.org/wiki/Barycentric_coordinate_system</a>.
</div>
<div id="ref-wikipedia-contributors-2022C" class="csl-entry"
role="doc-biblioentry">
Wikipedia: Computer. n.d. <span>“Computer.”</span> Accessed April 22,
2022. <a
href="https://en.wikipedia.org/wiki/Computer">https://en.wikipedia.org/wiki/Computer</a>.
</div>
<div id="ref-wikipedia-contributors-2022O" class="csl-entry"
role="doc-biblioentry">
Wikipedia: Differential geometry of surfaces. n.d. <span>“Differential
Geometry of Surfaces.”</span> Accessed April 22, 2022. <a
href="https://en.wikipedia.org/wiki/Differential_geometry_of_surfaces">https://en.wikipedia.org/wiki/Differential_geometry_of_surfaces</a>.
</div>
<div id="ref-wikipedia-contributors-2022H" class="csl-entry"
role="doc-biblioentry">
Wikipedia: Distribución de probabilidad. n.d. <span>“Distribución de
Probabilidad.”</span> Accessed April 22, 2022. <a
href="https://es.wikipedia.org/wiki/Distribuci%C3%B3n_de_probabilidad">https://es.wikipedia.org/wiki/Distribuci%C3%B3n_de_probabilidad</a>.
</div>
<div id="ref-wikipedia-contributors-2022L" class="csl-entry"
role="doc-biblioentry">
Wikipedia: Estimador. n.d. <span>“Estimador.”</span> Accessed April 22,
2022. <a
href="https://es.wikipedia.org/wiki/Estimador?oldformat=true">https://es.wikipedia.org/wiki/Estimador?oldformat=true</a>.
</div>
<div id="ref-wikipedia-contributors-2022J" class="csl-entry"
role="doc-biblioentry">
Wikipedia: Expected value. n.d. <span>“Expected Value.”</span> Accessed
April 22, 2022. <a
href="https://en.wikipedia.org/wiki/Expected_value">https://en.wikipedia.org/wiki/Expected_value</a>.
</div>
<div
id="ref-wikipedia-funcion-de-distribucion-de-reflectancia-bidireccional-2022"
class="csl-entry" role="doc-biblioentry">
Wikipedia: Función de distribución de reflectancia bidireccional. n.d.
<span>“Función de Distribución de Reflectancia Bidireccional.”</span>
Accessed April 22, 2022. <a
href="https://es.wikipedia.org/wiki/Funci%C3%B3n_de_distribuci%C3%B3n_de_reflectancia_bidireccional">https://es.wikipedia.org/wiki/Funci%C3%B3n_de_distribuci%C3%B3n_de_reflectancia_bidireccional</a>.
</div>
<div id="ref-wikipedia-contributors-2022I" class="csl-entry"
role="doc-biblioentry">
Wikipedia: Función de probabilidad. n.d. <span>“Función de
Probabilidad.”</span> Accessed April 22, 2022. <a
href="https://es.wikipedia.org/wiki/Funci%C3%B3n_de_probabilidad">https://es.wikipedia.org/wiki/Funci%C3%B3n_de_probabilidad</a>.
</div>
<div id="ref-wikipedia-contributors-2022A" class="csl-entry"
role="doc-biblioentry">
Wikipedia: history of photography. n.d. <span>“History of
Photography.”</span> Accessed April 22, 2022. <a
href="https://en.wikipedia.org/wiki/History_of_photography">https://en.wikipedia.org/wiki/History_of_photography</a>.
</div>
<div id="ref-wikipedia-contributors-2022F" class="csl-entry"
role="doc-biblioentry">
Wikipedia: Implicit surface. n.d. <span>“Implicit Surface.”</span>
Accessed April 22, 2022. <a
href="https://en.wikipedia.org/wiki/Implicit_surface">https://en.wikipedia.org/wiki/Implicit_surface</a>.
</div>
<div id="ref-wikipedia-contributors-2022B" class="csl-entry"
role="doc-biblioentry">
Wikipedia: Kodak. n.d. <span>“Kodak.”</span> Accessed April 22, 2022. <a
href="https://es.wikipedia.org/wiki/Kodak">https://es.wikipedia.org/wiki/Kodak</a>.
</div>
<div id="ref-wikipedia-contributors-2022M" class="csl-entry"
role="doc-biblioentry">
Wikipedia: Método de la transformada inversa. n.d. <span>“Método de La
Transformada Inversa.”</span> Accessed April 22, 2022. <a
href="https://es.wikipedia.org/wiki/M%C3%A9todo_de_la_transformada_inversa">https://es.wikipedia.org/wiki/M%C3%A9todo_de_la_transformada_inversa</a>.
</div>
<div id="ref-wikipedia-contributors-2021A" class="csl-entry"
role="doc-biblioentry">
Wikipedia: Parametric surface. n.d. <span>“Parametric Surface.”</span>
Accessed April 22, 2022. <a
href="https://en.wikipedia.org/wiki/Parametric_surface">https://en.wikipedia.org/wiki/Parametric_surface</a>.
</div>
<div id="ref-wikipedia-contributors-2022K" class="csl-entry"
role="doc-biblioentry">
Wikipedia: Probability density function. n.d. <span>“Probability Density
Function.”</span> Accessed April 22, 2022. <a
href="https://en.wikipedia.org/wiki/Probability_density_function">https://en.wikipedia.org/wiki/Probability_density_function</a>.
</div>
<div id="ref-wikipedia-contributors-2021D" class="csl-entry"
role="doc-biblioentry">
Wikipedia: Radiometry. n.d. <span>“Radiometry.”</span> Accessed March
20, 2022. <a
href="https://en.wikipedia.org/wiki/Radiometry">https://en.wikipedia.org/wiki/Radiometry</a>.
</div>
<div id="ref-wikipedia-contributors-2022N" class="csl-entry"
role="doc-biblioentry">
Wikipedia: Rejection sampling. n.d. <span>“Rejection Sampling.”</span>
Accessed April 22, 2022. <a
href="https://en.wikipedia.org/wiki/Rejection_sampling">https://en.wikipedia.org/wiki/Rejection_sampling</a>.
</div>
<div id="ref-wikipedia-contributors-2022D" class="csl-entry"
role="doc-biblioentry">
Wikipedia: rendering (computer graphics). n.d. <span>“Rendering
(Computer Graphics).”</span> Accessed April 22, 2022. <a
href="https://en.wikipedia.org/wiki/Rendering_(computer_graphics)">https://en.wikipedia.org/wiki/Rendering_(computer_graphics)</a>.
</div>
<div id="ref-wikipedia-contributors-2021B" class="csl-entry"
role="doc-biblioentry">
Wikipedia: Rendering equation. n.d. <span>“Rendering Equation.”</span>
Accessed April 22, 2022. <a
href="https://en.wikipedia.org/wiki/Rendering_equation">https://en.wikipedia.org/wiki/Rendering_equation</a>.
</div>
<div id="ref-wikipedia-transmittance-2021" class="csl-entry"
role="doc-biblioentry">
Wikipedia: Transmittance. n.d. <span>“Transmittance.”</span> Accessed
April 22, 2022. <a
href="https://en.wikipedia.org/wiki/Transmittance">https://en.wikipedia.org/wiki/Transmittance</a>.
</div>
<div id="ref-wikipedia-contributors-2021C" class="csl-entry"
role="doc-biblioentry">
Wikipedia: Variable aleatoria. n.d. <span>“Variable Aleatoria.”</span>
Accessed April 22, 2022. <a
href="https://es.wikipedia.org/wiki/Variable_aleatoria">https://es.wikipedia.org/wiki/Variable_aleatoria</a>.
</div>
</div>
<section class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>No entraremos en detalle sobre la
naturaleza de la luz. Sin embargo, si te pica la curiosidad, hay muchos
divulgadores como <span class="citation"
data-cites="quantumfracture-2021">(<a href="#ref-quantumfracture-2021"
role="doc-biblioref">QuantumFracture n.d.</a>)</span> que han tratado el
tema con suficiente profundidad.<a href="#fnref1" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>Recuerda que estamos omitiendo la
longitud de onda <span class="math inline">\(\lambda\)</span>.<a
href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>En su defecto, si tenemos una función
de densidad <span class="math inline">\(f_X\)</span>, podemos hallar la
función de distribución haciendo <span class="math inline">\(F_X(x) =
P[X &lt; x] = \int_{x_{min}}^{x}{f_X(t)dt}\)</span>.<a href="#fnref3"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4" role="doc-endnote"><p>Esto no es del todo cierto. Aunque
generalmente suelen ser excepciones debido al coste computacional de RT
en tiempo real, existen algunas implementaciones que son capaces de
correrlo por software. Notablemente, el motor de Crytek, CryEngine, es
capaz de mover ray tracing basado en hardware y en software <span
class="citation" data-cites="crytek-2020">(<a href="#ref-crytek-2020"
role="doc-biblioref">Crytek n.d.</a>)</span><a href="#fnref4"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5" role="doc-endnote"><p>Afortunadamente, esto tampoco es
completamente cierto. La compañía desarrolladora y distribuidora de
videojuegos Valve Corporation ha creado una pieza de software
fascinante: <a
href="https://github.com/ValveSoftware/Proton">Proton</a>. Proton
utiliza Wine para emular software en Linux que solo puede correr en
plataformas Windows. La versión 2.5 añadió soporte para traducción de
bindings de DXR a KHR, lo que permite utilizar DirectX12 ray tracing en
sistemas basados en Linux. El motivo de este software es expandir el
mercado de videojuegos disponibles en su consola, la Steam Deck.<a
href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
</body>
</html>
